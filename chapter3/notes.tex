\documentclass[10pt,a4paper]{article}
\include{../packages}
\include{../macros}

\title{Optimization Methods}
\author{Chapter 3: Second-Order Methods for Unconstrained Optimization}
\date{}
\begin{document}
	\maketitle
	\section{Newton Method}
	In the previous chapter, we have studied optimization problems like $\min_{x\in\Rn} f(x)$ with $f\C(\Rn)$, in particular, we only used first order information to build our methods. In this chapter we assume that $f\Cii(\Rn)$, and we will present second-order methods, that is, in addition to the information on function values and gradients, we will employ evaluations of the Hessian matrices. We will start from the most famous second-order method, namely Newton's method, whose main idea is the following. Given an iterate $x_k$, the next iterate $x_{k+1}$ is chosen to minimize the quadratic approximation of the function around $x_k$:
	\begin{equation*}
		x_{k+1} = \argmin_{x \in \mathbb{R}^n} \left\{ f(x_k) + \grad(x_k)^T (x - x_k) + \frac{1}{2}(x - x_k)^T \hess(x_k)(x - x_k) \right\}.
	\end{equation*}
	The above update formula is not well-defined unless we further assume that $\hess(x_k)$ is positive definite. In that case, the unique minimizer of the minimization problem above is the unique stationary point:
	\begin{equation*}
		\grad(x_k) + \hess(x_k)(x_{k+1} - x_k) = 0,
	\end{equation*}
	which is the same as
	\begin{equation}\label{eq:newton}
		x_{k+1} = x_k - (\hess(x_k))^{-1} \grad(x_k).
	\end{equation}
	The vector $-(\hess(x_k))^{-1} \grad(x_k)$ is called the Newton direction, and the algorithm induced by the update formula \eqref{eq:newton} is called the pure Newton's method. Note that when $\hess(x_k)$ is positive definite for any $k$, Newton's directions are descent directions (Exercise).
	
	\begin{algorithm}[H]\label{alg:newton}
		\caption{Pure Newton}
		
		\KwIn{Pick $x_0\in \Rn$ arbitrarly, $d_0=\grad(x_0)$.}
		
		$k = 0$
		
		\While{$\grad(x_k) \neq 0$}{
			
			Compute a direction $d_k$ as a solution to the linear system $\hess(x_k) d_k = -\grad(x_k)$
			
			$x_{k+1} = x_k +d_k$
			
			$k = k+1$
		}
	\end{algorithm}
At the very least, Newton's method requires that $\hess(x)$ is positive definite for every $x \in \mathbb{R}^n$, which in particular implies that there exists a unique optimal solution $x^*$. However, this is not enough to guarantee convergence, as the following example illustrates.

\begin{example}
	Consider the function $f(x) = \sqrt{1 + x^2}$ defined over the real line. The minimizer of $f$ over $\mathbb{R}$ is of course $x = 0$. The first and second derivatives of $f$ are
	\begin{equation*}
		f'(x) = \frac{x}{\sqrt{1 + x^2}}, \quad f''(x) = \frac{1}{(1 + x^2)^{3/2}}.
	\end{equation*}
	Therefore, (pure) Newton's method has the form
	\begin{equation*}
		x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)} = x_k - x_k(1 + x_k^2) = -x_k^3.
	\end{equation*}
	We therefore see that for $|x_0| \geq 1$ the method diverges and that for $|x_0| < 1$ the method converges very rapidly to the correct solution $x^* = 0$.
\end{example}
\noindent Despite the fact that a lot of assumptions are required to be made in order to guarantee the convergence of the method, Newton's method does have one very attractive feature: under certain assumptions one can prove local \emph{quadratic rate of convergence}, which means that near the optimal solution the errors $e_k = \|x_k - x^*\|$ (where $x^*$ is the unique optimal solution) satisfy the inequality $e_{k+1} \leq M e_k^2$ for some positive $M > 0$. This property essentially means that the number of accuracy digits is doubled at each iteration.

\begin{theorem}[Quadratic local convergence of Newton's method]
	Let $f\in \Cii(\Rn)$ with $\hess$ Lipschitz continuous, i.e., $ \exists\, L > 0$ for which $\|\hess(x) - \hess(y)\| \leq L\|x - y\|$ for any $x, y \in \mathbb{R}^n$. Moreover, let $\hess(x)$ be positive definite for any $x \in \mathbb{R}^n$. Let $\{x_k\}_{k \geq 0}$ be the sequence generated by Newton's method, and let $x^*$ be the unique minimizer of $f$ over $\mathbb{R}^n$. Then for any $k = 0, 1, \ldots$ the inequality
	\begin{equation*}
		\|x_{k+1} - x^*\| \leq \frac{L}{2m}\|x_k - x^*\|^2 
	\end{equation*}
	holds. In addition, if $\|x_0 - x^*\| \leq \frac{m}{L}$, then
	\begin{equation*}
		\|x_k - x^*\| \leq \frac{2m}{L} \left(\frac{1}{2}\right)^{2^k}.
	\end{equation*}
\end{theorem}
\begin{proof}
	Let $k$ be a nonnegative integer. Then
	\begin{align*}
		x_{k+1} - x^* &= x_k - (\hess(x_k))^{-1} \grad(x_k) - x^* \nonumber \\
		&\stackrel{\grad(x^*) = \mathbf{0}}{=} x_k - x^* + (\hess(x_k))^{-1}(\grad(x^*) - \grad(x_k)) \nonumber \\
		&= x_k - x^* + (\hess(x_k))^{-1} \int_0^1 [\hess(x_k + t(x^* - x_k))](x^* - x_k) dt \nonumber \\
		&= (\hess(x_k))^{-1} \int_0^1 [\hess(x_k + t(x^* - x_k)) - \hess(x_k)](x^* - x_k) dt. \nonumber
	\end{align*}
	Combining the latter equality with the fact that $\hess(x_k) \succeq mI$ implies that $\|(\hess(x_k))^{-1}\| \leq \frac{1}{m}$. Hence,
	\begin{align*}
		\|x_{k+1} - x^*\| &\leq \|(\hess(x_k))^{-1}\| \left\| \int_0^1 [\hess(x_k + t(x^* - x_k)) - \hess(x_k)](x^* - x_k) dt \right\| \nonumber \\
		&\leq \|(\hess(x_k))^{-1}\| \int_0^1 \left\| [\hess(x_k + t(x^* - x_k)) - \hess(x_k)](x^* - x_k) \right\| dt \nonumber \\
		&\leq \|(\hess(x_k))^{-1}\| \int_0^1 \|\hess(x_k + t(x^* - x_k)) - \hess(x_k)\| \cdot \|x^* - x_k\| dt \nonumber \\
		&\leq \frac{L}{m} \int_0^1 t \|x_k - x^*\|^2 dt = \frac{L}{2m} \|x_k - x^*\|^2. \nonumber
	\end{align*}
	We will prove inequality (5.4) by induction on $k$. Note that for $k = 0$, we assumed that
	\begin{equation*}
		\|x_0 - x^*\| \leq \frac{m}{L},
	\end{equation*}	
	so in particular
	\begin{equation*}
		\|x_0 - x^*\| \leq \frac{2m}{L} \left(\frac{1}{2}\right)^{2^0},
	\end{equation*}
	establishing the basis of the induction. Assume that (5.4) holds for an integer $k$, that is, $\|x_k - x^*\| \leq \frac{2m}{L} \left(\frac{1}{2}\right)^{2^k}$, we will show it holds for $k + 1$. Indeed, by (5.3) we have
	\begin{equation*}
		\|x_{k+1} - x^*\| \leq \frac{L}{2m} \|x_k - x^*\|^2 \leq \frac{L}{2m} \left(\frac{2m}{L} \left(\frac{1}{2}\right)^{2^k}\right)^2 = \frac{2m}{L} \left(\frac{1}{2}\right)^{2^{k+1}},
	\end{equation*}
	proving the desired result.
\end{proof}
\end{document}