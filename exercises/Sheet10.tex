\documentclass{ExerciseSheet}

%Set Number of the Exercise sheet and the submission deadline.
\setExerciseSheetNumber{10}
\setSubmissionDate{xx.xx.2024}

%boolean variable to determine whether the solutions should be included
\newif\ifsolutions
\solutionstrue
%\solutionsfalse

%We have a figure in this sheet
\usepackage{graphicx}

\begin{document}


%Start with exercises
%-----------------------------------------------------------------------%


\subsection*{Notation}
\vskip 0.5cm 
%-----------------------------------------------------------------------%
\begin{problem}
     Let $f\in C^{1,1}_L(C)$ be bounded below with $C$ convex and closed. We consider the problem
     \begin{align}\label{problem}
         \min_{s.t.~x\in C.}f(x)
     \end{align}
     And we define the gradient mapping by
    \begin{align*}
        G_M(x) = M\left[ x - P_C\left(x - \frac{1}{M}\nabla f(x) \right) \right], \text{ where $M>0.$}
    \end{align*}
   
    \begin{enumerate}
 %        \item Apply the Decrease Lemma for Convex Constrained Problems to prove for any $x\in C$ and $t\in (0,\frac{2}{L})$
	
 %             the following inequality holds
	% \begin{align*}
	% 	f(x)-f(P_C(x-t\nabla f(x))) \geq t\left(1-   
 %            \frac{Lt}{2}\right) \left\| G_{\frac{1}{t}}(x)\right\|^2.
	% \end{align*}
      
        \item For gradient projection method, Backtracking is a process that requires three parameter; $s>0,$\\ $\alpha\in(0,1)$
        $\beta\in(0,1)$ to select the stepsize $t_k$ as follows:\\
        \ First, $t_k$ is initialize by $s.$ Then, while
            \begin{align*}
                f(x_k) - f\left(P_C(x_k -t_k\nabla f(x_k))\right) < \alpha t_k \left\| G_{\frac{1}{t_k}}(x_k)\right\|^2,
            \end{align*}
            we set $t_k\leftarrow \beta t_k.$ That is the choice of the stepsize is given by $t_k=s\beta^{i_k},$ where $i_k$ is the smallest positive integer that satisfies the condition
            \begin{align}\label{backtracking}
                f(x_k) - f\left(P_C(x_k -s\beta^{i_k}\nabla f(x_k))\right) \geq \alpha s\beta^{i_k}\left\| G_{\frac{1}{s\beta^{i_k}}}(x_k)\right\|^2
            \end{align}
    Prove that the chosen stepsize $t_k$ of Backtracking procedure satisfies
      \begin{align}\label{stepsize lower bound}
          t_k\geq \min\left\{s,\frac{2(1-\alpha)\beta}{L} \right\}.
      \end{align}
        \item Suppose $L_1\geq L_2,$ then prove the following monotonicity property of the norm of the gradient mapping $G_M(x)$
        \begin{align}\label{monoticity 1}
            \left\|G_{L_1}(x)\right\| \geq \left\|G_{L_2}(x)\right\|
        \end{align}
 and 
     \begin{align}\label{monoticity 2}
            \frac{\left\|G_{L_1}(x)\right\|}{L_1} \leq  \frac{\left\|G_{L_2}(x)\right\|}{L_2}
        \end{align}
 for all $x\in \R^n.$
     \item Let $\{x_k\}_{k\geq0}$ be the sequence generated by the gradient projection method for solving Problem $\eqref{problem}$ with a stepsize chosen by the backtracking procedure with parameters $s>0,~\alpha\in(0,1),~\beta\in(0,1).$ We set $M=\alpha\min\left\{s,\frac{2(1-\alpha)\beta}{L}\right\}$
     \begin{itemize}
         \item [a.]Prove for any $k\geq0$
            \begin{align}\label{decrease}
                f(x_k)-f(x_{k+1})\geq M\left\|G_{\frac{1}{s}}(x_k)\right\|^2, %\text{  }
           \end{align}
       

         \item [b.] Prove that: the sequence $\{f(x_k)\}_{k\geq0}$ is nonincreasing, converges and  $\displaystyle G_{\frac{1}{s}}(x_k)\to 0$ as $k\to \infty$
         \item [c.] Let $f^*$ be the limit of the convergent sequence $\{f(x_k)\}_{k\geq0}.$ Prove for all $n$
           \begin{align*}
               \min_{k=0,1,\dots,n}\left\|G_{\frac{1}{s}}(x_k)\right\|\leq \sqrt{\frac{f(x_0)-f^*}{M(n+1)}}
           \end{align*}
      \end{itemize}
     \end{enumerate}
 
\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}%Lemma 9.12, Lemma 9.13 and  Theorem 9.14 from Beck's book
  \begin{enumerate}
      \item Applying the decreasing lemma for convex constrained problem (Lemma 3.1 of lectures note) and substituting $x=x_k$ yields
    \begin{align*}
                f(x_k) - f\left(P_C(x_k -t_k\nabla f(x_k))\right) \geq t_k\left(1-\frac{Lt_k}{2}\right)\left\| G_{\frac{1}{t_k}}(x_k)\right\|^2
            \end{align*}
  where $t_k\in(0,~\frac{2}{L}).$ Moreover, we observe that
    \begin{align*}
        \left(1-\frac{Lt_k}{2}\right)\geq \alpha \quad \Leftrightarrow \quad \frac{2(1-\alpha)}{L}\geq t_k
    \end{align*}
hence, all step size $t_k\leq \frac{2(1-\alpha)}{L}$ satisfy Backtracking condition in $\eqref{backtracking}.$ On the other hand,  $t_k$ such that $\frac{t_k}{\beta}> \frac{2(1-\alpha)}{L}$ will not satisfies Backtracking condition $\eqref{backtracking}$. This leads to the following  lower bound of the step size $t_k> \frac{2(1-\alpha)\beta}{L}$. Therefore, $t_k\geq \min\left\{s,\frac{2(1-\alpha)\beta}{L} \right\}$
 will be a good choice.

    \item Applying the second projection theorem yields for all $v\in \R^n$ and $w\in C$
        \begin{align*}
            \left<v - P_C(v) , P_C(v) - w \right> \geq 0.
        \end{align*}
substituting $v=x-\frac{1}{L_1}\nabla f(x)$ and $w= P_C\left( x - \frac{1}{L_2}\nabla f(x) \right)$ gives
 
 \begin{align*}
            \left<x-\frac{1}{L_1}\nabla f(x) - P_C\left( x-\frac{1}{L_1}\nabla f(x) \right) , P_C\left(x-\frac{1}{L_1}\nabla f(x) \right) - P_C\left( x - \frac{1}{L_2}\nabla f(x) \right) \right> \geq 0.
        \end{align*}
        that is 
 \begin{align}\label{sec pro 1}
            \left<\frac{1}{L_1}G_{L_1}(x)- \frac{1}{L_1}\nabla f(x), \frac{1}{L_2}G_{L_2}(x)-\frac{1}{L_1}G_{L_1}(x) \right> \geq 0.
        \end{align}
Permuting $L_1$ and $L_2$ gives
         \begin{align}\label{sec pro 2}
            \left<\frac{1}{L_2}G_{L_2}(x)- \frac{1}{L_2}\nabla f(x), \frac{1}{L_1}G_{L_1}(x)-\frac{1}{L_2}G_{L_2}(x) \right> \geq 0.
        \end{align}
then multiplying $\eqref{sec pro 1}$ by $L_1$ and $\eqref{sec pro 2}$ by $L_2$ and summing them yields
        \begin{align*}
            \left<G_{L_1}(x)- \nabla f(x), \frac{1}{L_2}G_{L_2}(x)-\frac{1}{L_1}G_{L_1}(x) \right>+  \left<G_{L_2}(x)- \nabla f(x), \frac{1}{L_1}G_{L_1}(x)-\frac{1}{L_2}G_{L_2}(x) \right> \geq 0
        \end{align*}
        note that 
        \begin{align*}
            \left<G_{L_2}(x)- \nabla f(x), \frac{1}{L_1}G_{L_1}(x)-\frac{1}{L_2}G_{L_2}(x) \right> &= -\left<G_{L_2}(x)- \nabla f(x), \frac{1}{L_2}G_{L_2}(x) -\frac{1}{L_1}G_{L_1}(x) \right>\\
                    &= \left<\nabla f(x)- G_{L_2}(x), \frac{1}{L_2}G_{L_2}(x) -\frac{1}{L_1}G_{L_1}(x) \right>
        \end{align*}
        substituting this into the previous inequality yields
        \begin{align*}
            \left<G_{L_1}(x)- \nabla f(x), \frac{1}{L_2}G_{L_2}(x)-\frac{1}{L_1}G_{L_1}(x) \right>+  \left<\nabla f(x)- G_{L_2}(x), \frac{1}{L_2}G_{L_2}(x) -\frac{1}{L_1}G_{L_1}(x) \right> \geq 0
        \end{align*}
    hence,
        \begin{align*}
            \left<G_{L_1}(x)- G_{L_2}(x), \frac{1}{L_2}G_{L_2}(x)-\frac{1}{L_1}G_{L_1}(x) \right>\geq0
        \end{align*}
        expanding this gives
        \begin{align*}
            \frac{1}{L_2}\left<G_{L_1}(x),G_{L_2}(x)\right>-\frac{1}{L_1}\left\|G_{L_1}(x)\right\|^2-\frac{1}{L_2}\left\|G_{L_2}(x)\right\|^2+ \frac{1}{L_1}\left<G_{L_2}(x),G_{L_1}(x)\right>\geq0
        \end{align*}
    therefore,
       \begin{align}\label{sec por 3}
            \frac{1}{L_1}\left\|G_{L_1}(x)\right\|^2+\frac{1}{L_2}\left\|G_{L_2}(x)\right\|^2\leq \left(\frac{1}{L_1} +\frac{1}{L_2}\right)\left\|G_{L_1}(x)\right\|\left\|G_{L_2}(x)\right\|
        \end{align}
      where applied Cauchy Schwarz inequality on $\left<G_{L_1}(x),G_{L_2}(x)\right>.$ Now if $G_{L_2}(x)=0,$ then $\eqref{sec por 3}$ implies $G_{L_1}(x)=0.$ Consequently, $\eqref{monoticity 1}$ and $\eqref{monoticity 2}$ hold. Next, we consider the case $G_{L_2}(x)\neq0,$ and set $t=\frac{G_{L_1}(x)}{G_{L_2}(x)}.$ Dividing $\eqref{sec por 3}$ by $\left\|G_{L_2}(x)\right\|$ and substituting $t$ yields
        \begin{align*}
            \frac{1}{L_1}t^2- \left(\frac{1}{L_1} +\frac{1}{L_2}\right)t+\frac{1}{L_2}\leq 0
        \end{align*}

    Given that $t=1,\frac{L_1}{L_2}$ are the roots of the second degree polynomial in $t$ we get
        \begin{align*}
            1\leq t\leq\frac{L_1}{L_2}
        \end{align*}
    that is 
       \begin{align*}
            1\leq \frac{G_{L_1}(x)}{G_{L_2}(x)}\leq\frac{L_1}{L_2}
        \end{align*}
    it follows
      \begin{align*}
            \left\|G_{L_2}(x)\right\| \leq \left\|G_{L_1}(x)\right\|
        \end{align*}
 and 
     \begin{align*}
            \frac{\left\|G_{L_1}(x)\right\|}{L_1} \leq  \frac{\left\|G_{L_2}(x)\right\|}{L_2}
        \end{align*}

    \item \begin{itemize}
            \item [a.] It follows by the Backtracking procedure that 
            \begin{align*}%\label{backtracking}
                f(x_k) - f\left(x_{k+1})\right) \geq \alpha t_{k}\left\| G_{\frac{1}{t_k}}(x_k)\right\|^2
            \end{align*}
    substituting \eqref{stepsize lower bound} the lower bound of $t_k$ in the above gives
            \begin{align*}%\label{backtracking}
                f(x_k) - f\left(x_{k+1})\right) \geq M\left\| G_{\frac{1}{t_k}}(x_k)\right\|^2.
            \end{align*}
   Moreover, $\eqref{stepsize lower bound}$ gives $t_k\leq s$ thus $\frac{1}{s}\leq\frac{1}{t_k}.$ Applying the monotonicity property in $\eqref{monoticity 1}$ gives
             \begin{align*}
            \left\|G_{\frac{1}{s}}(x)\right\| \leq \left\|G_{\frac{1}{t_k}}(x)\right\|
        \end{align*}
     Altogether,
       \begin{align*}%\label{backtracking}
                f(x_k) - f\left(x_{k+1})\right) \geq M\left\| G_{\frac{1}{s}}(x_k)\right\|^2.
            \end{align*}

    \item [b.] From Question 3-a we have 
                 \begin{align*}%\label{backtracking}
                f(x_k) - f\left(x_{k+1})\right) \geq M\left\| G_{\frac{1}{s}}(x_k)\right\|^2
            \end{align*}
        in addition $M>0,$ therefore, $f(x_k) \geq f\left(x_{k+1})\right)$ which means sequence $\{f(x_k)\}_k$ is decreasing. Further, since $\{f(x_k)\}_k$ is bounded below, then it converges. Therefore $f(x_k) - f\left(x_{k+1})\right) \to 0$ as $k\to 0.$ It follows from $\eqref{decrease}$ that $G_{\frac{1}{s}}(x_k) \to 0$ as $k\to 0$.    
        
    \item [c.] Summing up the inequality in Question 3-a from 0 to $n$ we get
             \begin{align*}%\label{backtracking}
                f(x_0) - f\left(x_{n+1})\right) \geq M\sum_{k=0}^{n}\left\| G_{\frac{1}{s}}(x_k)\right\|^2.
            \end{align*}
    using the fact that $f\left(x_{n+1})\right)\geq f^*$ we have 
            \begin{align*}%\label{backtracking}
                f(x_0) - f^* \geq M\sum_{k=0}^{n}\left\| G_{\frac{1}{s}}(x_k)\right\|^2.
            \end{align*}

            
          \end{itemize}
\end{enumerate}


\end{solution}

\fi

%-----------------------------------------------------------------------%
% \vskip 0.5cm
% \begin{problem}
% \end{problem}

% \ifsolutions
% \vskip 0.3cm

% \begin{solution}
% \end{solution}

% \fi

%-----------------------------------------------------------------------%
\vskip 0.5cm

\begin{exo}
Consider function $f$ define on $\R^2$ by
  \begin{align*}
      f(x, y) = x^4 + y^4 -2(x - y)^2.
  \end{align*}
  \begin{enumerate}
      \item [1-a)] Prove that there exist $\alpha\in \R_+,$ $\beta\in \R$ (and determine them) such that
      \begin{align*}
          f(x,y)\geq \alpha\|(x,y)\|^2 + \beta
      \end{align*}
    \item [1-b)] Show that the problem 
        \begin{align} \label{pb}
            \inf_{(x,y)\in \R^2} f(x,y)
        \end{align}
    possess at least one solution.
    
    \item [2)] Is the function $f$ convex on $\R^2?$ Justify your answer
    \item [3)] Determine the critical points of $f,$ and specify their nature (local minimum, local maximum, saddle point,...). Then solve problem $\eqref{pb}.$
    
  \end{enumerate}

  
\end{exo}

\ifsolutions
\vskip 0.3cm
\begin{solution}

\begin{itemize}
    \item [1-a)]A direct computation gives \begin{align*}
        f(x, y) = x^4 + y^4 -2x^2 -2y^2 +4xy
    \end{align*}
    Using the fact that for all $x,y\in\R~xy\geq-\frac{1}{2}(x^2+y^2)$ we get
    \begin{align*}
        f(x, y) &\geq x^4 + y^4 -4x^2 -4y^2 \\
                &= x^4 +\epsilon^4 + y^4 +\epsilon^4 -4x^2 -4y^2 -2\epsilon^4 \text{ for all $\epsilon\in\R$}
    \end{align*}
    Using the fact that for all $x,\epsilon\in\R~x^4+\epsilon^4\geq x^2\epsilon^2$ we get
    \begin{align*}
        f(x,y)\geq (2\epsilon^2-4)x^2 + (2\epsilon^2-4)y^2 -2\epsilon^4
    \end{align*}
    the above inequality holds for all $\epsilon\in\R$ in particular for $\epsilon=\sqrt{3}$ that is
    \begin{align}
        f(x,y) &\geq 2(x^2+y^2) - 18\nonumber\\
                &= 2\|(x,y)\|^2 - 18    \label{lowerbound f}    
    \end{align}
    hence, take $\alpha=2$ and $\beta=-18$
    
    \item [1-b)] Application \\
        Since $\displaystyle\lim_{\|(x,y)\|\to\infty}(2\|(x,y)\|^2 - 18)=\infty$ it follows from $\eqref{lowerbound f}$ $\displaystyle\lim_{\|(x,y)\|\to\infty}f(x,y)=\infty,$ thus, $f$ is coercive on $\R^2$ which closed. Therefore, by Theorem 3.8 of Chapter 1 of the lecture note Problem $\ref{pb}$ has a solution on $\R^2.$
    \item [2)] The Hessian of is given by
                \begin{align*}
                    Hess f(x,y)=4\begin{pmatrix}
                        3x^2-1 & 1\\
                        1 & 3y^2-1
                    \end{pmatrix}.
                \end{align*}
    hence,
     \begin{align*}
                    Hess f(0,0)=4\begin{pmatrix}
                        -1 & 1\\
                        1 & 3-1
                    \end{pmatrix}.
                \end{align*}
\end{itemize}

 
\end{solution}

\fi

%-----------------------------------------------------------------------%
\vskip 0.5cm
\begin{exo}
Let $f : C\to R$ be a convex function defined on a convex set $C\subset R^n$. Let $A \in \R^{n\times m}$ and $b \in \R^n.$ We define the function g  by
    \begin{align*}
        g(y)=f(Ay+b)
    \end{align*}
 \begin{enumerate}
     \item Let $M\subset \R^n$ be a convex set, and let $A\in\R^{m\times n}$. Prove that the set
        \begin{align*}
            A^{-1}(M)= \{x\in\R^n~:~ Ax\in M\}
        \end{align*}
is convex.
    \item Use the previous question to prove that the set 
    \begin{align*}
        D= \{y \in \R^n~:~ Ay+b\in C\}
    \end{align*}
    is convex

  \item Prove that function $g$ is convex over the convex set D.
   \end{enumerate}
\end{exo}


\ifsolutions
\vskip 0.3cm
\begin{solution}%%
   proof of Theorem 7.17 in Beck's book [1]
\end{solution}
\fi

%-----------------------------------------------------------------------%
\vskip 0.5cm
\begin{exo}
 Consider the function
  \begin{align*}
      f(x,y)=\frac{x^2}{y}
  \end{align*}
 defined over $\R\times \R_{++}=\{(x,y):~y>0\}$\\
 Show that $f$ is convex. 	
\end{exo}

\ifsolutions
\vskip 0.3cm
\begin{solution}
    Example 7.15
\end{solution}
\fi

\end{document}