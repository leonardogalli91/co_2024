\documentclass{ExerciseSheet}

%Set Number of the Exercise sheet and the submission deadline.
\setExerciseSheetNumber{5}
\setSubmissionDate{xx.xx.2024}

%boolean variable to determine whether the solutions should be included
\newif\ifsolutions
\solutionstrue
%\solutionsfalse

%We have a figure in this sheet
\usepackage{graphicx}

\begin{document}


%Start with exercises
%-----------------------------------------------------------------------%


\subsection*{Notation}
\vskip 0.5cm 
%-----------------------------------------------------------------------%
\begin{problem}
 Consider the quadratic function
  \begin{align*}
      f(x) = x^TAx - 2b^Tx
  \end{align*}
  where $A\in \R^{n\times n}$ is positive definite and $b\in \R^n.$
  \begin{enumerate}
      \item [1)] Show that $f \in C^{1,1}_L(\Rn)$ and determine the Lipschitz constant $L$ of the gradient of $f.$
      \item[2)] Show that $f$ has a global minimum.
      \item[3)] Numerical experiment:\\
      \begin{itemize}
          \item [a)]  Write a Python or Matlab function of gradient descent method that finds the optimal solution of 
      \begin{align*}
          \min_{x\in\Rn} f(x)
      \end{align*}
      with a tolerance $\epsilon.$

      \item[b)] Use the function implemented in $a)$ to solve the minimization problem
         \begin{align*}
             \min_{x,y} x^2 + 2y^2
         \end{align*}
         with a constant stepzise $\eta=\frac{1}{L},$ an initial value vector $x_0=(2,1)^T,$ and tolerance parameter $\epsilon=10^{-5}.$ 
      \end{itemize}  
  \end{enumerate}
\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
Let $h\in\Rn$ we have,
\begin{align*}
    f(x+h) &= (x+h)^TA(x+h) -2b^T(x+h) \\
           &= (x+h)^TAx + (x+h)^TAh -2b^Tx -2b^Th\\
           &= x^TAx + h^TAx + x^TAh + h^TAh -2b^Tx - 2b^Th
\end{align*}
that is 
 \begin{align*}
     f(x+h) &= f(x) + h^T(A+A^T)x - 2h^Tb + h^TAh
 \end{align*}
 therefore,
 $\nabla f(x) = (A+A^T)x - 2b.$ Let $x,y\in\Rn$ it follows

\end{solution}

\fi

%-----------------------------------------------------------------------%
\vskip 0.5cm
\begin{problem}
Suppose that $f \in C^{1,1}_L(\Rn)$ and assume that $\nabla^2 f(x) \succcurlyeq 0$ for any $x \in \Rn$. Suppose that the optimal value of the problem 
\begin{align*}
    \min_{x\in\Rn} f(x)
\end{align*} 
is $f^*$. Let $\{ x_k\}_{k \geq0}$ be the sequence generated by the gradient method with constant stepsize $\frac{1}{L}$. \vspace{0.3cm}

Show that if $\{ x_k\}_{k \geq0}$ is bounded, then $f(x_k) \to f^*$ as $k \to \infty$.

\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
Let us start by proving that $\{f(x_k)\}_k$ is a decreasing sequence. The Taylor expansion of $f$ yields
\begin{align*}
    f(x_{k+1}) = f(x_k)+ \big< \nabla f(x_k), x_{k+1}-x_k \big> +\frac{1}{2} \big<(x_{k+1}-x_k)^T \nabla^2 f(z_{\xi}), x_{k+1}-x_k \big>
\end{align*}
where $z_{\xi}=x_k + \xi(x_{k+1}-x_k)$ with $\xi\in [0, 1].$
substituting the schematic of gradient method with constant  stepsize $\eta=\frac{1}{L},$ yields

  \begin{align}
    f(x_{k+1}) &= f(x_k)+ \big< \nabla f(x_k), -\eta\nabla f(x_k) \big> +\frac{1}{2}\eta^2 \big<(\nabla f(x_k))^T \nabla^2 f(z_{\xi}), \nabla f(x_k) \big>\nonumber\\
    &\leq f(x_k) -\eta\left\|\nabla f(x_k)\right\|^2 +\frac{1}{2}\eta^2\left\|\nabla^2 f(z_{\xi})\right\|\left\|\nabla f(x_k)\right\|^2 \label{hess estimate}
 \end{align}
where we applied Cauchy-Schwarz to get the above inequality. Furthermore, $f \in C^{1,1}_L(\Rn),$ gives by Theorem 2.1 (of Chapter 2 of the lecture note) $\left\|\nabla^2 f(z_{\xi})\right\|\leq L=\frac{1}{\eta}.$ Substituting the into the above into $\eqref{hess estimate}$ yields

  \begin{align*}
    f(x_{k+1})&\leq f(x_k) -\eta\left\|\nabla f(x_k)\right\|^2 +\frac{1}{2}\eta\left\|\nabla f(x_k)\right\|^2\\
    &=f(x_k) -\frac{1}{2}\eta\left\|\nabla f(x_k)\right\|^2
 \end{align*}
that is
 \begin{align}
   0\leq \frac{1}{2}\eta\left\|\nabla f(x_k)\right\|^2&\leq f(x_k) - f(x_{k+1}) \label{decreasing f}
 \end{align}
therefore,  $\{f(x_k)\}_k$ is decreasing. In addition, $f is$ bounded below by $f^*$ as this is the optimal value of the minimization problem, hence, $\{f(x_k)\}_k$ converges. It now remains to show that the $\displaystyle \lim_{k\to\infty}f(x_k)=f^*.$ Given that,  $\{ x_k\}_{k \geq0}$ is bounded in $\Rn$ by Bolzanoâ€“Weierstrass theorem its admit a limit point $\displaystyle\lim_{k\to\infty}x_k=x^*$.\\
Let us prove $\displaystyle\lim_{k\to\infty}x_k=x^*$ is a critical point.
 Summing up $\eqref{decreasing f}$ from $0$ to $n$ yields
  \begin{align*}
      \frac{1}{2}\eta\sum_{k=0}^{n}\left\|\nabla f(x_k)\right\|^2&\leq f(x_0) - f(x_{n+1})
  \end{align*}
  where we observed that the right hand side is telescoping sums. Since $f^*\leq f(x_{k+1}),$ it follows,

  \begin{align*}
      \eta\sum_{k=0}^{n}\left\|\nabla f(x_k)\right\|^2&\leq f(x_0) - f^*< \infty     
  \end{align*}
therefore,  $\displaystyle \lim_{k\to\infty}\left\|\nabla f(x_k)\right\|=0$ thus
 $\displaystyle \lim_{k\to\infty}\nabla f(x_k)=0$ moreover, the continuity of $\nabla f$ gives
 \begin{align*}
     \nabla f (\lim_{k\to\infty}x_k)= \lim_{k\to\infty}\nabla f(x_k)=0
 \end{align*}
hence, $\displaystyle\lim_{k\to\infty}x_k=x^*$ is a critical point. Given that,  $\nabla^2 f(x) \succcurlyeq 0$ for any $x \in \Rn,$ then $x^*$ is the unique optimal point that is $f(x^*)=f^*.$ Since $f$ is continuous, 
\begin{align*}
    \lim_{k\to\infty}f(x_k)= f(\lim_{k\to\infty}x_k)=f^*
\end{align*}
\end{solution}

\fi

%-----------------------------------------------------------------------%
\vskip 0.5cm

\begin{exo}
Let $f\in C^2(\Rn)$ prove that if
 \begin{align*}
     \| \nabla f(x) -\nabla f(y)\| \leq L \|x-y\| \quad \forall x,y \in \Rn
 \end{align*}

Then \begin{align*}
    \|\nabla^2 f(x)\|\leq L \quad \forall x\in \Rn
\end{align*}
	
\end{exo}

\ifsolutions
\vskip 0.3cm
\begin{solution}
\end{solution}

\fi

%-----------------------------------------------------------------------%
\vskip 0.5cm
\begin{exo}
Let $f \in C^{1,1}_L(\R^m)$, and let $A \in \R^{m\times n}, ~b\in \R^m$.\\
Show that the function 
 \begin{align*}
     g : \Rn \to \R \text{ defined by } g(x) = f(Ax+b)
 \end{align*}
 satisfies $g \in C^{1,1}_{\Tilde{L}}(\Rn)$, where the Lipschitz constant is given as $\Tilde{L} = \|A\|L.$
\end{exo}


\ifsolutions
\vskip 0.3cm
\begin{solution}
Let $x,y\in\Rn$ Let us prove that 
\begin{align*}
    \|\nabla g(x) - \nabla g(y) \| \leq \Tilde{L} \|x - y \|.
\end{align*}
It follows from the definition of $g$ that
\begin{align*}
    \|\nabla g(x) - \nabla g(y) \| &= \|\nabla f(Ax+b) - \nabla f(Ay+b) \| \\
    &\leq L \|Ax+b -(Ay+b)\|\\
    &= L\|A(x-y)\|\\
    &\leq L\|A\|\|x-y\|
\end{align*}
where we used the fact that $f \in C^{1,1}_L(\R^m)$ to get the first inequality and Cauchy-Schwarz to get the last inequality.
\end{solution}
\fi

%-----------------------------------------------------------------------%
% \vskip 0.5cm
% \begin{exo}
	
% \end{exo}

% \ifsolutions
% \vskip 0.3cm
% \begin{solution}
% \end{solution}
% \fi

\end{document}