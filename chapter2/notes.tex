\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage[left=2.50cm, right=2.50cm, top=2.0cm, bottom=2.0cm]{geometry}
\include{../macros}
\title{Continuous Optimization}
\author{Chapter 2: Gradient Descent}
\date{}
\begin{document}
	\maketitle
	\section{Descent Direction Methods}
	\noindent In this chapter we consider the unconstrained minimization problem
	\begin{equation*}
		\min_{x\in\Rn} \;\; f(x).
	\end{equation*}
The iterative algorithms that we will consider in this chapter take the form
\begin{equation*}
	x_{k+1} = x_k +t_k d_k \quad k=0,1, \dots,
\end{equation*}
where $d_k$ is the so-called direction and $t_k$ is the step size. We will limit ourselves to descent
directions, whose definition is now given.
\begin{definition}
	Let $f:\Rn\to\R$ with $f\in\C(\Rn)$. A vector $0\neq d\in \Rn$ is called a descent direction of $f$ if the directional derivative $f'(x,d)$ is negative, i.e., 
	\begin{equation*}
		f'(x,d)= \grad(x)^T d < 0.
	\end{equation*}
\end{definition}
In particular, by taking small enough steps, descent directions lead to a decrease of the objective function.
\begin{lemma}[descent property of descent directions]
	Let $f:\Rn\to\R$ with $f\in\C(\Rn)$ and let $x\in \Rn$. Suppose that $d$ is a descent direction of $f$ at $x$. Then, there exists $\epsilon>0$ such that 
	\begin{equation*}
		f(x+td) < f(x) \quad \forall \, t \in (0,\epsilon].
	\end{equation*} 
\end{lemma}
\begin{proof}
	Since $f'(x,d)<0$, it follows from the definition of the directional derivative that 
	\begin{equation*}
		\lim_{t\to 0^+}\frac{f(x+td)-f(x)}{t} = f'(x,d) <0.
	\end{equation*}
Therefore, there exists an $\epsilon>0$ such that 
\begin{equation*}
	\frac{f(x+td)-f(x)}{t}<0,
\end{equation*}
for any $t\in(0,\epsilon)$
\end{proof}
\begin{algorithm}[H]\label{alg}
	\caption{Schematic Descent Directions Method}
	
	\KwIn{$x_0\in \Rn$}
	
	$k = 0$
	
	\While{Termination criterion is not satisfied}{
				
			Pick a descent direction $d_k$
			
			Find a step size $t_k$ satisfying $f(x_k+t_kd_k)<f(x_k)$
			
			$x_{k+1} = x_k+t_kd_k$
			
			$k = k+1$
		}
\end{algorithm}
\noindent Various choices are still unspecified: which direction to take, how to select the step size, what termination criterion to use.
\section{Gradient Method}
\noindent The most important choice in the algorithm above concerns the selection of the descent direction. One obvious choice is to pick the steepest (normalized) direction, i.e., $d_k =-\grad(x_k)/||\grad(x_k)||$. In fact, this direction minimizes the directional derivatives between all normalized directions. 
\begin{lemma}
	Let $f:\Rn\to\R$ with $f\in\C(\Rn)$ and let $x\in\Rn$ be non-stationary (i.e., $\grad(x)\neq0$). Then the optimal solution of the problem
	\begin{equation*}
		\begin{split}
			\min \;\; &f'(x,d),\\
			\st& ||d||=1.
		\end{split}
	\end{equation*}
is $d=-\frac{\grad(x)}{||d||}$.
\end{lemma}
\begin{proof}
	As $f\in \C(\Rn)$ and by Cauchy-Schwarz, we have 
	\begin{equation*}
		f'(x,d)=\grad(x)^Td \geq -||\grad(x)||\cdot ||d|| = -||\grad(x)||.
	\end{equation*}
Thus, $-||\grad(x)||$ is a lower bound for the optimal value of the problem. On the other hand, by plugging  $d = -\grad(x)/||\grad(x)||$ in the objective function we get 
\begin{equation*}
	f'\left(x,-\frac{\grad(x)}{||\grad(x)||}\right)=-\grad(x)^T\left(\frac{\grad(x)}{||\grad(x)||}\right)= -||\grad(x)||,
\end{equation*}
and we thus come to the conclusion that the lower bound is attained at $d=-\frac{\grad(x)}{||d||}$.
\end{proof}
\noindent Thus, the gradient method selects $d_k = -\grad(x_k)$ which is obviously a descent direction, i.e., 
\begin{equation*}
\grad(x_k)^Td_k = -\grad(x_k)^T\grad(x_k) = -||\grad(x)||^2.
\end{equation*}
\noindent To define an implementable method, the second important choice we have to make is the selection of the step size $t$. In particular, this will be clearer once we provide the Descent Lemma below, which require the gradient to be Lipschitz continuous. 
\begin{definition}[Lipschitz Continuous Gradient]
	$\grad(x)$ is said to be Lipschitz continuous if 
	\begin{equation*}
		|| \grad(x) -\grad(y)|| \leq L ||x-y|| \quad \forall x,y \in \Rn.
	\end{equation*}
The class of functions with Lipschitz continuous gradient with constant $L$ is denoted by $\LC(\Rn)$.
\end{definition}
\begin{theorem}
	Let $f\in\Cii(\Rn)$. Then the following two claims are equivalent:
	\begin{itemize}
		\item[(a)] $f\in \LC(\Rn)$
		\item[(b)] $||\hess(x)||\leq L \quad \forall x\in \Rn$.
	\end{itemize}
\begin{proof}
	$(b)\Rightarrow (a)$. Suppose that $||\hess(x)||\leq L \quad \forall x\in \Rn$. By the fundamental theorem of calculus we have $\forall x,y \in \Rn$
	\begin{equation*}
		\begin{split}
			\grad(y)=\grad(x) + \int_{0}^{1} \hess(x+t(y-x)) (y-x) dt = \grad(x) + \left(\int_{0}^{1} \hess(x+t(y-x)) dt  \right) \cdot (y-x)
		\end{split}
	\end{equation*}
Thus, 
\begin{equation*}
	\begin{split}
		||\grad(y)-\grad(x)|| &= \left \| \left(\int_{0}^{1} \hess(x+t(y-x)) dt  \right) \cdot (y-x)\right \|\\
		&\leq \left\|\int_{0}^{1} \hess(x+t(y-x)) dt  \right\| \cdot \|(y-x) \|\\
		& \leq \left(\int_{0}^{1}|| \hess(x+t(y-x))|| dt  \right)\cdot \|(y-x) \|\\
		& \leq L \|(y-x) \|
	\end{split}
\end{equation*}
	$(a)\Rightarrow (b)$. Exercise.
\end{proof}
\end{theorem}
\begin{lemma}[Descent Lemma (prequel)] Let $f\in \LC(\Rn)$. Then for any $x,y \in \Rn$
	\begin{equation*}
		f(y) \leq f(x) + \grad(x)^T(y-x) + \frac{L}{2} \|x-y\|^2.
	\end{equation*}
\end{lemma}
\begin{proof}
From the fundamental theorem of calculus and differentiability of $f$ we have 
\begin{equation*}
	\begin{split}
		f (y) & = f(x) + \int_{0}^{1} \grad((1-t)x + t y)^T\! (y-x) \; dt\\
		& = f(x) + \int_{0}^{1} \grad((1-t)x + t y)^T\! (y-x) - \grad(x)^T(y-x) \; dt + \grad(x)^T(y-x)\\ 
		&\leq f(x) + \int_{0}^{1} \| \grad((1-t)x + t y) - \grad(x)\|\! \cdot\! \|y-x\| \; dt + \grad(x)^T(y-x)\\  
		& \leq f(x) + \int_{0}^{1} L \| t(y-x) \|\! \cdot\! \|y-x\| \; dt + \grad(x)^T(y-x)\\
		& = f(x) + L\|y-x\|^2 \cdot \frac{t^2}{2} \Big|_0^1 + \grad(x)^T(y-x)\\
		& = f(x) + \grad(x)^T(y-x) + \frac{L}{2} \|y-x\|^2,
	\end{split}
\end{equation*}
where the second inequality follows from the Lipschitz continuity of $\grad$.
\end{proof}
\begin{lemma}[Descent Lemma] Let $f\in \LC(\Rn)$. Then for any $x \in \Rn$ and $t>0$
	\begin{equation*}
		f(x) - f(x-t\grad(x)) \geq t (1-\frac{Lt}{2}) \| \grad(x)\|^2.
	\end{equation*}
\end{lemma}
\begin{proof}
	The result simply follows by applying the descent lemma (prequel) on $x$ and $y=x-\grad(x)$
	\begin{equation*}
		f(x-t\grad(x))\leq f(x) -t ||\grad(x)\|^2 + \frac{Lt^2}{2} \|\grad(x)\|^2 = f(x) - t (1-\frac{Lt}{2}) \| \grad(x)\|^2
	\end{equation*}
\end{proof}
\noindent In particular, this holds for $x=x_k$ and $x_{k+1} = x_k -\grad(x_k)$,
\begin{equation*}
	f(x_k) - f(x_{k+1}) \geq t (1-\frac{Lt}{2}) \| \grad(x_k)\|^2,
\end{equation*}
which in turns implies that if we select $t\in (0,\frac{2}{L})$ we ensure a decrease of the objective function at each iteration. In particular, if we want to achieve the largest guarantee bound on the decrease, then we seek the maximum of $t (1-\frac{Lt}{2})$ w.r.t. $t$, which is attained at $t=\frac{1}{L}$ with a decrease that becomes
\begin{equation}\label{eq:decrease}
	f(x_k) - f(x_{k+1}) \geq \frac{1}{L} \| \grad(x_k)\|^2.
\end{equation}
At this point we can write down the Gradient Method in terms of an implementable algorithm.

\begin{algorithm}[H]\label{gd}
	\caption{Gradient Descent (GD) Method}
	
	\KwIn{Pick $x_0\in \Rn$ arbitrarly, chose $\epsilon>0$ (e.g., $10^{-4}$).}
	
	$k = 0$
	
	\While{$\|\grad(x_k)\|\leq \epsilon$}{
		
		$x_{k+1} = x_k - \frac{1}{L} \grad(x_k)$
		
		$k = k+1$
	}
\end{algorithm}
\noindent Let us now prove convergence for GD, in particular that $\grad(x_k)$ goes to zero.
\begin{theorem}[Convergence of GD]\label{thm:GD_convergence}
	Let $f\in \LC(\Rn)$ and let $\{x_k\}_k$ be a sequence generated by GD for solving $\min_{x\in\Rn}\;f(x)$. Assume that $f$ is bounded below over $\Rn$, i.e., there exists $m\in \R$ such that $f(x)>m \;\; \forall\, x\in \Rn$. Then we have the following
	\begin{itemize}
		\item[(a)] The sequence $\{f(x_k)\}_k$ is nonincreasing. In addition, for any $k\geq 0$, $f(x_{k+1}) < f(x_k)$ unless $\grad(x_k)=0$.
		\item[(b)] $\grad(x_k) \to 0$ as $k\to \infty$.
	\end{itemize}
\begin{proof}
	(a) directly follows from \eqref{eq:decrease}, as $f(x_{k+1}) < f(x_k)$ and the equality $f(x_{k+1}) = f(x_k)$ only holds when $\grad(x_k)=0$.
	(b) Since the sequence $\{f(x_k)\}_k$ is nonincreasing and bounded from below, it converges. Thus, $f(x_k) - f(x_{k+1}) \to 0$ as $k\to \infty$, which combined with \eqref{eq:decrease} implies that $\|\grad(x_k)\|\to 0$ as $k\to \infty$.  
\end{proof}
\end{theorem}
\noindent Moreover, we can provide the rate of convergence of GD.
\begin{theorem}[Rate of Convergence of GD]
	Under the setting of Theorem \ref{thm:GD_convergence}, let $f^*$ be the limit of the convergent sequence $\{f(x_k)\}_k$. Then for any $T=0,1, \dots$
	\begin{equation*}
		\min_{k=0,1,\dots, T} \| \grad(x_k) \leq \sqrt{\frac{L (f(x_0) -f^*)}{T+1}}
	\end{equation*}
\end{theorem}
\begin{proof}
	Summing the inequality \eqref{eq:decrease} over $k=0, 1, \dots, T$, we obtain 
	\begin{equation*}
		f(x_0) - f(x_{T+1}) = \frac{1}{L}\sum_{k=0}^{T} \|\grad(x_k)\|^2\geq \frac{T+1}{L} \min_{k=0,1,\dots, T} \| \grad(x_k)\|^2
	\end{equation*}
which concludes the proof.
\end{proof}
%TODO: how to remove the min?
%\bibliographystyle{plain}
%\bibliography{../biblio}
\subsection{Line search methods}
The gradient method as defined above can only be employed when we know or we can compute the Lipschitz constant $L$, on the other hand, we would like to have a general method that can be applied on any unconstrained optimization problem. An alternative for selecting the step size is provided by line search methods. Consider a direction $d_k$, one option would be to exactly minimize along the direction $d_k$, i.e., \textbf{exact line search}
\begin{equation*}
	t_k \in \argmin_{t>0} f(x_k+t d_k).
\end{equation*}
However, this approach is not always viable and even when it is, it might be costly. Another option is instead that of accepting a step that will make the function value decrease "sufficiently", namely to apply an \textbf{inexact line search}. In particular, the first line search proposed in the literature is called Armijo line search and it requires the following
\begin{equation}\label{eq:armijo}
	f(x_k +t_k d_k) \leq f(x_k) + \alpha t_k\grad(x_k)^Td_k.
\end{equation}
Notice that if we define $\phi(t)=f(x_k+td_k)$ we can rewrite the inequality above as
\begin{equation*}
\phi(t_k) \leq \phi(0) + \alpha t_k \phi'(0) \qquad \with \alpha \in (0,1).
\end{equation*}
As depicted in Figure 1, the condition requires that the new function value $\phi(t_k)$ stays below the line passing for $(0,\phi(0))$ and with $\alpha\phi'(0)$ as inclination. Notice that as $\phi'(0)<0$ and $\alpha<1$, the line $y=\phi(0) +\alpha t_k \phi'(0)$ is not as inclined as the tangent in 0. The way for selecting a step $t_k$ that satisfies \eqref{eq:armijo} is suggested by the figure.
\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{lines}
	\caption{The figure represents the Armijo line search condition (the notation in this figure is different from the text, replace $\alpha$ in the figure with $t$ from the text.)}
\end{figure}
In particular, the method is called backtracking and it is described below.
\begin{algorithm}[H]\label{alg:armijo}
	\caption{Backtracking on Armijo line search}
	
	\KwIn{Pick $s>0$, $\alpha,\beta \in(0,1)$.}
	
	$i=0$	
	
	\Do{$f(x_k +t_k d_k) > f(x_k) + \alpha t_k \grad(x_k)^Td_k$}{
		
		$t_k = s \beta^i$
		
		$i=i+1$
	}
	
\end{algorithm}
Let us first show that this method terminates in a finite amount of steps
\begin{lemma}
	Let $f\in \C(\Rn)$, $x\in \Rn$ and $d\in \Rn$ be a descent direction. Then Algorithm \ref{eq:armijo} terminates in a finite amount of steps with a $t_k>0$ that satisfies \eqref{eq:armijo}. Moreover, one of the following holds
	\begin{itemize}
		\item[(a)] $t_k=s$
		\item[(b)] $t_k\leq \beta s$ such that $f(x_k +\frac{t_k}{\beta} d_k) > f(x_k) + \alpha \frac{t_k}{\beta} \grad(x_k)^Td_k$
	\end{itemize}
\begin{proof}
	Let us first prove that the algorithm terminates in a finite amount of steps. By contradiction there are no finite value of $i$ for which \eqref{eq:armijo} is satisfied, that is 
	\begin{equation*}
		\frac{f(x_k +s \beta^i d_k)- f(x_k)}{s \beta^i} > \alpha \grad(x_k)^Td_k.
	\end{equation*}
Given $\beta<1$ we have that $\lim_{i\to \infty} \beta^i =0$ and thus, with $i\to \infty$ the LHS of the inequality above is the directional derivative of $f$ along $d_k$. In particular, we get
\begin{equation*}
	\grad(x_k)^Td_k \geq \alpha\grad(x_k)^Td_k,
\end{equation*}
which is a contradiction, as $\grad(x_k)^Td_k<0$ and $\alpha <1$.
Following the steps of the algorithm, either the first guess $s$ is accepted or $t_k\leq s\beta$. In the second case, given $t_k$ the outcome of the algorithm, the step size before the last backtracking ($\frac{t_k}{\beta}$) was surely not accepted, from which (b) follows.
\end{proof}
	
\end{lemma}

\end{document}