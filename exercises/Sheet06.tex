\documentclass{ExerciseSheet}

%Set Number of the Exercise sheet and the submission deadline.
\setExerciseSheetNumber{6}
\setSubmissionDate{xx.xx.2024}

%boolean variable to determine whether the solutions should be included
\newif\ifsolutions
\solutionstrue
% \solutionsfalse

%We have a figure in this sheet
\usepackage{graphicx}

\begin{document}


%Start with exercises
%-----------------------------------------------------------------------%
\begin{problem}[Corollary 4.1]
    Let $f$ be $L$-smooth function. Prove that the iteration complexity of the gradient descent algorithm 
    \begin{itemize}
        \item on convex functions $f$ is $\mathcal{O}(\epsilon^{-1})$.
        \item on strongly convex $f$ is $\mathcal{O}(\log(\epsilon^{-1})).$
        \item on $\mu$-PL $f$ is $\mathcal{O}(\log(\epsilon^{-1})).$
    \end{itemize}
\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution} .

\begin{itemize}
    \item From Theorem 4.4, we have for $L$-smooth convex function
    \begin{align*}
        f(x_k)-f(x^*)\leq \frac{L\lVert x_0-x^*\rVert^2}{2k}.
    \end{align*}
    If $f(x_k)-f(x^*)>\epsilon$ for the first $k_\epsilon$, then it holds that
    \begin{align*}
         \frac{L\lVert x_0-x^*\rVert^2}{2k_\epsilon}> \epsilon \quad \Rightarrow \quad k_\epsilon <\frac{L\lVert x_0-x^*\rVert^2}{2\epsilon},
    \end{align*}
    which concludes the proof.
    \item By Lemma 2.3, we have 
    \begin{align*}
        f(x_k)-f(x_{k+1})\geq \frac{1}{2L}\lVert\nabla f(x_k)\rVert^2.
    \end{align*}
    Moreover, $\mu$-strongly convexity implies $\mu$-PL, i.e.,
    \begin{align*}
        \lVert \nabla f(x_k)\rVert^2 \geq 2\mu(f(x_k)-f(x^*))
    \end{align*}
    where $f(x^*) =\inf f(x)$. Combining the above two inequalities yields 
    \begin{align*}
        f(x_{k+1}) \leq f(x_k) -\frac{\mu}{L}(f(x_k)-f(x^*)).
    \end{align*}
    Subtracting both sides gives 
    \begin{align*}
        f(x_{k+1})-f(x^*) &\leq f(x_k)-f(x^*) -\frac{\mu}{L}(f(x_k)-f(x^*))\\
        & = \left(1-\frac{\mu}{L}\right)(f(x_k)-f(x^*))\\
        & \leq \left(1-\frac{\mu}{L}\right)^{k+1}(f(x_0)-f(x^*))
    \end{align*}
    Let $\delta:= 1-\frac{\mu}{L}$. Thus, we have
    \begin{align*}
        f(x_k)-f(x^*) &\leq \delta^k(f(x_0)-f(x^*)).
    \end{align*}
    If $f(x_k)-f(x^*) >\epsilon$ for the first $k_\epsilon$ iterations, then it holds that $\epsilon<\rho^{k_\epsilon}(f(x_0)-f(x^*))$. Taking a logarithm to both sides yields
    \begin{align*}
        \log(\epsilon) < k_\epsilon\log\left(\delta\right)+\log(f(x_0)-f(x^*)).
    \end{align*}
    Note that $\delta \in [0,1)$. Rearranging it gives
    \begin{align*}
        k_\epsilon < \frac{\log(\epsilon^{-1})+\log(f(x_0)-f(x^*))}{\log(\delta^{-1})}
    \end{align*}
    which completes the proof.

\end{itemize}
\end{solution}

\fi

% %-----------------------------------------------------------------------%
\vskip 0.5cm
\begin{problem}[Lemma 5.3]
    Let $f\in C^1(\R^n)$ be $\mu$-strongly convex. Prove that for all $x,y \in \R^n$, it holds that
    \begin{align*}
        f(y) \geq f(x) + \nabla f(x)^T(y-x)+\frac{\mu}{2}\lVert y-x\rVert^2.
    \end{align*}
\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
    Lemma 5.1 implies that strongly convex function $f$ satisfies $f(x)=g(x)+\frac{\mu}{2}\lVert x\rVert^2$ for convex function $g$. By the convexity of $g$, we obtain
    \begin{align*}
        g(y) \geq g(x) + \langle \nabla g(x),y-x\rangle.
    \end{align*}
    Since $g(x) = f(x) - \frac{\mu}{2}\lVert x\rVert^2$, we plug it into the above inequality
    \begin{align*}
        f(y) - \frac{\mu}{2}\lVert y\rVert^2 \geq f(x) - \frac{\mu}{2}\lVert x\rVert^2 + \langle \nabla f(x)-\mu x, y-x\rangle. 
    \end{align*}
    After rearranging it, we complete the claim.
\end{solution}

\fi

% %-----------------------------------------------------------------------%
\vskip 0.5cm

\begin{problem}[Lemma 5.4]
    Let $f\in C^2(\R^n)$ be $\mu$-strongly convex. Prove that for all $x\in \R^n$, for every eigenvalue $\lambda$ of $\nabla^2 f(x)$, it holds that $\lambda\geq \mu$. Moreover, if $f\in C^{1,1}_L(\R^n)$, for every eigenvalue $\lambda$ of $\nabla^2 f(x)$, prove that $\lambda\leq L$, i.e., $L \cdot I_d\succcurlyeq \nabla^2 f(x) \succcurlyeq \mu \cdot I_d$ where $I_d$ is the identity matrix.
\end{problem}
\ifsolutions
\vskip 0.3cm

\begin{solution}
    Since $f$ is $\mu$-strongly convex, we have $f(x)=g(x)-\frac{\mu}{2}\lVert x\rVert^2$ for convex function $g:\R^n \rightarrow \R$ by Lemma 5.1. Then, it holds that $\nabla ^2 f(x) = \nabla^2 g(x)+\mu I_d$. Since $g$ is convex, by Theorem 4.3, all eigenvalues of $g$ are non-negative. Thus, $\lambda \geq \mu$. On the other hand, since $f\in C^{1,1}_L(\R^n)$, by Theorem 2.1, we obtain $\lVert\nabla^2 f(x)\rVert \leq L$ for all $x\in\R^n$. Thus, for any non-zero eigenvector $v\in\R^n$, it holds that
    \begin{align*}
        \lVert\nabla^2 f(x)v\rVert\leq \lVert\nabla^2 f(x)\rVert\lVert v\rVert \leq L \lVert v\rVert
    \end{align*}
    and 
    \begin{align*}
        \lVert\nabla^2 f(x)v\rVert = \lVert \lambda v \rVert = |\lambda | \lVert v\rVert.
    \end{align*}
    Thus, we have $|\lambda | \leq L$.
\end{solution}

\fi

% %-----------------------------------------------------------------------%
\vskip 0.5cm
\begin{problem}[Lemma 6.2]
    Let $f\in C^{1,1}(\R^n)$ and $\mu>0$. Prove that if $f$ is strongly convex, the following hold for $x,y \in \R^n$:
    \begin{itemize}
        \item $f$ satisfies $\mu$-PL.
        \item $\lVert \nabla f(x)-\nabla f(y)\rVert\geq \mu \lVert x-y\rVert$.
        \item $f(y)\leq f(x)+\langle \nabla f(x), y-x\rangle +\frac{1}{2\mu}\lVert  \nabla f(y)- \nabla f(x)\rVert^2$.
        \item $\langle \nabla f(x)-\nabla f(y),x-y \rangle \leq \frac{1}{\mu}\lVert  \nabla f(y)- \nabla f(x)\rVert^2.$
    \end{itemize}
\end{problem}
\ifsolutions
\vskip 0.3cm

\begin{solution} . 

\begin{itemize}
    \item From Problem 2, $\mu$-strongly convex implies for all $x,y\in\R^n$,
    \begin{align*}
        f(y) \geq f(x) + \nabla f(x)^T(y-x)+\frac{\mu}{2}\lVert y-x\rVert^2.
    \end{align*}
    We minimize both sides with respect to $y$. The left-hand side immediately gives $\min f(y) = f(x^*)$. Now, we minimize the right-hand side of the inequality with respect to $y$. Define the function:
    \begin{align*}
        g(y):= f(x) + \nabla f(x)^T(y-x)+\frac{\mu}{2}\lVert y-x\rVert^2.
    \end{align*}
    The minimizer of $g(y)$ occurs at $y=- \frac{\nabla f(x)}{\mu}+x$, since the second derivative with respect to $y$ is positive. Plugging the minimizer yields
    \begin{align*}
        g(y) \geq f(x) - \frac{1}{2\mu}\lVert \nabla f(x)\rVert^2.
    \end{align*}
    Thus, it holds that 
    \begin{align*}
        f(x^*) \geq f(x) - \frac{1}{2\mu}\lVert \nabla f(x)\rVert^2,
    \end{align*}
    which completes the claim by substituting $x$ with $x_k$.
    \item Since $f$ is $\mu$-strongly convex, we have $f(x)=g(x)-\frac{\mu}{2}\lVert x\rVert^2$ for convex function $g:\R^n \rightarrow \R$ by Lemma 5.1. By convexity of $g$, it holds that for all $x,y\in\R^n$ and $x\not= y$,
    \begin{align*}
        \langle \nabla g(x) - \nabla g(y),x-y\rangle \geq 0.
    \end{align*}
    Plugging $g(x)=f(x)+\frac{\mu}{2}\lVert x\rVert^2$ into the above inequality yields 
    \begin{align} \label{eq: midstep}
        \langle \nabla f(x) - \nabla f(y),x-y\rangle   \geq \mu \lVert x-y\rVert^2.
    \end{align}
    Applying the Cauchy-Schwarz inequality to the left-hand side and dividing both sides by $\lVert x-y\rVert$ complete the proof.
    \item Define $\phi_x(z):=f(z)-\langle \nabla f(x),z\rangle$. Since $\phi_x(z)$ is $\mu$-strongly convex, we have with $z^*=x$
    \begin{align*}
        \phi_x(y) - \phi_x(x) \leq \frac{1}{2\mu}\lVert \nabla \phi_x(y)\rVert^2 = \frac{1}{2\mu}\lVert \nabla f(y)-\nabla f(x)\rVert^2.
    \end{align*}
    Moreover, it holds that
    \begin{align*}
         \phi_x(y) - \phi_x(x) = f(y)-\langle \nabla f(x),y\rangle - \left(f(x)-\langle \nabla f(x),x\rangle\right) = f(y)-f(x)-\langle \nabla f(x),y-x\rangle.
    \end{align*}
    Combining them together yields
    \begin{align*}
        f(y)\leq f(x)+\langle \nabla f(x), y-x\rangle +\frac{1}{2\mu}\lVert  \nabla f(y)- \nabla f(x)\rVert^2.
    \end{align*}
    To complete the proof, we will show that $\phi_x(z)$ is $\mu$-strongly convex. For $z, w \in \R^n$. it holds that
    \begin{align*}
        \langle \nabla \phi_x(z)-\nabla \phi_x(w),z-w \rangle = \langle \nabla f(z)-\nabla f(w),z-w \rangle \geq \mu \lVert z-w \rVert^2
    \end{align*}
    where the last inequality holds due to \eqref{eq: midstep} and it is equivalent to $\mu$-strongly convexity.
    \item Interchanging $x$ and $y$ in above claim yields
    \begin{align*}
        f(x)\leq f(y)+\langle \nabla f(y), x-y\rangle +\frac{1}{2\mu}\lVert  \nabla f(x)- \nabla f(y)\rVert^2.
    \end{align*}
    Adding the above inequality to the previous result completes the proof.
\end{itemize}
\end{solution}

\fi

% %-----------------------------------------------------------------------%
\vskip 0.5cm
\begin{problem}[Proposition 8.2]
    Let $\{x_k\}_k$ be the sequence generated by 
    \begin{align*}
        x_{k+1}=x_k-\frac{1}{\mu_k^{BB1}} \nabla f(x_k)
    \end{align*}
    where $f(x) = x^TQx+b^Tx+c$ with $Q$ symmetric and positive definite, with $\lambda_1<2\lambda_n$ and $\mu_k^{BB1} = \frac{s_k^TQs_k}{s_k^Ts_k}$.
    Prove that
    \begin{align*}
        \lVert \nabla f(x_k)\rVert \leq \delta^k\lVert \nabla f(x_0)\rVert
    \end{align*}
    with $\delta = \frac{\lambda_1}{\lambda_n}-1$.
\end{problem}
\ifsolutions
\vskip 0.3cm

\begin{solution}
    For the update $x_{k+1}=x_k-\frac{1}{\mu_k^{BB1}} \nabla f(x_k)$, it holds that
    \begin{align}
        \nabla f(x_{k+1}) &= Q x_{k+1}+c \nonumber \\
        & = Q\left(x_{k+1}=x_k-\frac{1}{\mu_k^{BB1}} \nabla f(x_k)\right)+c \nonumber\\
        & = Q x_k +c - \frac{1}{\mu_k^{BB1}} \nabla f(x_k)\nonumber\\
        & = \frac{1}{\mu_k^{BB1}}\left( \mu_k^{BB1}I_d - Q \right)\nabla f(x_k)\nonumber\\
        & \leq \prod_{j=0}^k \frac{1}{\mu_j^{BB1}}\left( \mu_j^{BB1}I_d - Q \right)\nabla f(x_0)\label{eq: apply} .
    \end{align}
    Let $v_i\in\R^n$ be the orthogonal eigenvectors of $Q$, associated to the eigenvalues $\lambda_i$. Then, for $\nabla f(x_0)$, there exists constant $\beta_1^0, \beta_2^0,\ldots,\beta_n^0$ such that
    \begin{align*}
        \nabla f(x_0) = \sum_{i=1}^n \beta_i^0 v_i.
    \end{align*}
    Plugging it into \eqref{eq: apply} yields
    \begin{align*}
        \nabla f(x_{k+1})&\leq \prod_{j=0}^k \frac{1}{\mu_j^{BB1}}\left( \mu_j^{BB1}I_d - Q \right)\sum_{i=1}^n \beta_i^0 v_i\\
        & = \sum_{i=1}^n\prod_{j=0}^k \frac{1}{\mu_j^{BB1}}\left( \mu_j^{BB1}I_d - Q \right)\beta_i^0 v_i\\
        & = \sum_{i=1}^n\prod_{j=0}^k \frac{1}{\mu_j^{BB1}}\left( \mu_j^{BB1} - \lambda_i \right)\beta_i^0 v_i\\
        & = \prod_{j=0}^k \frac{1}{\mu_j^{BB1}}\left( \mu_j^{BB1} - \lambda_i \right)\nabla f(x_0)\\
        & \leq \left( 1- \frac{\lambda_n}{\lambda_1}\right)^{k+1}\nabla f(x_0)
    \end{align*}
    where the last inequality holds due to $\mu_k^{BB1} \in [\lambda_n, \lambda_1]$. Thus, taking the norm to both sides completes the proof.
\end{solution}

\fi

% %-----------------------------------------------------------------------%

\end{document}