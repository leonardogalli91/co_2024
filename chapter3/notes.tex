\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage[left=2.50cm, right=2.50cm, top=2.0cm, bottom=2.0cm]{geometry}
\include{../macros}
\title{Continuous Optimization}
\author{Chapter 3: Constrained Optimization}
\date{}
\begin{document}
	\maketitle
\section{Definitions}
	In this chapter we will consider constrained optimization problems with the following shape
	\begin{equation}\label{eq:problem}
		\begin{split}
			\min \;\; &f(x)\\
			\st& x \in C
		\end{split}
	\end{equation}
\begin{definition}[Convex Set]
	A set $C$ is said to be convex if given $x_1,x_2\in C$ and $\lambda\in [0,1]$, then $\lambda x_1 +(1-\lambda) x_2 \in C.$
\end{definition}
\begin{definition}[Convex Function]
A function $f:C\to \R$ defined on a convex set $C$ is said to be convex if given $x_1,x_2\in C$ and $\lambda\in [0,1]$, then 
\begin{equation*}
	f(\lambda x_1 +(1-\lambda) x_2) \leq \lambda f(x_1) +(1-\lambda) f(x_2).
\end{equation*}
\end{definition}
\begin{definition}[Strictly Convex Function]
A function $f:C\to \R$ defined on a convex set $C$ is said to be strictly convex if given $x_1,x_2\in C$ and $\lambda\in [0,1]$, then 
\begin{equation*}
	f(\lambda x_1 +(1-\lambda) x_2) < \lambda f(x_1) +(1-\lambda) f(x_2).
\end{equation*}
\end{definition}
\noindent A function is called concave if $-f$ is convex and strictly concave if $-f$ is strictly convex.
\section{Characterizations of Convex Functions}
\begin{theorem}[Gradient characterization of convex functions]\label{thm:gradient_ineq}
	Let $f\in \C(C)$, where $C$ is convex. Then f is convex over $C$ if and only if
	\begin{equation}\label{eq:grad_ineq}
		f(x) +\grad(x)^T(y-x)\leq f(y) \quad \forall x, y\in C.
	\end{equation}
\end{theorem}
\begin{proof}
	Exercise.
\end{proof}
\begin{proposition}[Sufficiency of stationarity under convexity]\label{prop:stationarity}
	Let $f\in \C(C)$, where $C\subseteq\Rn$ is convex. Suppose that $\nabla f(x^*)=0$ for some $x^*\in C$. Then $x^*$ is a global minimizer of $f$ over $C$.
\end{proposition}
\begin{proof}
	Let $z\in C$. Plugging $x=x^*$ and $y=z$ in Theorem \ref{thm:gradient_ineq} we obtain that 
	\begin{equation*}
		f(z)\geq f(x^*) +\grad(x^*)^T(z-x^*),
	\end{equation*}
which implies that $f(z)\geq f(x^*) $ because $\grad(x^*)=0$.
\end{proof}
We note that Proposition \ref{prop:stationarity} establishes only the sufficiency of the stationarity condition $\grad(x^*) = 0$ for guaranteeing that $x^*$ is a global optimal solution. When $C$is not the
entire space, this condition is not necessary, in fact it might be that the points for which $\grad(\cdot)=0$ are not in $C$. On the other hand, when $C=\Rn$ and $f$ is convex, $\grad(x^*) = 0$ is both sufficient and necessary condition for $x^*$ to be a global minimum.
We can now establish the conditions under which a twice continuously differentiable function $f$ is convex.
\begin{theorem}[Second order characterization of convexity]
	Let $f\in \Cii(C)$, where $C\subseteq\Rn$ is convex and open. Thus, we have that $f$ is convex iff $\hess(x)\succcurlyeq0\quad \forall x\in C.$
\end{theorem}
\begin{proof}
	Suppose that $\hess(x) \succcurlyeq 0$ for all $x \in C$. We will prove \eqref{eq:grad_ineq} which is enough to establish convexity. Let $x,y\in C$, then by the Mean Value Theorem$^2$ we get that there exists $z\in[x,y]$ (and hence $z\in C$) for which 
	\begin{equation}\label{eq:mvt2_app}
		f(y) = f(x)+ \grad (x)^T(y-x) +\frac{1}{2}(y-x)^T\hess(z)(y-x).
	\end{equation}
Since $\hess(z) \succcurlyeq 0$, it follows that $(y-x)^T\hess(z)(y-x)\geq0$, which implies \eqref{eq:grad_ineq}.
\end{proof}
\section{Stationarity}

\begin{definition}[Stationary points of convex constrained problems]
	Let $f\in \C(C)$, where $C$ is closed and convex. Then $x^*$ is a stationary point of \eqref{eq:problem} if $\grad(x^*)(x-x^*)\geq 0 \; \forall x\in C.$ 
\end{definition}
\noindent In words, this means that there are no feasible descent directions of $f$ at $x^*$. This suggests that stationarity is in fact a necessary condition for a local minimum of \eqref{eq:problem}.
\begin{theorem}[Stationarity as necessary optimality condition]\label{thm:stationarity}
	Let $f\in \C(C)$, where $C$ is closed and convex and let $x^*$ be a local minimum of \eqref{eq:problem}. Then $x^*$ is a stationary point of \eqref{eq:problem}.
\end{theorem}
\begin{proof}
	Let $x^*$ be a local minimum of $f$ and assume by contradiction that is not a stationary point of \eqref{eq:problem}. Then there exists $x\in C$ such that $\grad(x^*)(x-x^*)< 0$. Therefore, $f'(x,d)<0$, where $d=x-x^*$. Hence, by Lemma 1.1 of Chapter 2, there exists $\epsilon\in(0,1)$ such that $f(x^*+td)<f(x^*)\;\forall t\in(0,\epsilon).$ Since $C$ is convex, we have that $x+td = (1-t)x^*+tx\in C$, leading to the conclusion that $x^*$ is not a local optimum of \eqref{eq:problem}, which is a contradiction.
\end{proof}

\begin{theorem}[Stationarity as necessary optimality condition]
	Let $f\in \C(C)$, where $C$ is closed and convex and $f$ is also convex. Let $x^*$ be a local minimum of \eqref{eq:problem}. Then $x^*$ is a stationary point of \eqref{eq:problem} iff $x^*$ is a optimal solution of \eqref{eq:problem}.
\end{theorem}
\begin{proof}
	The necessity of the stationarity condition follows from Theorem \ref{thm:stationarity}. To prove the sufficiency, assume that $x^*$ is a stationary point of \eqref{eq:problem} and let $x\in C$. Then
	\begin{equation*}
		s
	\end{equation*}
\end{proof}

\bibliographystyle{plain}
\bibliography{../biblio}
\end{document}