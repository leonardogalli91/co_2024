\documentclass{ExerciseSheet}

%Set Number of the Exercise sheet and the submission deadline.
\setExerciseSheetNumber{4}
\setSubmissionDate{xx.xx.2024}

%boolean variable to determine whether the solutions should be included
\newif\ifsolutions
\solutionstrue
%\solutionsfalse

%We have a figure in this sheet
\usepackage{graphicx}

\begin{document}


%Start with exercises
%-----------------------------------------------------------------------%


%\subsection*{Notation}
a\vskip 0.5cm 
%-----------------------------------------------------------------------%

\begin{problem}
Suppose that $f \in C^{1,1}_L(\Rn)$ and assume that $\nabla^2 f(x) \succcurlyeq 0$ for any $x \in \Rn$. Suppose that the optimal value of the problem 
\begin{align*}
    \min_{x\in\Rn} f(x)
\end{align*} 
is $f^*$. Let $\{ x_k\}_{k \geq0}$ be the sequence generated by the gradient method with constant stepsize $\frac{1}{L}$. \vspace{0.3cm}

Show that if $\{ x_k\}_{k \geq0}$ is bounded, then $f(x_k) \to f^*$ as $k \to \infty$.

\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
Let us start by proving that $\{f(x_k)\}_k$ is a decreasing sequence. The Taylor expansion of $f$ yields
\begin{align*}
    f(x_{k+1}) = f(x_k)+ \big< \nabla f(x_k), x_{k+1}-x_k \big> +\frac{1}{2} \big<(x_{k+1}-x_k)^T \nabla^2 f(z_{\xi}), x_{k+1}-x_k \big>
\end{align*}
where $z_{\xi}=x_k + \xi(x_{k+1}-x_k)$ with $\xi\in [0, 1].$
substituting the schematic of gradient method with constant  stepsize $\eta=\frac{1}{L},$ yields

  \begin{align}
    f(x_{k+1}) &= f(x_k)+ \big< \nabla f(x_k), -\eta\nabla f(x_k) \big> +\frac{1}{2}\eta^2 \big<(\nabla f(x_k))^T \nabla^2 f(z_{\xi}), \nabla f(x_k) \big>\nonumber\\
    &\leq f(x_k) -\eta\left\|\nabla f(x_k)\right\|^2 +\frac{1}{2}\eta^2\left\|\nabla^2 f(z_{\xi})\right\|\left\|\nabla f(x_k)\right\|^2 \label{hess estimate}
 \end{align}
where we applied Cauchy-Schwarz to get the above inequality. Furthermore, $f \in C^{1,1}_L(\Rn),$ gives by Theorem 2.1 (of Chapter 2 of the lecture note) $\left\|\nabla^2 f(z_{\xi})\right\|\leq L=\frac{1}{\eta}.$ Substituting the into the above into $\eqref{hess estimate}$ yields

  \begin{align*}
    f(x_{k+1})&\leq f(x_k) -\eta\left\|\nabla f(x_k)\right\|^2 +\frac{1}{2}\eta\left\|\nabla f(x_k)\right\|^2\\
    &=f(x_k) -\frac{1}{2}\eta\left\|\nabla f(x_k)\right\|^2
 \end{align*}
that is
 \begin{align}
   0\leq \frac{1}{2}\eta\left\|\nabla f(x_k)\right\|^2&\leq f(x_k) - f(x_{k+1}) \label{decreasing f}
 \end{align}
therefore,  $\{f(x_k)\}_k$ is decreasing. In addition, $f$ is bounded below by $f^*$ as this is the optimal value of the minimization problem, hence, $\{f(x_k)\}_k$ converges. It now remains to show that the $\displaystyle \lim_{k\to\infty}f(x_k)=f^*.$ Given that,  $\{ x_k\}_{k \geq0}$ is bounded in $\Rn$ by Bolzanoâ€“Weierstrass theorem its admit a limit point $\displaystyle\lim_{k\to\infty}x_k=x^*$.\\
Let us prove $\displaystyle\lim_{k\to\infty}x_k=x^*$ is a critical point.
 Summing up $\eqref{decreasing f}$ from $0$ to $n$ yields
  \begin{align*}
      \frac{1}{2}\eta\sum_{k=0}^{n}\left\|\nabla f(x_k)\right\|^2&\leq f(x_0) - f(x_{n+1})
  \end{align*}
  where we observed that the right hand side is telescoping sums. Since $f^*\leq f(x_{k+1}),$ it follows,

  \begin{align*}
      \eta\sum_{k=0}^{n}\left\|\nabla f(x_k)\right\|^2&\leq f(x_0) - f^*< \infty     
  \end{align*}
therefore,  $\displaystyle \lim_{k\to\infty}\left\|\nabla f(x_k)\right\|=0$ thus
 $\displaystyle \lim_{k\to\infty}\nabla f(x_k)=0$ moreover, the continuity of $\nabla f$ gives
 \begin{align*}
     \nabla f (\lim_{k\to\infty}x_k)= \lim_{k\to\infty}\nabla f(x_k)=0
 \end{align*}
hence, $\displaystyle\lim_{k\to\infty}x_k=x^*$ is a critical point. Given that,  $\nabla^2 f(x) \succcurlyeq 0$ for any $x \in \Rn,$ then $x^*$ is the unique optimal point that is $f(x^*)=f^*.$ Since $f$ is continuous, 
\begin{align*}
    \lim_{k\to\infty}f(x_k)= f(\lim_{k\to\infty}x_k)=f^*
\end{align*}
\end{solution}

\fi
%%----------------------------------------------------------------

\begin{problem}
In this problem we want to work on the exact line search. 

As a reminder exact line search is used to determine the step size in the Descent Direction Method.  
So suppose we have a function $f:\R^n \rightarrow \R$ and a time step $k$ and a descent direction $d_k$, then we choose the step size as the minimizer along this direction,  i.e.
\begin{equation*}
    t_k \in \argmin_{t>0} f(x_k+td_k)
\end{equation*}
\renewcommand{\labelenumi}{\alph{enumi})}
We start with an example. 
\begin{enumerate}
    \item Let $A\in \R^{n\times n}$ be positive definite, $b\in \R^n$ and $c\in \R$. Define the function $f:\R^n \rightarrow \R$ by $$f(x)=x^TAx + 2b^Tx +c .$$ Let $d\in \R^n$ be a descent direction of $f$ at a point $x$. What is the result of the exact line search? 
\end{enumerate}
\noindent Now we do the analytical analysis.
\begin{enumerate}[resume]
    \item Let $f\in C^{1,1}_L(\R^n)$ and $(x_k)_{k\in \N}$ be a sequence generated by the gradient method for solving $\min_{x\in \R^n} f(x)$ with the exact line search determining the stepsize. \\
    Show that \begin{equation*}
        f(x_k) - f(x_{k+1}) \geq \frac{1}{2L} \|\nabla f(x_k)\|_2^2.
    \end{equation*}
\end{enumerate}
\end{problem}
\ifsolutions
\vskip 0.3cm
\begin{solution}
    \renewcommand{\labelenumi}{\alph{enumi})}

\begin{enumerate}
    \item Set $g:\R \rightarrow \R$, $g(t)=f(x+td)$. Now the goal is to minimizie (the one dimensional function) g. We reformulate
    \begin{align*}
        g(t) &= (x+td)^TA (x+td) + 2b^T(x+td) + c \\
        &= t^2d^TAd + 2t \left(d^TAx + b^Td\right) + x^TAx + 2b^Tx + c \\
        &=t^2d^TAd + 2t \left(d^TAx + b^Td\right) + f(x)
    \end{align*}
    Now 
    \begin{align*}
        g'(t)&= 2d^TAd t + 2\left(d^TAx + b^Td\right).
    \end{align*}
    Setting the derivative to zero yields
    \begin{equation}
        g'(t) = 0 \Leftrightarrow t = -\frac{d^TAx + b^Td}{d^TAd},
    \end{equation}
    where we use that $A$ is positive definite and $d\neq 0$
    Question: Why is $t>0$, descent direction!! \\ 
    Now we calculate the second derivative
    \begin{equation*}
        g''(t)=2d^TAd >0.
    \end{equation*}
%    \textbf{Also discuss when the solution is unique etc in the tutorial!!}
    \item By the Descent Lemma (Lemma 2.3 in the lecture notes) we know
    \begin{equation*}
        f(x)-f(x-t\nabla f(x)) \geq t\left(1-\frac{Lt}{2}\right) \|\nabla f(x)\|^2, \quad \forall t\geq 0.
    \end{equation*}
    Let $t_k \in \argmin_{t>0} f(x_k +t\nabla f(x_k))$, and define then $x_{k+1}=x_k - t_k \nabla f(x_k)$. By the definition of $t_k$ we have $f(x_{k+1})\leq f(x_k - \frac{1}{L}\nabla f(x_k))$ and hence
    \begin{equation*}
        f(x_k) - f(x_{k+1}) \geq f(x_k) - f(x_k - \frac{1}{L}\nabla f(x_k)) \geq \frac{1}{L}(1-\frac{1}{2}) \|\nabla f(x_k)\|^2.
    \end{equation*}
 \end{enumerate}
\end{solution}

\fi
%--------------------------------------------------------------------
\vskip 0.5cm
\begin{problem}
    
 \textit{Let $f$ be a twice continuously differentiable function over $\mathbb{R}^n $. Then the following two claims are equivalent:}

\begin{itemize}
    \item[(a)] $f \in C^{1,1}_L(\mathbb{R}^n)$.
    \item[(b)] $ \|\nabla^2 f(x)\| \leq L \text{ for any } \mathbf{x} \in \mathbb{R}^n$.
\end{itemize}
\textbf{Note:} $(b) \implies (a)$ has been proven in class. Do for the case where $(a) \implies (b).$
\end{problem}

\ifsolutions
\vskip 0.3cm
\begin{solution}

\textbf{(a) $\Rightarrow$ (b).} Suppose now that $f \in C^{1,1}_L$. Then by the fundamental theorem of calculus, for any $\mathbf{d} \in \mathbb{R}^n$ and $\alpha > 0$, we have
\[
\nabla f(x + \alpha \mathbf{d}) - \nabla f(x) = \int_0^{\alpha} \nabla^2 f(x + t \mathbf{d}) \mathbf{d} \, dt.
\]

Thus,
\[
\left\| \left( \int_0^{\alpha} \nabla^2 f(x + t \mathbf{d}) \, dt \right) \mathbf{d} \right\| = \| \nabla f(x + \alpha \mathbf{d}) - \nabla f(x) \| \leq \alpha L \|\mathbf{d}\|.
\]

Dividing by $\alpha$ and taking the limit $\alpha \to 0^+$, we obtain
\[
\left\| \nabla^2 f(x) \mathbf{d} \right\| \leq L \|\mathbf{d}\|,
\]
implying that $\|\nabla^2 f(x)\| \leq L$. \hfill $\square$

\end{solution}
\fi









%------------------------------------------------------------------
\vskip 0.5cm
\begin{problem}
    \textbf{(Convergence of GD with Armijo Line Search (part 1))}. 
Let $ f \in \mathcal{C}^1(\mathbb{R}^n) $ and let $ \{x_k\}_k $ be a sequence generated by Algorithm 3. 
Assume that $f $ is bounded from below over $\mathbb{R}^n $. Then we have the following

\begin{itemize}
    \item[(a)] The sequence $ \{f(x_k)\}_k $ is nonincreasing.
    \item[(b)] $t_k \nabla f(x_k) \to 0 $ as $k \to \infty $.
\end{itemize}
\end{problem}
%-----------------------------------------------------------------------%
\ifsolutions
\vskip 0.3cm
\begin{solution}
Let $\{x_k\}$ be the sequence generated by gradient descent with Armijo line search, i.e.,
\[
x_{k+1} = x_k - t_k \nabla f(x_k),
\]
where $t_k > 0$ satisfies the Armijo condition:
\[
f(x_k - t_k \nabla f(x_k)) \leq f(x_k) - \alpha t_k \|\nabla f(x_k)\|^2,
\]
for some fixed $\alpha \in (0,1)$.
\begin{enumerate}
    \item \textbf{ Monotonicity:}  


By the Armijo condition applied at each step, we have:
\[
f(x_{k+1}) \leq f(x_k) - \alpha t_k \|\nabla f(x_k)\|^2.
\]
Thus, $f(x_{k+1}) \leq f(x_k)$ for all $k$, so the sequence $\{f(x_k)\}$ is nonincreasing.

\item \textbf{ Vanishing gradient step:}  
From part (1), since $\{f(x_k)\}$ is nonincreasing and $f$ is bounded from below, the sequence $\{f(x_k)\}$ converges to some $f^* > -\infty$. 

%Since $ f(x_k) $ is strictly decreasing and bounded below, it converges to some limit $ f^* \in \mathbb{R} $.

%\[
%\lim_{k \to \infty} \nabla f(x_k) = 0 \quad \Rightarrow %\quad \text{Every accumulation point } x^* \text{ satisfies %} \nabla f(x^*) = 0
%\]%
\vspace{0.5cm}
Let $ \phi(t) = f(x_k + t d_k) $. By the mean value theorem, for some $ \theta_k \in (0, t_k)$, we have:

\[
f(x_k + t_k d_k) - f(x_k) = t_k \phi'(\theta_k)
\]

where:

\[
\phi'(t) = \nabla f(x_k + t d_k)^T d_k
\]

Hence:

\[
f(x_k + t_k d_k) - f(x_k) = t_k \nabla f(x_k + \theta_k d_k)^T d_k
\]

By the Armijo condition:

\[
f(x_k + t_k d_k) - f(x_k) \leq \alpha t_k \nabla f(x_k)^T d_k
\Rightarrow \nabla f(x_k + \theta_k d_k)^T d_k \leq \alpha \nabla f(x_k)^T d_k
\]

Recall $ d_k = -\nabla f(x_k) $, so:

\[
\nabla f(x_k)^T d_k = -\|\nabla f(x_k)\|^2 < 0
\]

Both sides are negative, and this gives a bound on how small the gradient can remain unless $ t_k \to 0 $. By the backtracking rule, we cannot make $t_k$ too small unless $||\nabla f(x_k)|| \to 0$




Hence, the telescoping sum:
\[
\sum_{k=0}^\infty \alpha t_k \|\nabla f(x_k)\|^2 \leq f(x_0) - f^* < \infty.
\]
implies that the series $\sum_{k=0}^\infty t_k \|\nabla f(x_k)\|^2$ converges. Therefore,
\[
t_k \|\nabla f(x_k)\|^2 \to 0.
\]
Taking square roots (and using $t_k > 0$), we get:
\[
t_k \|\nabla f(x_k)\| \to 0,
\]
which proves that $t_k \nabla f(x_k) \to 0$ as  $k$ tends to $\infty$.
\end{enumerate}
\hfill $\blacksquare$
\end{solution}
\fi
%-----------------------------------------------------------------------%
\vskip 0.5cm
\begin{problem}

    Let $f \in C^{1,1}_L(\R^m)$, and let $A \in \R^{m\times n}, ~b\in \R^m$.\\
Show that the function 
 \begin{align*}
     g : \Rn \to \R \text{ defined by } g(x) = f(Ax+b)
 \end{align*}
 satisfies $g \in C^{1,1}_{\Tilde{L}}(\Rn)$, where the Lipschitz constant is given as $\Tilde{L} = \|A\|^2L.$

\end{problem}
\ifsolutions
\vskip 0.3cm
\begin{solution}
Let $x,y\in\Rn$ Let us prove that 
\begin{align*}
    \|\nabla g(x) - \nabla g(y) \| \leq \Tilde{L} \|x - y \|.
\end{align*}
It follows from the definition of $g$ that
\begin{align*}
    \|\nabla g(x) - \nabla g(y) \| &= \|A^T\nabla f(Ax+b) - A^T\nabla f(Ay+b) \| \\
    &\leq L \|A^T\| \|Ax+b -(Ay+b)\|\\
    &= L\|A^T\|\|A(x-y)\|\\
    &\leq L\|A\|^2\|x-y\|
\end{align*}
where we used the fact that $f \in C^{1,1}_L(\R^m)$ to get the first inequality and Cauchy-Schwarz to get the last inequality.
\end{solution}
\fi

\vskip 0.5cm

\begin{problem}
Let $\{x_k\}_{k\in \N}$ be a sequence generated by gradient method with exact line search for solving a problem of minimizing a continuously differentiable function f. Then for any $k$
\begin{equation*}
    \left(x_{k+2} - x_{k+1}\right)^T\left(x_{k+1}-x_k\right) = 0
\end{equation*}
\end{problem}

\ifsolutions
\vskip 0.3cm
\begin{solution}
%We have $x_{k+1}-x_k=-t_k \nabla f(x_k)$ for all $k$, hence the %above simplifies to 
%\begin{equation*}
 %   t_{k+1}t_k\nabla f(x_{k+1})^T \nabla f(x_k) = 0,
%\end{equation*}
%which is the case (since $t_k>0$) iff 
%\begin{equation*}
 %  \nabla f(x_{k+1})^T \nabla f(x_k) = 0,
%\end{equation*}
%We know $t_k \in \argmin_{t > 0} g(t):=f(x_k - t \nabla %f(x_k))$, so $0= g'(t_k)= -\nabla f(x_k)^T \nabla f(x_k - t_k %\nabla f(x_k))= - \nabla f(x_k)^T \nabla f(x_{k+1})$


The update rule for gradient descent is given by:
\[
    x_{k+1} = x_k - \alpha_k \nabla f(x_k),
\]
where $\alpha_k > 0$ is the step size chosen by exact line search:
\[
    \alpha_k = \arg\min_{\alpha > 0} f(x_k - \alpha \nabla f(x_k)).
\]

Then we compute:
\[
    x_{k+1} - x_k = -\alpha_k \nabla f(x_k),
\]
\[
    x_{k+2} - x_{k+1} = -\alpha_{k+1} \nabla f(x_{k+1}).
\]

So their inner product is:
\begin{align*}
    (x_{k+2} - x_{k+1})^T (x_{k+1} - x_k)
    &= (-\alpha_{k+1} \nabla f(x_{k+1}))^T (-\alpha_k \nabla f(x_k)) \\
    &= \alpha_k \alpha_{k+1} \nabla f(x_{k+1})^T \nabla f(x_k).
\end{align*}

Now, using the property of exact line search, we know that $\alpha_k$ minimizes the function
\[
    \phi(\alpha) = f(x_k - \alpha \nabla f(x_k)).
\]

Hence, the derivative of $\phi$ at $\alpha_k$ vanishes:
\[
    \phi'(\alpha_k) = -\nabla f(x_k - \alpha_k \nabla f(x_k))^T \nabla f(x_k) = 0.
\]

But $x_{k+1} = x_k - \alpha_k \nabla f(x_k)$, so:
\[
    \nabla f(x_{k+1})^T \nabla f(x_k) = 0.
\]

Substituting this into our earlier expression:
\[
    (x_{k+2} - x_{k+1})^T (x_{k+1} - x_k) = \alpha_k \alpha_{k+1} \cdot 0 = 0.
\]

\hfill $\blacksquare$
\end{solution}

\fi
%-----------------------------------------------------------------------%
% \vskip 0.5cm
% \begin{exo}
	
% \end{exo}

% \ifsolutions
% \vskip 0.3cm
% \begin{solution}
% \end{solution}
% \fi

\end{document}