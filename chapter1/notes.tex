\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=2.50cm, right=2.50cm, top=2.0cm, bottom=2.0cm]{geometry}
\include{../macros}
\title{Continuous Optimization}
\author{Chapter 1: Basics of Optimization}
\date{}
\begin{document}
	\maketitle
	\section{Optimization problems}
	In this course we will focus on finite-dimensional continuous optimization problems
	\begin{equation}\label{eq:problem}
	\begin{split}
		\min \;\; &f(x)\\
		\st&g_i(x) \leq 0 \quad i=1, \dots, m\\
		& h_j(x) = 0 \quad j=1, \dots, p
	\end{split}
	\end{equation}
\begin{itemize}
	\item $x \in \Rn$: vector of variables to be chosen ($n$ scalar variables $x_1 , \dots , x_n$)
	\item $f$: objective function to be minimized
	\item $g_1,\dots, g_m$: inequality constraint functions to be satisfied
	\item $h_1,\dots, h_p$: equality constraint functions to be satisfied
\end{itemize}
The feasible set can also be denoted by $S=\{x \in \Rn: g_i(x) \leq 0 \; i=1, \dots, m, \;\; h_j(x) = 0 \; j=1, \dots, p\}$.
\subsection{Classification}
Concerning the dimensionality, optimization problems can be 
\begin{itemize}
	\item infinite-dimensional: when the space of variables is infinite dimensional, e.g., elements can be vectors of functions of one ore more variables;
	\item finite-dimensional: when the space of variables is finite-dimensional, e.g., $\Rn.$
\end{itemize}
Concerning the granularity of the space, optimization problems can be 
\begin{itemize}
	\item discrete: when at least one variable is an integer;
	\item continuous: when all variables involved are continuous.
\end{itemize}
Concerning the hypothesis of linearity, optimization problems are also called
\begin{itemize}
\item linear programming: when the objective function is linear and the
feasible set is defined by a system of linear equalities and inequalities;
\item nonlinear problems: when the objective function is nonlinear
and/or at least one constraint is defined by a nonlinear function.
\end{itemize}
Concerning the hypothesis of differentiability, we can distinguish between
\begin{itemize}
\item smooth optimization: when the functions defining the objective
and the constraints are continuously differentiable;
\item nonsmooth optimization problems: when the functions defining the objective and the constraints are continuous, but typically not differentiable in all points of the feasible set. Weaker notions of derivatives are used to solve this problem.
\item derivative-free optimization: this branch of optimization consider cases in which evaluating the objective function/constraints is very costly, in particular the functions defining the objective and the constraints might not be continuous. A similar field is that of black-box optimization, in which the optimizer assumes to not have any information on the objective function or on the constraints.
\end{itemize}
Concerning the hypothesis of convexity, we can distinguish between
\begin{itemize}
	\item convex optimization, when the functions defining the objective and the constraints are all convex;
	\item nonconvex optimization, when the functions defining the objective and the constraints might be nonconvex.
\end{itemize}

\begin{example}[The transportation problem (Linear Programming)]
Consider a company producing and selling a certain commodity. The company has a set of $N$ sources, where the commodity is produced and a set of $M$ demand centers where the commodity is sold. At each source $i \in \{1, \dots, N\}$, a given quantity $q_i$ of commodity is available. Each demand center $j \in \{1, \dots, M\}$ requires a given quantity $d_j$. We indicate by $c_{ij}$ the cost per unit of transporting the commodity from source $i$ to demand center $j$.

\par The problem is to determine the quantity to be transported from each source to
each demand center in such a way that
\begin{itemize}
	\item the availability constraints at the sources are satisfied;
	\item the requirements of the demand centers are satisfied;
	\item the total transportation cost is minimized.
\end{itemize}
To formulate the problem, we indicate by $x_{ij}$ the quantity of the commodity transported from source $i$ to demand center $j$. Then the optimization problem is

\begin{equation*}
	\begin{split}
		\min \;\; &\sum_{i=1}^{N}\sum_{j=1}^{M}c_{ij}x_{ij}\\
		\st&\sum_{j=1}^{M}x_{ij} \leq q_i\quad i=1, \dots, m\\
		& h_j(x) = 0 \quad j=1, \dots, p
	\end{split}
\end{equation*}
%TODO: remark for myself, draw the network
\end{example}
\begin{example}[Training of Neural Networks (Unconstrained Optimization)] Let us start from describing a single (artificial) neuron. In particular, this unit takes as an input a vector and performs a scalar product with the weights of the neuron. In a second step, this value passes through a thresholding function $\sigma$ called activation function. Inspired by real neurons, also this unit propagates its output only if the weighted sum of its inputs (in the natural neuron, the potential difference)  is over a certain threshold. Neural networks stack various neurons in parallel to build a layer and, in turn, concatenate various layer (deep networks). This model is very expressive, in fact already a wide-enough 2-layer neural network is able to approximate any possible function of the input.
	\par Neural networks are the most widely used machine learning model and they can be formally defined as follows. 
	\begin{itemize}
		\item $\mathcal{D}=\{(x_i, y_i)\}_{i=1}^{M}$ be a training set of $M$ instances
		\begin{itemize}
			\item $x_i \in \mathbb{R}^{d}$ $d$ is the amount of features of the dataset (e.g., pixels)
			\item $y_i\in\mathbb{R}$
		\end{itemize}
		\item the $j$-th layer applied on a generic input $n_{j-1}$-dimensional input $x$:\\
		$g_j(x) := \sigma (W_j x +b_j )$, $W_j \in \R^{n_{j-1}\times n_j}$, $b_j \in \R^{n_j}$, $\sigma$ is applied component-wise,\\ e.g., ReLU $\sigma(x)= \begin{cases} 0 & \text{if} \;\;x<0\\ x &\text{otherwise} \end{cases}$ %(ReLU)
		\item the weights: $w = \begin{pmatrix}
			\text{vec}(W_1)\\
			\vdots\\
			\text{vec}(W_L)
		\end{pmatrix}\in \mathbb{R}^{n}$
		\item the network applied on the $i$-th instance: $h_i(w):=g_L \circ \dots \circ g_1(x_i)$ 
		\item the losses: $f_i(w):=\mathcal{L}(h_i(w),y_i)$, e.g., $\mathcal{L}(h_i(w),y_i)= (h_i(w)-y_i)^2$%(sofmax)
	\end{itemize}
	
	\begin{equation*}
		\min_{w\in \mathbb{R}^n} f(w) = \frac{1}{M}\sum_{i=1}^{M} f_i(w)
	\end{equation*}
%TODO: remark for myself, draw the network
\end{example}
\begin{example}[Portfolio Optimization (Convex Quadratic Programming)] Portfolio selection theory studies how to allocate an investorâ€™s available capital into a prefixed set of assets with the aims of maximizing the expected return and minimizing the investment risk. Let $n$ be the number of available assets, let $\mu\in \Rn$ be the vector of expected returns of the assets, and let $Q \in \Rnn$ be a symmetric positive semidefinite matrix whose generic element $q_{ij}$ is the covariance of returns of assets i and j. We assume that the available (normalized) capital is fully invested. Then, let $x \in \Rn$ be the vector of decision variables, where $x_i$ is the fraction of the available capital to be invested into asset $i$, with $i = 1, \dots, n$. 
\par By this notation, $\mu^T x$ is the expected return of the portfolio and $x^TQx$ is the covariance of the portfolio which
can be used as a measure of the risk connected with the investment (diversification of the portfolio). 
In the traditional Markowitz portfolio selection model \cite{markowits52a}, the optimization problem is stated as the
following convex quadratic programming problem
\begin{equation*}
	\begin{split}
		\min \;\; &x^TQx\\
		\st& \mu^Tx\geq \beta \\
		& e^Tx =1\\
		& x\geq 0,
	\end{split}
\end{equation*}
where $\beta$ is the desired expected return of the portfolio and $e\in \Rn$ denotes the column vector of all ones.
\end{example}
\begin{example}[Shortest Path (Discrete Optimization, not covered by this course)] Let $G=(V,E)$ be a graph with $V$ the set of vertices and $E$ the set of edges (directed, $(i,j)\neq(j,i)$, with $i,j$ being two vertices) with $n=|E|$. Let $c_{ij}\geq0$ be the cost of choosing the edge $(i,j)$, e.g., travel time, travel fee, difference in hight, and let $x_{ij}\in \{0,1\}^{n}$ be the variable telling if we select or not the edge $(i,j)$. Also, let $s$ be the starting node and $d$ the destination, then the optimization problem can be stated as
\begin{equation*}
	\begin{split}
		\min \;\; &c^Tx\\
		\st& x\geq 0\\
		& \sum_{j} x_{ij} - \sum_{j} x_{ji} = \begin{cases}1 \quad & i=s\\
			0 & i\neq s,d\\
			-1 & i=d
		\end{cases} \qquad \forall i \in V
	\end{split}
\end{equation*}
%TODO: remark for myself, draw the network
\end{example}

\section{Mathematical Preliminaries}

\subsection{The space $\Rn$}
The vector space $\Rn$  is the set of $n$-dimensional column vectors with real components endowed with the component-wise addition operator
\begin{equation*}
	\begin{pmatrix}
		x_1\\
		x_2\\
		\vdots\\
		x_n
	\end{pmatrix}
+
\begin{pmatrix}
	y_1\\
	y_2\\
	\vdots\\
	y_n
\end{pmatrix}
= 
\begin{pmatrix}
	x_1+y_1\\
	x_2+y_2\\
	\vdots\\
	x_n + y_n
\end{pmatrix}
\end{equation*}
and the scalar product
\begin{equation*}
	\lambda \begin{pmatrix}
		x_1\\
		x_2\\
		\vdots\\
		x_n
	\end{pmatrix}
= 
\begin{pmatrix}
	\lambda x_1\\
	\lambda x_2\\
	\vdots\\
	\lambda x_n
\end{pmatrix}
\end{equation*}
The standard basis of $\Rn$ is $e_1, e_2, \dots, e_n$, where $e_i$ is the $n$-length column vector whose $i$th component is 1 while all the others are 0.\\
The nonnegative orthant $\Rn_+:=\{(x_1,x_2, \dots, x_n)^T: x_1, x_2,\dots,x_n\geq 0\}$.\\
The positive orthant $\Rn_{++}:=\{(x_1,x_2, \dots, x_n)^T: x_1, x_2,\dots,x_n> 0\}$.\\
The set of all real-valued matrices of order $m \times n$ is denoted by $\R^{m\times n}$.\\
The identity matrix will be denoted by $\Id$, where its dimension will be clear from the context.
\subsection{Inner Products}
\begin{definition}[Inner product] An inner product on $\Rn$ is a map $\inprod{\cdot}{\cdot}: \Rn \times \Rn \to \R$
	with the following properties
	\begin{enumerate}
\item Symmetry: $\inprod{x}{y} = \inprod{y}{x} \quad \forall x, y \in \Rn$;
\item Additivity: $\inprod{x}{y+z} = \inprod{x}{y} + \inprod{x}{z}  \quad \forall x, y,z \in \Rn$;
\item Homogeneity: $\inprod{\lambda x}{y} = \lambda \inprod{x}{y}\quad \forall x, y \in \Rn$;
\item Positive definiteness: $\inprod{x}{x} \geq 0 \quad \forall x \in \Rn$ and $\inprod{x}{x} = 0 \Leftrightarrow x =0.$
	\end{enumerate}
\end{definition}
In this course the inner product we will use is the standard dot product:
\begin{equation*}
	\inprod{x}{y} = x^T y = \sum_{i=1}^{n}x_i y_i \quad \forall x,y\in \Rn
\end{equation*}
Notice that the dot product is not the only inner product in $\Rn$, for instance let $w\in \Rn_{++},$ it is easy to show that the following weighted dot product is also an inner product
\begin{equation*}
\langle x, y\rangle_w = \sum_{i=1}^{n}w_i x_i y_i.
\end{equation*}
\subsection{Vector norms}
\begin{definition} (Vector Norm)
	A norm $||.||$ on $\Rn$ is a function $||.||:\Rn \to \R$ satisfying the following:
	\begin{enumerate}
		\item Nonnegativity: $||x|| \geq 0 \quad \forall x \in \Rn$ and $||x|| = 0 \Leftrightarrow x =0$;
		\item Positive Homogeneity:  $||\lambda x|| = |\lambda| ||x|| \quad \forall x \in \Rn$ and $\lambda\in \R$;
		\item Triangle Inequality: $||x+y|| \leq ||x|| + ||y|| \quad \forall x,y \in \Rn$.
	\end{enumerate}
\end{definition}
One natural way to generate a norm on $\Rn$ is to take any inner product $\inprod{\cdot}{\cdot}$ on $\Rn$ and define the associated norm
\begin{equation*}
	||x||:= \sqrt{\inprod{x}{x}} \quad \forall x \in \Rn
\end{equation*}
which can be proved to be a norm. If the inner product is the dot product, then the associated norm is the Euclidean norm or $l_2$ norm, i.e., $||\cdot||_2$. In the rest of the course, the default norm will be the Euclidean norm and the subscript $2$ will be omitted. This norm belongs to the class of $l_p$ norms (for $p\geq 1$, with $0\leq p<1$ these are quasi-norms) defined by 
\begin{equation*}
	||x||_p:= \sqrt[p]{\sum_{i=1}^{n}|x_i|^p}.
\end{equation*}
By computing the limit of the $l_p$-norm with $p\to\infty$ we achieve the $l_{\infty}$ norm, i.e.,
\begin{equation*}
	||x||_{\infty} := \max_{i\in[n]} |x_i|.
\end{equation*}
where $[n]:=\{1, \dots,n\}$ for any $n \in \N$.
\par An important inequality connecting the dot product of two vectors and their norms is the Cauchyâ€“Schwarz inequality, which will be used frequently throughout the book.
\begin{lemma}[Cauchyâ€“Schwarz inequality]
	For any $x,y \in \Rn$,
	\begin{equation*}
		|x^Ty| \leq ||x||\cdot||y||.
	\end{equation*}
Equality is satisfied if and only if $x$ and $y$ are linearly dependent.
\end{lemma}
\subsection{Matrix norms}
Similarly to vector norms, we can define the concept of a matrix norm.
\begin{definition} (Operator/Matrix Norm)
	A norm $||.||$ on $\Rmn$ is a function $||.||:\Rmn \to \R$ satisfying the following:
	\begin{enumerate}
		\item Nonnegativity: $||A|| \geq 0 \quad \forall A \in \Rmn$ and $||A|| = 0 \Leftrightarrow A =0$;
		\item Positive Homogeneity:  $||\lambda A|| = |\lambda| ||A|| \quad \forall A \in \Rmn$ and $\lambda\in \R$;
		\item Triangle Inequality: $||A+B|| \leq ||A|| + ||B|| \quad \forall A,B \in \Rmn$.
	\end{enumerate}
\end{definition}
Given a matrix $A\in \Rmn$ and two norms $||.||_p$ and $||.||_q$ on $\Rn$ and $\R^m$, respectively, the induced operator/matrix norm $||A||_{a\to b}$ is defined by
\begin{equation*}
	||A||_{p\to q} = \max_{||x||_p=1} ||Ax||_q. 
\end{equation*}
%TODO: remark for myself, point their attention to the concordance of the dimension between A and x
It can be shown that the above definition implies that for any $x \in \Rn$ it holds the inequality
\begin{equation*}
	||Ax||_q \leq ||A||_{p\to q} ||x||_p.
\end{equation*}
An induced matrix norm is indeed a norm in the sense that it satisfies the three properties required above. We refer to the matrix norm $||\cdot||_{p,q}$s the $(p, q)$-norm. When $p=q$, we will simply refer to it as an $p$-norm and omit one of the subscripts in its notation.
\par An important operator norm is the spectral norm, where $p=q=2$. In this case, it can be proved that the operator norm coincide with the maximum singular value of $A \Rmn$ (see below for more details on eigenvalues and singular values)
\begin{equation*}
	||A||_2 = ||A||_{2\to2} = \sqrt{\lambda_1 (A^TA)} =: \sigma_1 (A).
\end{equation*}
The default operator norm for this course will be the spectral norm, so when the subscript is omitted we will implicitly refer to this norm.
\par When $p=q=1$, the operator norm reduces to the 
\begin{equation*}
	||A||_1 = ||A||_{1\to1} = \max_{j\in[n]} \sum_{i=1}^{n}.
\end{equation*}
This norm is also called maximum absolute row sum norm.
\par When $p=q=\infty$,

\bibliographystyle{plain}
\bibliography{../biblio}
\end{document}