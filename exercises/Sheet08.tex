\documentclass{ExerciseSheet}

%Set Number of the Exercise sheet and the submission deadline.
\setExerciseSheetNumber{8}
\setSubmissionDate{xx.xx.2024}

%boolean variable to determine whether the solutions should be included
\newif\ifsolutions
\solutionstrue
% \solutionsfalse

%We have a figure in this sheet
\usepackage{graphicx}

\begin{document}


%Start with exercises
%-----------------------------------------------------------------------%

%-----------------------------------------------------------------------%
\begin{problem}
Let $f\in C_L^{1,1}(\R^n)$ and $f\in C^2(\R^n)$. Let $\{x_k\}$ and $\{d_k\}$ be the sequences generated by Algorithm 2 (TNCG) in the lecture note. Assume that $d_k$ satisfies 
    \begin{align*}
        \nabla f(x_k)^Td_k \leq -c_1 \lVert \nabla f(x_k)\rVert^2, \quad \lVert d_k \rVert \leq c_2 \lVert \nabla f(x_k)\rVert
    \end{align*}
    with certain $c_1, c_2>0$. Prove that a Line Search method on $x_k$ along $d_k$ with initial step size 1 terminates in a finite number of iterations with $t_k \geq \min \left\{1,\frac{2c_1(1-\alpha)\beta}{Lc_2^2 }\right\}$.
\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
    Let us prove that the algorithm terminates in finite steps. By contradiction, there exists no finite $i$ such that
    \begin{align*}
        \frac{f(x_k+\beta^id_k)-f(x_k)}{\beta^i}>\alpha \nabla f(x_k)^Td_k.
    \end{align*}
    Given $\beta<1$, we have $\lim_{t\rightarrow \infty}\beta^i = 0$. Thus, the LHS of the inequality is the directional derivative of $f$ along $d_k$ as $i\rightarrow \infty$. In particular, we have
    \begin{align*}
        \nabla f(x_k)^T d_k > \alpha \nabla f(x_k)^T d_k,
    \end{align*}
    which is a contradiction, as $\nabla f(x_k)^T d_k<0$ and $\alpha <1$. Thus, the algorithm accepts either the initial guess $1$ or $t_k\leq \beta$. In the latter case, given $t_k$ from the output of the line-search, the step size before the last backtracking $t_k/\beta$ is not accepted, which gives 
    \begin{align}\label{eq: linesearch}
        f\left(x_k + \frac{t_k}{\beta}d_k\right) >f(x_k) + \alpha \frac{t_k}{\beta} \nabla f(x_k)^T d_k.
    \end{align}
    Now, recall Descent Lemma (part 1), i.e., for any $x,y \in\R^n$, 
    \begin{align*}
        f(y) \leq f(x) + \nabla f(x)^T ( y-x)+\frac{L}{2}\lVert x-y\rVert^2.
    \end{align*}
    We replace $x=x_k$ with $y=x_k+\frac{t_k}{\beta}d_k$, which gives
    \begin{align}\label{eq: descent lemma}
        f\left(x_k + \frac{t_k}{\beta}d_k\right) \leq f(x_k)+\frac{t_k}{\beta}\nabla f(x_k)^T d_k+\frac{Lt_k^2}{2\beta^2}\lvert d_k\rVert^2.
    \end{align}
    Combining \eqref{eq: linesearch} and \eqref{eq: descent lemma} gives
    \begin{align*}
        f(x_k) + \alpha \frac{t_k}{\beta} \nabla f(x_k)^T d_k < f(x_k)+\frac{t_k}{\beta}\nabla f(x_k)^T d_k+\frac{Lt_k^2}{2\beta^2}\lvert d_k\rVert^2.
    \end{align*}
    Rearranging it yields
    \begin{align*}
        0<(1-\alpha)\nabla f(x_k)^T d_k +\frac{Lt_k^2}{2\beta^2}\lvert d_k\rVert^2.
    \end{align*}
    Since $\nabla f(x_k)^Td_k \leq -c_1 \lVert \nabla f(x_k)\rVert^2,  \lVert d_k \rVert \leq c_2 \lVert \nabla f(x_k)\rVert$ for some $c_1,c_2>0$, it holds that
    \begin{align*}
        0<(1-\alpha)(-c_1 \lVert \nabla f(x_k)\rVert^2)  +\frac{Lt_k^2}{2\beta^2}c_2^2 \lVert \nabla f(x_k)\rVert^2,
    \end{align*}
    which gives $t_k > \frac{2 c_1 (1-\alpha)\beta}{Lc_2^2}$. 
\end{solution}

\fi

%-----------------------------------------------------------------------%
\vskip 0.5cm

\begin{problem}
    Prove that the iteration complexity of Algorithm 2 (TNCG) is $\mathcal{O}(\epsilon^{-2})$.
\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
    After step 17 of the algorithm, it holds that
    \begin{align*}
        f\left(x_k + t_kd_k\right) \leq f(x_k) + \alpha t_k \nabla f(x_k)^T d_k.
    \end{align*}
    Since $t_k \geq \min \left\{1,\frac{2c_1(1-\alpha)\beta}{Lc_2^2 }\right\}$ and $\nabla f(x_k)^Td_k \leq -c_1 \lVert \nabla f(x_k)\rVert^2$ for some $c_1>0$, we have
    \begin{align*}
        f\left(x_k + t_kd_k\right)&\leq f(x_k) - \alpha  c_1 t_k\lVert \nabla f(x_k)\rVert^2\\
        &  \leq f(x_k) - \alpha  c_1 \min \left\{1,\frac{2c_1(1-\alpha)\beta}{Lc_2^2 }\right\} \lVert \nabla f(x_k)\rVert^2.
    \end{align*}
    Let $M:= c_1 \min \left\{1,\frac{2c_1(1-\alpha)\beta}{Lc_2^2 }\right\}$. Then, it holds that
    \begin{align*}
        f(x_k) - f(x_{k+1}) \geq M\lVert \nabla f(x_k)\rVert^2.
    \end{align*}
    Summing the inequality over $k=0,1,\ldots,T-1$, we obtain
    \begin{align*}
        f(x_0)-f(x_T)\geq M\sum_{k=0}^{T-1}\lVert \nabla f(x_k)\rVert^2.
    \end{align*}
    If $\lVert \nabla f(x_k)\rVert>\epsilon$ for the first $k_\epsilon$ iterations, it holds that 
    \begin{align*}
        f(x_0)-f(x_{k_\epsilon})\geq M k_\epsilon \epsilon^2.
    \end{align*}
    Since $f(x_k)\geq f(x^*)$ for all $k$, we have
    \begin{align*}
        k_\epsilon < \frac{f(x_0)-f(x^*)}{M}\epsilon^{-2}.
    \end{align*}
\end{solution}

\fi%-----------------------------------------------------------------------%

\vskip 0.5cm

\begin{problem}
 Let $f:\R^n\rightarrow \R$ be twice continuously differentiable on $\R^n$ and let $\{x_k\}$ be an infinite sequence generated by the algorithm
 \begin{align*}
     x_{k+1}=x_k+t_k d_k,
 \end{align*}
 where $t_k$ is computed by Armijo line search. Suppose that the following conditions hold:
 \begin{itemize}
     \item[(i)] $\{x_k\}$ converges to $x^*$, where $\nabla f(x^*)=0$ and $\nabla^2 f(x^*)$ is positive definite.
     \item[(ii)] There exists an index $k_1$ such that for all $k \geq \hat{k}$, the search direction $d_k$ is Newton's direction, that is,
     \begin{align*}
         d_k = -(\nabla^2 f(x_k))^{-1} \nabla f(x_k).
     \end{align*}
 \end{itemize}
 Then, prove that if $\gamma \in \left(0,\frac{1}{2}\right)$, there exists an index $k' \geq k_1$ such that for all $k\geq k'$, it holds that
 \begin{align*}
     f(x_k+d_k) \leq f(x_k)+\gamma\nabla f(x_k)^T d_k.
 \end{align*}
\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
    As $\{x_k\}\rightarrow x^*$, by assumptions there exists some $k_1\geq \hat{k}$ sufficiently large for the points $x_k$, with $k\geq k_1$ to remain in a closed ball around $x^*$ where $\nabla^2 f(x_k)$ is positive definite and 
    \begin{align*}
         \nabla^2 f(x_k)d_k = \nabla f(x_k).
    \end{align*}
    Then, for $k\geq k_1$ and some $\eta>0$, we have
    \begin{align*}
        -\nabla f(x_k)^T d_k = d_k^T\nabla^2 f(x_k)d_k \geq \eta \lVert d_k\rVert^2,
    \end{align*}
    where $\eta\leq \lambda_{min}\nabla^2 f(x)$ for $x$ in the closed ball considered. And the Hessian is uniformly continuous in the closed ball, and we fix  $\epsilon := \left(\frac{1}{2}-\gamma\right)\eta>0$. Hence there exists $\delta>0$ such that $\lVert x-y\rVert<\delta$, $\lVert \nabla^2 f(x)-\nabla^2f(y)\lVert \leq  \epsilon$ for all $x,y$ in the closed ball. Moreover, since $\nabla f(x_k)\rightarrow 0$ and $\nabla^2 f$ is non-singular, there exists $k_2$ such that for $k\geq k_2$, $\lVert d_k\rVert \leq \delta$.
    
    Then, for $k \geq k':=\max\{k_1,k_2\}$, using Taylor's theorem, we write
    \begin{align*}
        f(x_k+d_k) = f(x_k) + \nabla f(x_k)^T d_k + \frac{1}{2} d_k^T\nabla^2 f(z_k)d_k
    \end{align*}
    where $z_k=x_k+t_kd_k$ with $t_k \in (0,1)$. Then, it holds that
    \begin{align*}
        f(x_k+d_k) = f(x_k) +\frac{1}{2} \nabla f(x_k)^T d_k + \frac{1}{2} d_k^T(\nabla^2 f(z_k)-\nabla^2 f(x_k))d_k.
    \end{align*}
    Hence, we obtain
    \begin{align*}
        f(x_k+d_k)- f(x_k)-\gamma\nabla f(x_k)^T d_k &\leq \left(\frac{1}{2}-\gamma\right)\nabla f(x_k)^T d_k+ \frac{1}{2} d_k^T(\nabla^2 f(z_k)-\nabla^2 f(x_k))d_k \\
        & \leq -\left(\frac{1}{2}-\gamma\right)\eta \lVert d_k\rVert^2 + \frac{1}{2}\lVert \nabla^2 f(z_k)-\nabla^2 f(x_k)\rVert\lVert d_k\rVert^2\\
        & \leq \left(-\left(\frac{1}{2}-\gamma\right)\eta+\frac{\epsilon}{2}\right)\lVert d_k\rVert^2\\
        & = -\frac{1}{2}\left(\frac{1}{2}-\gamma\right)\eta\lVert d_k\rVert^2\\
        &\leq 0.
    \end{align*}
\end{solution}

\fi

%-----------------------------------------------------------------------%


\vskip 0.5cm

\begin{problem}
    Let $f:\R^n\rightarrow \R$ be twice continuously differentiable on $\R^n$. Suppose that the sequence $\{x_k\}$ generated by Algorithm 2 (TNCG) converges to $x^*$, where $\nabla f(x^*)=0$ and $\nabla^2 f(x^*)$ is positive definite. Then, prove that the following holds:
    \begin{itemize}
        \item[(i)] There exists a value $\bar{\epsilon}>0$ such that, for $k$ sufficiently large and for any $\epsilon_2 \in (0,\bar{\epsilon}]$, the test at Step 5 is never satisfied.
        \item[(ii)] The sequence $\{x_k\}$ converges to $x^*$ with superlinear convergence rate.
        \item[(iii)] If the Hessian matrix $\nabla^2 f$ is Lipschitz-continuous in a neighborhood of $x^*$, the sequence $\{x_k\}$ converges to $x^*$ with quadratic convergence rate.
    \end{itemize}
    To solve $(ii)$, use the following proposition without proof:
\end{problem}
\noindent \textbf{Proposition 1.} Let $f:\R^n\rightarrow \R$ be twice continuously differentiable on $\R^n$ and let $\{x_k\}$ be a sequence generated by $x_{k+1}=x_k+t_kd_k$. Suppose $\{x_k\}$ converges to $x^*$ where $\nabla f(x^*)=0$ and $\nabla^2 f(x^*)$ is positive definite. Assume that $\nabla f(x_k)\not=0$ and 
\begin{align}\label{eq: condition}
    \lim_{k\rightarrow \infty} \frac{\lVert d_k+\nabla^2 f(x^*)^{-1}\nabla f(x_k)\rVert}{\lVert \nabla f(x_k)\rvert}=0.
\end{align}
Then, if $\gamma \in (0,1/2)$, there exists an index $k'$ such that for all $k\geq k'$, we have
\begin{align*}
    f(x_k+d_k) \leq f(x_k) + \gamma \nabla f(x_k)^T d_k,
\end{align*}
that is, the unit step size is accepted by Armijo rule. Morevoer, we have
\begin{align*}
    \lim_{k\rightarrow \infty}\frac{\lVert x_{k+1}-x^*\rVert}{\lVert x_k-x^*\rVert}=0.
\end{align*}
\ifsolutions
\vskip 0.3cm

\begin{solution}.

    \begin{itemize}
        \item[(i)] The convergence of $\{x_k\}$ and the continuity of the Hessian and the assumption that $\nabla^2 f(x^*)$ is positive definite imply that for sufficiently large $k$, $\lambda_{min}(\nabla^2 f(x_k))\geq \bar{\lambda}>0$. Set $\bar{\epsilon}=\frac{\bar{\lambda}}{4}$, and let $\epsilon \in (0,\bar{\epsilon}]$. For $k$ sufficiently large, we can write 
        \begin{align*}
            s_i^T\nabla^2 f(x_k)s_k \geq \frac{\bar{\lambda}}{2}\lVert s_i\rVert^2 > \epsilon\lVert s_i\rVert^2,
        \end{align*}
        which concludes the proof.
        \item[(ii)]
        To apply Proposition 1, it is enough to show that $d_k$ generated from Algorithm 2 satisfies \eqref{eq: condition}. For sufficiently large $i$, since the test step at 11 is satisfied, i.e., 
        \begin{align*}
            \lVert \nabla q_i \rVert \leq \eta \lVert \nabla f(x_k)\rVert \min \left\{\frac{1}{k+1}, \lVert \nabla f(x_k)\rVert\right\}.
        \end{align*}
        From $\nabla q_i = \nabla f(x_k)+\nabla^2 f(x_k)d_k $, we write
        \begin{align}
            \lVert\nabla f(x_k)+\nabla^2 f(x_k)d_k\lVert \leq \eta_k \lVert\nabla f(x_k)\lVert 
        \end{align}
        where $\eta_k:=\eta \min \left\{\frac{1}{k+1}, \lVert \nabla f(x_k)\rVert\right\}$. Then, we have
        \begin{align*}
            \eta_k \geq \frac{\lVert\nabla f(x_k)+\nabla^2 f(x_k)d_k\rVert}{\lVert\nabla f(x_k)\lVert } \geq \frac{\lVert d_k + \nabla^2 f(x_k)^{-1} \nabla f(x_k)\rVert}{\lVert\nabla^2 f(x_k)^{-1}\lVert \lVert\nabla f(x_k)\lVert },
        \end{align*}
        thus it holds that
        \begin{align*}
            \frac{\lVert d_k + \nabla^2 f(x_k)^{-1} \nabla f(x_k)\rVert}{ \lVert\nabla f(x_k)\lVert } \leq \lVert\nabla^2 f(x_k)^{-1}\lVert \eta_k.
        \end{align*}
        Then, we obtain
        \begin{align*}
            \frac{\lVert d_k+\nabla^2 f(x^*)^{-1}\nabla f(x_k)\rVert}{\lVert \nabla f(x_k)\rvert} &\leq \frac{\lVert d_k+\nabla^2 f(x_k)^{-1}\nabla f(x_k)\rVert}{\lVert \nabla f(x_k)\rvert}+\lVert \nabla^2 f(x^*)^{-1}-\nabla^2 f(x_k)^{-1}\rVert\\
            &\leq \lVert\nabla^2 f(x_k)^{-1}\lVert \eta_k+\lVert \nabla^2 f(x^*)^{-1}-\nabla^2 f(x_k)^{-1}\rVert\\
            & \rightarrow 0
        \end{align*}
        for sufficiently large $k$.
        \item[(iii)] By assumption, since $\nabla f(x^*)=0$ and $\nabla^2 f(x^*)$ is positive definite, there exists $k_1$ sufficiently large, for $k\geq k_1$, there exists $m,M,r>0$ such that $x_k \in B_r(x^*)$ and $MI \succeq \nabla^2f(x)\succeq  mI$ for $x \in B_r(x^*)$. Moreover, as the Hessian is Lipschitz continuous in $B_r(x^*),$ we have $\lVert\nabla^2 f(x)-\nabla^2 f(y)\rVert \leq L\lVert x-y\rVert $ for $x,y \in B_r(x^*)$ where $L$ is the Lipschitz constant.
        
        Let $r_k:=\nabla q_i(d_k)$. From step 11, we have that 
        \begin{align*}
            \lVert r_k \rVert \leq \eta \lVert \nabla f(x_k)\rVert \min \left\{\frac{1}{k+1}, \lVert \nabla f(x_k)\rVert\right\}. 
        \end{align*}
        For sufficiently large $k$, we claim that $\lVert r_k \rVert \leq \eta \lVert \nabla f(x_k)\rVert^2$. Walking toward the goal, we consider two cases. Suppose $\lVert \nabla f(x_k)\rVert \geq \frac{1}{k+1}$. Then, $\lVert r_k\rVert \leq \eta \lVert \nabla f(x_k)\rVert\frac{1}{k+1}\leq \eta \lVert \nabla f(x_k)\rVert^2$. Otherwise, $\lVert r_k\rVert \leq \eta \lVert \nabla f(x_k)\rVert^2$. Hence in either case, we obtain $\lVert r_k \rVert \leq \eta \lVert \nabla f(x_k)\rVert^2$.

        By Proposition 1 and $(ii)$, the unit step size is accepted by Armijo rule, that is, there exists an index $k'$ such that for all $k\geq k'$, we have $x_{k+1}=x_k +d_k$. Now, let $e_k=x_k-x^*$. We have that
        \begin{align*}
            &\nabla f(x_k)  = \int_0^1 \nabla^2 f(x^*+\tau e_k)e_k d\tau \\
           & \lVert \nabla^2 f(x_k)e_k-\nabla f(x_k)\rVert \leq \int_0^1 \lVert \nabla^2 f(x_k)-\nabla^2 f(x^*+\tau e_k)\rVert \lVert e_k\rVert d\tau \leq \frac{L}{2}\lVert e_k\rVert^2.
        \end{align*}
        Since $\nabla^2 f(x_k)d_k = -\nabla f(x_k)+r_k$, it holds that $\nabla^2 f(x_k) e_{k+1} = \nabla^2 f(x_k) e_{k}-\nabla f(x_k)+r_k.$ Then, we obtain
        \begin{align*}
            \lVert \nabla^2 f(x_k) e_{k+1} \rVert \leq \lVert \nabla^2 f(x_k) e_{k}-\nabla f(x_k)\rVert + \eta \lVert r_k\rVert \leq \frac{L}{2}\lVert e_k\rVert^2+\eta \lVert \nabla f(x_k)\rVert^2.
        \end{align*}
        Moreover, we have
        \begin{align*}
            \lVert e_{k+1}\rVert \leq \lVert \nabla^2 f(x_k)^{-1}\rVert \lVert \nabla^2 f(x_k)e_k\rVert\leq \frac{1}{m}\lVert \nabla^2 f(x_k)e_k\rVert
        \end{align*}
        and 
        \begin{align*}
            \lVert \nabla f(x_k)\rVert \leq \int_0^1 \lVert \nabla^2 f(x^*+\tau e_k)\rVert\lVert e_k\rVert d\tau \leq M \lVert e_k\rVert.
        \end{align*}
        Combining together, it holds that
        \begin{align*}
            \lVert e_{k+1}\rVert &\leq \frac{1}{m}\lVert \nabla^2 f(x_k)e_k\rVert\\
            &\leq \frac{1}{m}\left(\frac{L}{2}\lVert e_k\rVert^2+\eta \lVert \nabla f(x_k)\rVert^2\right)\\
            &\leq \frac{1}{m}\left(\frac{L}{2}\lVert e_k\rVert^2+\eta M^2 \lVert e_k\rVert^2\right) \\
            &= \frac{1}{m}\left(\frac{L}{2}+\eta M^2\right)\lVert e_k\rVert^2
        \end{align*}
        which completes the proof.
    \end{itemize}
\end{solution}

\fi
\end{document}