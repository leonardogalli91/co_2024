\documentclass{ExerciseSheet}
\usepackage{amsmath}

%Set Number of the Exercise sheet and the submission deadline.
\setExerciseSheetNumber{10}
\setSubmissionDate{xx.xx.2024}

%boolean variable to determine whether the solutions should be included
\newif\ifsolutions
\solutionstrue
% \solutionsfalse

%We have a figure in this sheet
\usepackage{graphicx}

\begin{document}


%Start with exercises
%-----------------------------------------------------------------------%

%-----------------------------------------------------------------------%
% \begin{problem}
%     Assume that $f\in C^{2,1}_L(\R^n)$, $f$ be lower bounded by $f^*$, that $\exists L_B>0$ such that $\|B_k\| \leq L_B$ and that $\Delta_0\geq \gamma_1 \delta \|\nabla(x_0)\|$. Algorithm 5 has an iteration complexity of $\mathcal{O}(\epsilon^{-3/2})$.
% \end{problem}

% \ifsolutions
% \vskip 0.3cm

% \begin{solution}
   
% \end{solution}

% \fi

%-----------------------------------------------------------------------%

\vskip 0.5cm

\begin{problem}
 Let $\{x_k\}$ and $\{\sigma_k\}$ be the sequences generated by Algorithm 5. Aussme that there exists $\sigma_\text{max}>0$ such that $\sigma_k\leq \sigma_\text{max}$. Then, it holds that
 \begin{align*}
     k \leq |\Scal_k| \left(1 + \frac{|\log \gamma_1|}{\log \gamma_2}\right) + \frac{1}{\log \gamma_2} \log \left(\frac{\sigma_{\max}}{\sigma_0}\right).
 \end{align*}
\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
    Let $\Scal_k$ and $\Ucal_k$ be the set of successful and unsuccessful iterations, respectively. From Step 9 of Algorithm 5, we have that for $0<\gamma_1<1<\gamma_2$ and $\sigma_{\min}<\sigma_0$, 
    \begin{align*}
        \sigma_{k+1}=\begin{cases}
            \max\{\gamma_1\sigma_k, \sigma_{\min} \}, & \rho_k\geq\eta_2,\\
            \sigma_k, & \rho_k \in [\eta_1,\eta_2),\\
            \gamma_2 \sigma_k, & \rho_k<\eta_1.
        \end{cases}
    \end{align*}
    Then, for $j\in\Scal_k$, we have $\gamma_1\sigma_j \leq \max\{\gamma_1\sigma_k, \sigma_{\min}\}=\sigma_{j+1}$ and for $j\in\Ucal_k$, we obtain $\gamma_2\sigma_j = \sigma_{j+1}$. Then, taking the product over all iterations gives 
    \begin{align}
        \sigma_0\gamma_1^{|\Scal_k|}\gamma_2^{|\Ucal_k|}\leq \sigma_k. \label{eq: lower bound of sigma}
    \end{align}
    Since $k=|\Scal_k|+|\Ucal_k|$, and by taking logarithms, it holds that 
    \begin{align*}
        |\Scal_k| \log\gamma_1 + (k-|\Scal_k|)\log \gamma_2 \leq \frac{\sigma_k}{\sigma_0}\leq \frac{\sigma_{\max}}{\sigma_0}
    \end{align*}
    where the last inequality holds due to $\sigma_k \leq \sigma_{\max}$ for all $k$. Rearranging it gives
    \begin{align*}
        k &\leq |\Scal_k| \left(1-\frac{\log\gamma_1}{\log\gamma_2}\right)+\frac{1}{\log\gamma_2}\log\left(\frac{\sigma_{\max}}{\sigma_0}\right)\\
        & =|\Scal_k| \left(1+\frac{|\log\gamma_1|}{\log\gamma_2}\right)+\frac{1}{\log\gamma_2}\log\left(\frac{\sigma_{\max}}{\sigma_0}\right)
    \end{align*}
    since $\gamma_1<1$. 

    To complete the proof, we prove \eqref{eq: lower bound of sigma} by mathematical induction. 
    \begin{itemize}
        \item Base case, $k=0$: $\sigma_0\gamma_1^{|\Scal_0|}\gamma_2^{|\Ucal_0|}=\sigma_0\gamma_1^0\gamma_2^0=\sigma_0.$
        \item Induction hypothesis: Assume $\sigma_0\gamma_1^{|\Scal_k|}\gamma_2^{|\Ucal_k|}\leq \sigma_k$ hold for some $k$.
        \item Induction step:
            \begin{itemize}
                \item If $k\in\Scal_k$, we know $\gamma_1\sigma_k \leq \sigma_{k+1}$. Also, $|\Scal_{k+1}|=|\Scal_k|+1$ and $|\Ucal_{k+1}|=|\Ucal_k|$. Thus,
                \begin{align*}
                    \sigma_{k+1} = \gamma_1\sigma_k \geq \gamma_1\left(\sigma_0\gamma_1^{|\Scal_k|}\gamma_2^{|\Ucal_k|}\right) = \sigma_0\gamma_1^{|\Scal_{k+1}|}\gamma_2^{|\Ucal_{k+1}|}.
                \end{align*}
                \item If $k\in\Ucal_k$, by a similar analysis, we obtain the claim.
            \end{itemize}
            
    \end{itemize}
    By induction, we conclude the proof.
\end{solution}

\fi

%-----------------------------------------------------------------------%


\vskip 0.5cm

\begin{problem}
  Let $s_k$ be computed as in Step 3 of Algorithm 5, then
	\begin{equation*}
		q_k(0)- q_k(s_k) > \frac{\sigma_k}{6} \|s_k\|^3.
	\end{equation*}
\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
    For Algorithm 5, we consider the regularized second order model:
    \begin{align*}
        m_k(s_k):=f(x_k)+\nabla f(x_k)^Ts_k+\frac{1}{2}s_k^T\nabla^2f(x_k)s_k+\frac{\sigma_k}{6}\lVert s_k\rVert^3 := q_k(s_k)+\frac{\sigma_k}{6}\lVert s_k\rVert^3.
    \end{align*}
    Since $q_k(0)=m_k(0)$, it holds that 
    \begin{align*}
        g_k(0)-q_k(s_k)=m_k(0)-\left(m_k(s_k)-\frac{\sigma_k}{6}\lVert s_k\rVert^3\right)\geq \frac{\sigma_k}{6}\lVert s_k\rVert^3
    \end{align*}
    where the last holds due to Step 3 of Algorithm 5.
\end{solution}

\fi

%-----------------------------------------------------------------------%


\vskip 0.5cm

\begin{problem}
 
Suppose $f\in C^{2,1}_L(\R^n)$ and $f$ is lower bounded by $f^*$. Then there exist a positive constant $K^S$, $K^A$ and $K^B$ such that for any $\epsilon\in(0,1]$, the Algorithm 5 requires at most
\begin{align*}
    K^A\frac{f(x_0)-f^*}{\epsilon^{3/2}}+K^B = \mathcal{O}(\epsilon^{-3/2})
\end{align*}
evaluations of $f$ and at most
\begin{align*}
    K^S\frac{f(x_0)-f^*}{\epsilon^{3/2}} = \mathcal{O}(\epsilon^{-3/2})
\end{align*}
evaluations of $\nabla f(x)$ and $\nabla^2 f(x)$ to produce an iterate $x_{k_\epsilon +1}$ such that $\lVert \nabla f(x_{k_\epsilon +1})\rVert\leq \epsilon$.
\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
    It is important to note that 
    \begin{itemize}
        \item[(i)] the objective function $f$ is evaluated at every iterations,
        \item[(ii)] the gradient and Hessian are only evaluated at successful iterations.
    \end{itemize}
    Thus, from the proof of Lemma 4.5, we have
    \begin{align*}
        |\Scal_{k_\epsilon}| < K^S \frac{f(x_0)-f^*}{\epsilon^{3/2}}
    \end{align*}
    where we define $K^S:= \frac{6(L+\theta+\sigma_\text{max})^{3/2}}{2^{3/2}\eta_1\sigma_\text{min}}$, which gives the number of evaluations for (ii). 

    Next, by Lemma 4.1, we have the upper bound on the number of iterations:
    \begin{align*}
     {k_\epsilon} &\leq |\Scal_{k_\epsilon}| \left(1 + \frac{|\log \gamma_1|}{\log \gamma_2}\right) + \frac{1}{\log \gamma_2} \log \left(\frac{\sigma_{\max}}{\sigma_0}\right)\\
     &<K^A\frac{f(x_0)-f^*}{\epsilon^{3/2}}+K^B,
    \end{align*}
    where $K^A:= K^S\left(1 + \frac{|\log \gamma_1|}{\log \gamma_2}\right)$ and $K^B:=\frac{1}{\log \gamma_2} \log \left(\frac{\sigma_{\max}}{\sigma_0}\right)$, which gives the number of evaluations for (i). 
\end{solution}

\fi


\end{document}