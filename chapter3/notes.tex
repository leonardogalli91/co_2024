\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage[left=2.50cm, right=2.50cm, top=2.0cm, bottom=2.0cm]{geometry}

\include{../macros}

\title{Continuous Optimization}
\author{Chapter 3: Constrained Optimization}
\date{}
\begin{document}
	\maketitle
\section{Definitions}
	In this chapter we will consider constrained optimization problems with the following shape
	\begin{equation}\label{eq:problem}
		\begin{split}
			\min \;\; &f(x)\\
			\st& x \in C
		\end{split}
	\end{equation}
\begin{definition}[Convex Set]
	A set $C$ is said to be convex if given $x_1,x_2\in C$ and $\lambda\in [0,1]$, then $\lambda x_1 +(1-\lambda) x_2 \in C.$
\end{definition}
\begin{definition}[Convex Function]
A function $f:C\to \R$ defined on a convex set $C$ is said to be convex if given $x_1,x_2\in C$ and $\lambda\in [0,1]$, then 
\begin{equation*}
	f(\lambda x_1 +(1-\lambda) x_2) \leq \lambda f(x_1) +(1-\lambda) f(x_2).
\end{equation*}
\end{definition}
\begin{definition}[Strictly Convex Function]
A function $f:C\to \R$ defined on a convex set $C$ is said to be strictly convex if given $x_1,x_2\in C$ and $\lambda\in [0,1]$, then 
\begin{equation*}
	f(\lambda x_1 +(1-\lambda) x_2) < \lambda f(x_1) +(1-\lambda) f(x_2).
\end{equation*}
\end{definition}
\noindent A function is called concave if $-f$ is convex and strictly concave if $-f$ is strictly convex.\\
Now, given $\Delta_k$ the unit-simplex, that is the
subset of $\R^k$ comprising all nonnegative vectors whose sum is 1, i.e., 
\begin{equation*}
	\{\lambda \in \R^k: \lambda\geq 0, e^t\lambda=1\},
\end{equation*}
we can provide the following very useful result by Jensen's.
\begin{theorem}[Jensen's Inequality]
	Let $f:C\to \R$ be a convex function over a convex set $C$. Then for any $x_1, x_2, \dots, x_k \in C$ and $\lambda\in \Delta_k$ we have
	\begin{equation}\label{eq:jensen}
		f\left(\sum_{i=1}^{k}\lambda_ix_i\right) \leq \sum_{i=1}^{k} \lambda_if(x_i).
	\end{equation}
\end{theorem}
\begin{proof}
	We will prove \eqref{eq:jensen} by induction on $k$. For $k=1$ the result is obvious ($f(x_1)\leq f(x_1)\;\;\forall x_1 \in C$). We now assume that \eqref{eq:jensen} holds for $k$ and we will prove that it also holds for $k+1$. Suppose we have $x_1, x_2, \dots, x_{k+1} \in C$ and $\lambda\in \Delta_{k+1}$, we will show that $f(z)\leq \sum_{i=1}^{k+1} \lambda_i f(x_i)$ with $z=\sum_{i=1}^{k+1} \lambda_i x_i$. If $\lambda_{k+1} =1$, then $z=x_{k+1}$ and \eqref{eq:jensen} is obvious. If $\lambda_{k+1}<1$, then
	\begin{equation*}
		\begin{split}
			f(z)& = f\left(\sum_{i=1}^k \lambda_i x_i + \lambda_{k+1}x_{k+1} \right)\\
			&= f\left((1-\lambda_{k+1})\sum_{i=1}^k \frac{\lambda_i}{1-\lambda_{k+1}} x_i + \lambda_{k+1}x_{k+1} \right)\\
			&\leq (1-\lambda_{k+1})f(v) + \lambda_{k+1} f(x_{k+1}),
		\end{split}
	\end{equation*}
with $v= \sum_{i=1}^k\frac{\lambda_i}{1-\lambda_{k+1}} x_i$. Since $\sum_{i=1}^k\frac{\lambda_i}{1-\lambda_{k+1}} = \frac{1-\lambda_{k+1}}{1-\lambda_{k+1}} = 1,$ it follows that $v$ is a convex combination of $k$ points from $C$, hence by the induction hypotesis we have that $f(v)\leq \sum_{i=1}^k\frac{\lambda_i}{1-\lambda_{k+1}} f(x_i)$, which combined with the equality above yields
\begin{equation*}
	f(z) \leq \sum_{i=1}^{k+1} \lambda_i f(x_i).
\end{equation*}
\end{proof}
\section{Characterizations of Convex Functions}
\begin{theorem}[Gradient characterization of convex functions]\label{thm:gradient_ineq}
	Let $f\in \C(C)$, where $C$ is convex. Then f is convex over $C$ if and only if
	\begin{equation}\label{eq:grad_ineq}
		f(x) +\grad(x)^T(y-x)\leq f(y) \quad \forall x, y\in C.
	\end{equation}
\end{theorem}
\begin{proof}
	Exercise.
\end{proof}
\begin{proposition}[Sufficiency of stationarity under convexity]\label{prop:stationarity}
	Let $f\in \C(C)$, where $C\subseteq\Rn$ is convex. Suppose that $\nabla f(x^*)=0$ for some $x^*\in C$. Then $x^*$ is a global minimizer of $f$ over $C$.
\end{proposition}
\begin{proof}
	Let $z\in C$. Plugging $x=x^*$ and $y=z$ in Theorem \ref{thm:gradient_ineq} we obtain that 
	\begin{equation*}
		f(z)\geq f(x^*) +\grad(x^*)^T(z-x^*),
	\end{equation*}
which implies that $f(z)\geq f(x^*) $ because $\grad(x^*)=0$.
\end{proof}
We note that Proposition \ref{prop:stationarity} establishes only the sufficiency of the stationarity condition $\grad(x^*) = 0$ for guaranteeing that $x^*$ is a global optimal solution. When $C$ is not the
entire space, this condition is not necessary, in fact it might be that the points for which $\grad(\cdot)=0$ are not in $C$. On the other hand, when $C=\Rn$ and $f$ is convex, $\grad(x^*) = 0$ is both sufficient and necessary condition for $x^*$ to be a global minimum.
We can now establish the conditions under which a twice continuously differentiable function $f$ is convex.
\begin{theorem}[Second order characterization of convexity]
	Let $f\in \Cii(C)$, where $C\subseteq\Rn$ is convex and open. Thus, we have that $f$ is convex iff $\hess(x)\succcurlyeq0\quad \forall x\in C.$
\end{theorem}
\begin{proof}
	Suppose that $\hess(x) \succcurlyeq 0$ for all $x \in C$. We will prove \eqref{eq:grad_ineq} which is enough to establish convexity. Let $x,y\in C$, then by the Mean Value Theorem$^2$ (Theorem 2.6 from Chapter 1) we get that there exists $z\in[x,y]$ (and hence $z\in C$) for which 
	\begin{equation}\label{eq:mvt2_app}
		f(y) = f(x)+ \grad (x)^T(y-x) +\frac{1}{2}(y-x)^T\hess(z)(y-x).
	\end{equation}
Since $\hess(z) \succcurlyeq 0$, it follows that $(y-x)^T\hess(z)(y-x)\geq0$, which implies \eqref{eq:grad_ineq}.
To prove the opposite direction, assume that $f$ is convex over $C$. Let $x\in C$ and $y\in \Rn$. Since $C$ is open, it follows that $x+\lambda y \in C$, for $0<\lambda<\epsilon$, where $\epsilon$ is a small enough positive constant. Using now the gradient characterization of convex functions \eqref{eq:grad_ineq} we get 
\begin{equation*}
	f(x+\lambda y) \geq f(x) + \lambda\grad(x)^Ty.
\end{equation*}
In addition, by the quadratic approximation theorem (Theorem 2.4 from Chapter 1), we have that 
\begin{equation*}
	f(x+\lambda y) = f(x) + \lambda \grad(x)^Ty + \frac{\lambda^2}{2}y^T\hess(x)y + o(\lambda^2||y||^2),
\end{equation*}
which combined with the above inequality gives 
\begin{equation*}
	\frac{\lambda^2}{2}y^T\hess(x)y + o(\lambda^2||y||^2) \geq 0 \quad \forall \lambda\in (0,\epsilon).
\end{equation*}
Dividing the latter inequality by $\lambda^2$ and taking the limit for $\lambda\to 0^+$, we have 
\begin{equation*}
	y^T\hess(x)y \geq 0 \quad \forall y \in \Rn,
\end{equation*}
which concludes the proof.
\end{proof}
The same theorem works with positive definiteness and strict convexity, meaning also that the minimum in this case is unique.

\section{Optimization over convex constraints}
From now on, we consider \eqref{eq:problem} where $C$ is convex. On the other hand, we will not always assume also $f$ to be convex. From the convexity of $f$ we have the following two theorems. Notice that the following result is not a direct consequence of Proposition \ref{prop:stationarity} as the local (and global) minimum, might be on the boundary of the set and not be stationary (in the sense of unconstrained optimization).
\begin{theorem}[global=local in convex optimization] Let $f:C\to\R$ be a convex function over a convex set $C\subseteq \Rn$. Let $x^*\in C$ be a local minimum of $f$ over $C$. Then $x^*$ is a global minimum of $f$ over $C$.	
\end{theorem}
\begin{proof}
	Since $x^*$ is a local minimum of $f$ over $C$ there exists $r$ such that $f(x)\geq f(x^*)$ for any $x\in C \cap B[x^*,r]$. Now let $y\in C$ with $y\neq x^*$. We want to show that $f(y) \geq f(x^*)$. Let $\lambda\in(0,1]$ be such that $x^*+\lambda(y-x^*)\in B[x^*,r]$, for instance $\lambda=\frac{r}{||y-x^*||}$. Now, since $x^*+\lambda (y-x^*) \in B[x^*,r]\cap C$, it follows that $f(x^*)\leq f(x^*+\lambda (y-x^*))$, and hence, by convexity of $f$, also 
	\begin{equation*}
		f(x^*)\leq f(x^*+\lambda (y-x^*)) \leq (1-\lambda)f(x^*) +\lambda f(y)
	\end{equation*}
Thus, $\lambda f(x^*) \leq \lambda f(y)$, which concludes the proof.
\end{proof}
\begin{theorem}[Convexity of the optimal set in convex optimization]\label{thm:unique}
	Let $f:C\to \R$ be a convex function with $C\subseteq \Rn$ convex. Then, the set of optimal solutions of the problem \eqref{eq:problem}, which we denote by $X^*$ is convex. Moreover, if $f$ is strictly convex over $C$, then there exists at most one optimal solution.
\end{theorem}
\begin{proof}
	If $X^*=\emptyset$, the result follows trivially. Suppose that $X\neq\emptyset$ and denote the optimal value of $f$ by $f^*$. Let $x,y\in C$ with $\lambda\in[0,1]$. Then, by convexity $f(\lambda x+(1-\lambda)y)\leq \lambda f^* +(1-\lambda)f^*= f^*$, hence $\lambda x +(1-\lambda)y$ is also optimal, i.e., it belongs to $X^*$, establishing the convexity of $X^*$. Suppose now that $f$ is strictly convex and $X^*$ is nonempty, and suppose by contradiction that there are 2 points $x,y$ in $X^*$. Then $\lambda x +(1-\lambda)y \in C$, and by the strict convexity of $f$ we have 
	\begin{equation*}
		f(\lambda x +(1-\lambda) y) < \lambda f(x) + (1-\lambda) f(y) = f^*,
	\end{equation*}
which is a contradiction to the fact that $f^*$ is the optimal value.
\end{proof}
\subsection{Stationarity}
Note that the following definition and the following theorem are given also for the more general case in which $f$ is not convex.
\begin{definition}[Stationary points of convex constrained problems]
	Let $f\in \C(C)$, where $C$ is closed and convex. Then $x^*$ is a stationary point of \eqref{eq:problem} if
	\begin{equation}\label{eq:stationarity}
	 \grad(x^*)^T(x-x^*)\geq 0 \; \forall x\in C.
\end{equation}
\end{definition}
\noindent In words, this means that there are no feasible descent directions of $f$ at $x^*$. This suggests that stationarity is in fact a necessary condition for a local minimum of \eqref{eq:problem}.
\begin{theorem}[Stationarity as necessary optimality condition of a convex constrained problem]\label{thm:stationarity}
	Let $f\in \C(C)$, where $C$ is closed and convex and let $x^*$ be a local minimum of \eqref{eq:problem}. Then $x^*$ is a stationary point of \eqref{eq:problem}.
\end{theorem}
\begin{proof}
	Let $x^*$ be a local minimum of $f$ and assume by contradiction that is not a stationary point of \eqref{eq:problem}. Then there exists $x\in C$ such that $\grad(x^*)(x-x^*)< 0$. Therefore, $f'(x,d)<0$, where $d=x-x^*$. Hence, by Lemma 1.1 of Chapter 2, there exists $\epsilon\in(0,1)$ such that $f(x^*+td)<f(x^*)\;\forall t\in(0,\epsilon).$ Since $C$ is convex, we have that $x^*+td = (1-t)x^*+tx\in C$, leading to the conclusion that $x^*$ is not a local optimum of \eqref{eq:problem}, which is a contradiction.
\end{proof}

\begin{theorem}[Stationarity as necessary and sufficient optimality condition for a convex problem]\label{thm:convex_stationarity}
	Let $f\in \C(C)$, where $C$ is closed and convex and $f$ is also convex. Let $x^*$ be a local minimum of \eqref{eq:problem}. Then $x^*$ is a stationary point of \eqref{eq:problem} iff $x^*$ is a optimal solution of \eqref{eq:problem}.
\end{theorem}
\begin{proof}
	The necessity of the stationarity condition follows from Theorem \ref{thm:stationarity}. To prove the sufficiency, assume that $x^*$ is a stationary point of \eqref{eq:problem} and let $x\in C$. Then, the gradient characterization of convex functions \eqref{eq:grad_ineq} and stationarity of $x^*$, we get
	\begin{equation*}
		f(x) \geq f(x^*) +\grad(x^*)^T(x-x^*) \geq f(x^*),
	\end{equation*}
which concludes the proof.
\end{proof}
\noindent Unfortunately, \eqref{eq:stationarity} is not an easy condition to check, we need something else.
\subsection{Orthogonal Projection}
We can instead characterize stationary points by using the projection operator.
Given a nonempty closed convex set $C$, the orthogonal projection operator $P_C:\Rn \to C$ is defined by 
\begin{equation}\label{eq:projection}
	P_C(x) = \argmin\left\{||x-y||^2: y\in C\right\}
\end{equation}
The orthogonal projection operator with input $x$ returns the vector in $C$ that is the closest (in $\ell_2$-norm)
to $x$. Note that the orthogonal projection operator is defined as a solution of a convex
optimization problem, specifically, a minimization of a convex quadratic function subject to a convex feasibility set. The first orthogonal projection theorem states that the orthogonal projection operator is in fact well-defined, meaning that the optimization problem in \eqref{eq:projection} has a unique optimal solution.
\begin{theorem}[First Projection Theorem]
	Let $C$ be a nonempty closed convex set. Then problem \eqref{eq:projection} has a unique optimal solution.
\end{theorem}
\begin{proof}
	As $C$ is closed and $||x-y||^2$ is coercive, we have that the problem admits at least one solution (by Theorem 3.8 of Chapter 1). Moreover, $||x-y||^2$ is strictly convex as the objective function is quadratic with
	positive definite Hessian (the identity). Thus, from Theorem \ref{thm:unique} we get that \eqref{eq:projection} has a unique solution.
\end{proof}
The second projection theorem, provides an useful characterization of the projection operator. Geometrically it states that for a given closed and convex set $C$, $x \in \Rn$, and for any $y\in C$, the angle between $x-P_C (x)$ and $y-P_C (x)$ is obtuse.
This phenomenon is illustrated in Figure \ref{fig:second_projection}.
\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{second_projection}.
	\caption{The orthogonal projection operator.} \label{fig:second_projection}
\end{figure}
\begin{theorem}[Second Projection Theorem]\label{thm:second_projection}
	Let $C$ be a nonempty closed convex set. Then $z=P_C(x)$ iff 
	\begin{equation}\label{eq:second_projection}
		(x-z)^T(y-z) \leq 0\quad \forall \,y\in C.
	\end{equation}
\end{theorem}
\begin{proof}
	$z=P_C(x)$ iff it is the optimal solution of \eqref{eq:projection} iff (by Theorem \ref{thm:convex_stationarity})
	\begin{equation*}
		\grad(z)^T(y-z)\geq 0 \quad \forall\,y\in C,
	\end{equation*}
which concludes the proof as $\grad(z) = x-z.$
\end{proof}
Another important property of the orthogonal projection operator is given in the
following theorem, which also establishes the so-called nonexpansiveness property of $P_C$.
\begin{theorem}[Nonexpansiveness of the projection operator]
	Let $C$ be a closed and convex set. Then, for any $v,w\in \Rn$
	\begin{itemize}
		\item[a)] 
		\begin{equation}\label{eq:nonexpansiveness_1}
			(P_C(v)-P_C(w))^T(v-w)\geq ||P_C(v)-P_C(w)||^2
		\end{equation}
		\item[b)]
		\begin{equation}\label{eq:nonexpansiveness_2}
			 ||P_C(v)-P_C(w)||\leq ||v-w||.
		\end{equation}
	\end{itemize}
\end{theorem}
\begin{proof}
	From Theorem \ref{thm:second_projection} we have that for any $x \in \Rn$ and $y\in C$
	\begin{equation*}
		(x-P_C(x))^T(y-P_C(x)) \leq 0.
	\end{equation*}
Replacing $x=v$ and $y=P_C(w)$ we have
\begin{equation*}
	(v-P_C(v))^T(P_C(w)-P_C(v)) \leq 0.
\end{equation*}
Replacing, instead, $x=w$ and $y=P_C(v)$
\begin{equation*}
	(w-P_C(w))^T(P_C(v)-P_C(w)) \leq 0.
\end{equation*}
Now, summing the two inequalities we get
\begin{equation*}
	(P_C(w)-P_C(v))^T(v-w+P_C(w)-P_C(v)) \leq 0,
\end{equation*}
and hence, 
\begin{equation*}
	(P_C(v)-P_C(w))^T(v-w) \geq ||P_C(w)-P_C(v)||^2.
\end{equation*}
To prove \eqref{eq:nonexpansiveness_2}, we note that if $P_C(v) = P_C(w)$, the inequality is trivial. Thus, we assume $P_C(v) \neq P_C(w)$. Then by Cauchy-Schwartz, we have 
\begin{equation*}
	(P_C(v)-P_C(w))^T(v-w) \leq ||P_C(v)-P_C(w)|| \cdot||v-w||,
\end{equation*}
which combined with \eqref{eq:nonexpansiveness_1} gives 
\begin{equation*}
	||P_C(v)-P_C(w)||^2 \leq ||P_C(v)-P_C(w)|| \cdot||v-w||,
\end{equation*}
which concludes the proof as $P_C(v) \neq P_C(w)$.
\end{proof}
Coming back to stationarity, let us provide the alternative characterization of a stationary point through the projection operator. Notice that this theorem holds also when $f$ is non-convex. 
\begin{theorem}\label{thm:stationarity_projection}
	Let $f\in C^1(C)$ with $C$ closed and convex and let $s>0$. $x^*$ is a stationary point of the problem \eqref{eq:problem} iff 
	\begin{equation}\label{eq:stationary_projection}
		x^*=P_C(x^*-s\grad(x^*)).
	\end{equation}
\end{theorem}
\begin{proof}
	By the second projection theorem (Theorem \ref{thm:second_projection}), we get that $x^*=P_C(x^*-s\grad(x^*))$ iff
	\begin{equation*}
		(x^*-s\grad(x^*) -x^*)^T(x-x^*)\leq 0, 
	\end{equation*}
which concludes the proof, as $x^*$ is a stationary point when $\grad(x^*)^T(x-x^*)\geq0$
\end{proof}
\subsection{Projected Gradient Method}
The characterization of stationary points through equation \eqref{eq:stationary_projection} directly suggest a new algorithm for solving convex constrained optimization methods. As we will see later, this algorithm finds stationary points despite $f$ being convex or not. \\
\begin{algorithm}[H]\label{alg}
	\caption{Projected Gradient (PG) Method}
	
	\KwIn{$x_0\in \Rn$, $\epsilon>0$, $t\in (0,\frac{L}{2})$}
	
	$k = 0$
	
	\While{$||x_{k-1}-x_k||> \epsilon$}{
				
		$x_{k+1} = P_C(x_k-t\grad(x_k)$
		
		$k = k+1$
	}
\end{algorithm}
The proof of convergence of PG is similar to that of GD. In particular, we first prove the Decrease Lemma for constrained optimization problem. 
\begin{lemma}[Decrease Lemma for Convex Constrained Problems]
	Let $f\in \LC(C)$, where $C$ is convex and closed. Then for any $x\in C$ and $t\in (0,\frac{2}{L})$ the following inequality holds
	\begin{equation*}
		f(x)-f(P_C(x-t\grad(x))) \geq t\left(1- \frac{Lt}{2}\right) \left\| \frac{1}{t}(x-P_C(x-t\grad(x))) \right\|^2.
	\end{equation*}
\end{lemma}
\begin{proof}
	Exercise.
\end{proof}
\noindent It is now convenient to define the gradient mapping as
\begin{equation}\label{eq:gradient}
	G_M(x) := M \left(x-P_C\left(x-\frac{1}{M}\grad(x)\right)\right) \quad\with M>0.
\end{equation}
Note that in the unconstrained case $G_M(x)=\grad(x)$ so the gradient mapping is an extension of the usual gradient operator. In addition, by Theorem \ref{thm:stationarity_projection}, $G_M(x) = 0$ iff $x$ is a stationary point of \eqref{eq:problem}. This means that we can look at $\|G_M(x)\|$ as an optimality measure. Moreover, the sufficient decrease stated above can be rewritten as 
\begin{equation*}
	f(x)-f(P_C(x-t\grad(x))) \geq t\left(1- \frac{Lt}{2}\right) \left\| G_{\frac{1}{t}} (x) \right\|^2.
\end{equation*}
This generalized sufficient decrease property allows us to prove similar results to those proven in the unconstrained case.
\begin{theorem}[Convergence of PG method]
	Let $f\in \LC(C)$, with $C$ closed and convex. Let $\{x_k\}_k$ be a sequence generated by Algorithm \ref{alg} for solving \eqref{eq:problem}. Assume that $f$ is bounded below over $C$. Then we have the following
	\begin{itemize}
		\item[(a)] The sequence $\{f(x_k)\}_k$ is nonincreasing. In addition, for any $k\geq 0$, $f(x_{k+1}) < f(x_k)$ unless $x_k$ is a stationary point.
		\item[(b)] $G_{\frac{1}{t}}(x_k) \to 0$ as $k\to \infty$.
	\end{itemize}
\end{theorem}
\noindent Notice that the theorem above only ensures convergence to a stationary point, which in the non-convex case might not be a global minimum. Also, the rate of convergence of PG is the same as that of GD, that is $\mathcal{O}(\frac{1}{\sqrt{T}})$. If we assume $f$ to be convex, we can instead ensure a faster rate of convergence, moreover, thanks to Theorem \ref{thm:convex_stationarity} all stationary points of \eqref{eq:problem} are global minima.
\begin{theorem}[Convergence of PG method for convex problems]
	Let $f\in \LC(C)$ be convex, with $C$ closed and convex. Let $\{x_k\}_k$ be a sequence generated by Algorithm \ref{alg} for solving \eqref{eq:problem}. Assume that the set of optimal solutions $X^*$ is nonempty and that $f^*$ is the optimal value. Then we have the following
	\begin{itemize}
		\item[(a)] for any $k\geq0$ and $x^*\in X^*$
		\begin{equation*}
			2t(f(x_{k+1}) -f^*) \leq ||x_k-x^*||^2 - ||x_{k+1}-x^*||^2,
		\end{equation*}
		\item[(b)] for any $n\geq 0$:
		\begin{equation*}
			f(x_n)-f^*\leq \frac{||x_0-x^*||}{2t n}.
		\end{equation*}
	\end{itemize}
\end{theorem}
\begin{proof}
	By the Descent Lemma (for unconstrained optimization, Lemma 2.2 from Chapter 2), we have 
	\begin{equation*}
		f(x_{k+1}) \leq f(x_k)+\grad(x_k)^T(x_{k+1}-x_k) +\frac{L}{2}||x_{k+1}-x_k||^2.
	\end{equation*}
Let $x^*$ be a global minimum of \eqref{eq:problem}, then the gradient characterization of convexity \eqref{eq:grad_ineq} implies that $f(x_k)\leq f(x^*)+\grad(x_k)^T(x_k-x^*)$, which together with the previous inequality implies that
\begin{equation}\label{eq:intermediate}
	f(x_{k+1}) \leq f(x^*)+\grad(x_k)^T(x_k-x^*)+\grad(x_k)^T(x_{k+1}-x_k) +\frac{L}{2}||x_{k+1}-x_k||^2.
\end{equation}
By the second projection theorem \eqref{eq:second_projection} applied on the projected point $x_{k+1}$, we have that 
\begin{equation*}
\left(x_k -t\grad(x_k) -x_{k+1}\right)^T\left(x^*-x_{k+1}\right)\leq 0
\end{equation*}
if and only if
\begin{equation*}
\grad(x_k)^T\left(x_{k+1}-x^*\right)+ \frac{1}{t}\left(x_k -x_{k+1}\right)^T\left(x^*-x_{k+1}\right)\leq0
\end{equation*}
if and only if
\begin{equation*}
	\grad(x_k)^T\left(x_{k+1}-x^*\right) \leq \frac{1}{t}(x_k-x_{k+1})^T\left(x_{k+1}-x^*\right).
\end{equation*}
Therefore, from the above inequality, \eqref{eq:intermediate} and $t\leq \frac{1}{L}$, we get
\begin{equation*}
	\begin{split}
		f(x_{k+1}) &\leq f(x^*)+\grad(x_k)^T(x_k-x^*)+\grad(x_k)^T(x_{k+1}-x_k) +\frac{L}{2}||x_{k+1}-x_k||^2\\
		& = f(x^*)+\grad(x_k)^T(x_{k+1}-x^*) +\frac{L}{2}||x_{k+1}-x_k||^2\\
		&\leq f(x^*)+\frac{1}{t}(x_k-x_{k+1})^T\left(x_{k+1}-x^*\right)+\frac{L}{2}||x_{k+1}-x_k||^2\\
		&\leq f(x^*)+\frac{1}{t}(x_k-x_{k+1})^T\left(x_{k+1}-x^*\right)+\frac{1}{2t}||x_{k+1}-x_k||^2\\
		&= f(x^*)+\frac{1}{2t}(x_k-x_{k+1})^T\left(x_{k+1}-x^*+x_k - x^*\right)\\
		&= f(x^*)+\frac{1}{2t}(x_k-x_{k+1}+x^*-x^*)^T\left(x_{k+1}-x^*+x_k - x^*\right)\\
		&= f(x^*)+\frac{1}{2t}(x_k-x^*)^T\left(x_{k+1}-x^*+x_k - x^*\right) + \frac{1}{2t}(x^*-x_{k+1})^T\left(x_{k+1}-x^*+x_k - x^*\right)\\
		&= f(x^*)+\frac{1}{2t}\left(||x_k-x^*||^2+(x_k-x^*)^T(x_{k+1}-x^*)-(x_k-x^*)^T(x_{k+1}-x^*) -||x_{k+1}-x^*||^2\right)\\
		&= f(x^*)+\frac{1}{2t}\left(||x_k-x^*\|^2 -\|x_{k+1}-x^*\|^2\right)
	\end{split}
\end{equation*}
establishing part (a). To achieve (b), we sum the inequalities (a) for $k=0,1, \dots, n-1$ and obtain
\begin{equation*}
	||x_n-x^*\|^2 -\|x_0-x^*\|^2 \leq 2t \sum_{k=0}^{n-1} \left(f(x^*)-f(x_{k+1})\right) \leq 2tn(f(x^*)-f(x_n)),
\end{equation*}
where in the last inequality we used the fact that $f(x_{k+1})\leq f(x_k)$, which, in turn, is a consequence of the Descent Lemma and the fact that $t\in(0, \frac{1}{L})$. Thus, 
\begin{equation*}
	f(x_n) -f(x^*) \leq \frac{||x_0-x^*\|^2 -\|x_n-x^*\|^2}{2tn}\leq \frac{||x_0-x^*\|^2}{2tn}.
\end{equation*}
\end{proof}




\section{KKT Conditions}
In this chapter we will derive the necessary optimality conditions, i.e., Karush-Kunh-Tucker conditions, for the most general case where $C$ is possibly nonconvex. In particular, we consider problems of the following shape
\begin{equation}\label{eq:last_problem}
\begin{split}
\min \;\; &f(x)\\
\st& g_i(x)\leq 0, \quad i=0, \dots, m,
\end{split}
\end{equation}
where $f,g_i\in \C(\R)$ but possibly not convex. Notice that this class of problems is very general, as equality constraints can be included observing that $h(x)=0$ can be replaced by 2 inequalities $h(x)\leq0$ and $-h(x)\leq 0$. From now on $C:=\{x\in\Rn: g_i(x)\leq 0, \; i\in [m] \}$. 
\begin{definition}[Feasible Descent Direction] A vector $d$ is called feasible descent direction at $x\in C$ if $\grad(x)^Td<0$ and there exists $\epsilon>0$ such that $x+td\in C$ for all $t\in [0,\epsilon].$

\end{definition}
\noindent Obviously, a necessary local optimality condition of a point x is that it does not have
any feasible descent directions.
\begin{lemma} Let $x^*$ be a local optimum of \eqref{eq:last_problem}, then there are no feasible descent directions at $x^*.$
\end{lemma}
\begin{proof}
The proof goes by contradiction and follows directly from the definition of feasible descent direction and directional derivative.
\end{proof}
\begin{definition}[Active Constraints] Let $g_i(x)\leq 0, \; i\in [m] $ be a set of inequalities. The active constraints at $\bar{x}$ are the constraints satisfied as equalities at $\bar{x}$. The set of active constraints is denoted by $I(x):=\{i\in[m]: g_i(x)=0\}.$
\end{definition}

\begin{lemma}
Let $x^*$ be a local minimum of the problem \eqref{eq:last_problem} and let $I(x^*)$ be the set of active constraints at $x^*$. Then, there does not exists a vector $d\in\Rn$ such that 
\begin{align*}
&\grad(x^*)^Td<0,\\
&\nabla g_i (x^*)^Td<0,\quad i \in I(x^*).
\end{align*}
\end{lemma}
\begin{proof}
Suppose by contradiction that $d$ satisfies the system of inequalities above. Then it follows that there exists $\epsilon_1>0$ such that $f(x^*+td)<f(x^*)$ and $g_i(x+td)<g(x^*)=0$ for any $t\in (0,\epsilon_1)$ and $i\in I(x^*)$. For any $i\not \in I(x^*)$, we have $g_i(x^*)<0$ and hence, by continuity of $g_i$ it follows that there exists $\epsilon_2>0$ such that $g_i(x^*+td)<0$ for any $t\in (0,\epsilon_2)$ and $i\not \in I$. We can thus conclude that 
\begin{align*}
&\grad(x^*+td)^Td<f(x^*),\\
&\nabla g_i (x^*+td)^Td<0,\quad i \in [m],
\end{align*}
for all $t\in (0,\min\{\epsilon_1,\epsilon_2\})$, which is a contradiction to the local optimality of $x^*$.
\end{proof}
We have thus shown that a necessary optimality condition for local optimality is the infeasibility of a certain system of strict inequalities. On the other hand, similarly to the stationarity condition, this system is difficult to use in practice. We will state now the Fritz-John conditions.
\begin{theorem}[Fritz-John Conditions]
Let $x^*$ be a local minimum of the problem \eqref{eq:last_problem}. Then there exists multipliers $\lambda_0,\dots, \lambda_1, \dots, \lambda_m$ such that they are not all zeros and such that 
\begin{equation}\label{eq:fritzjohn}
\begin{split}
\lambda_0\grad(x^*)+ \sum_{i=1}^{m} \lambda_i \nabla g_i(x^*) &= 0,\\
\lambda_ig_i(x^*)&=0 \quad i=1, \dots, m.
\end{split}
\end{equation}
\end{theorem}
\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{separation}.
	\caption{Strict separation of point from a closed and convex set.} \label{fig:separtation}
\end{figure}
\noindent In order to prove this theorem we need a rather large digression into the Alternative Theorems. We begin with a very simple yet powerful result on convex sets, namely the separation theorem between a point and a closed convex set. This result will be the basis for all the optimality conditions that will be discussed later on.
\begin{theorem}[Strict Separation Theorem]\label{thm:separation}
	Let $C$ be a closed and convex set and let $y\not \in C$. Then there exists $p\in \Rn \setminus \{0\}$ and $\alpha\in \Rn$ such that
	\begin{equation*}
		p^Ty>\alpha \qquad \text{and} \qquad p^Tx\leq \alpha \;\;\forall x\in C.
	\end{equation*}
\end{theorem}
\begin{proof}
By the second projection theorem, the vector $\bar{x}=P_C(y)\in C$ satisfies
\begin{equation*}
	(y-\bar{x})^T(x-\bar{x}) \leq 0 \;\; \forall x\in C
\end{equation*}
which is the same as 
\begin{equation*}
	(y-\bar{x})^T x \leq (y-\bar{x})^T \bar{x} \;\; \forall x\in C.
\end{equation*}
Denote $p=y-x\neq 0$ (since $y\not \in C$) and $\alpha= (y-\bar{x})^T \bar{x}$. Then we have that $p^Tx\leq \alpha\;\forall x\in C$. On the other hand,
\begin{equation*}
	p^Ty = (y-\bar{x})^Ty = (y-\bar{x})^T(y-\bar{x}) + (y-\bar{x})^T\bar{x} = ||y-\bar{x}||^2 + \alpha > \alpha,
\end{equation*}
and the result is established.
\end{proof}
\noindent Now, before going on with two more alternative theorems, we need to show that the conic hull of a fine set is closed and convex.
\begin{definition}[Conic Hull] Let $S\subseteq \Rn$. Then the conic hull of $S$, denoted by $\cone(S)$,
	is the set comprising all the conic combinations of vectors from S:
	\begin{equation*}
		\cone(S):=\left\{\sum_{i=1}^k \lambda_i x_i: x_1,\dots, x_k\in S, \lambda\in \R^k_+, k \in \N\right\}
	\end{equation*}
\end{definition}
\begin{lemma}\label{lemma:cone}
	Let $a_1, a_2, \dots a_k\in \Rn$. Then $\cone(\{a_1, \dots, a_k\})$ is closed and convex.
\end{lemma}
\begin{proof}
	Exercise.
\end{proof}
\noindent We can now go on with the next alternative theorem.
\begin{lemma}[Farkas' lemma, second formulation]\label{lemma:farkas}
	Let $c\in \Rn$ and $A\in \Rmn.$ Then the following two claims are equivalent:
	\begin{itemize}
		\item[M.] The implication $Ax\leq 0 \Rightarrow c^Tx\leq 0$ holds true. 
		\item[N.] There exists $y\in \Rn_+$ such that $A^Ty=c.$
	\end{itemize}
\end{lemma}
\begin{proof}
	Suppose that system $N$ is feasible. To see that the implication $M$ holds, suppose that $Ax\leq 0$ for some $x\in \Rn$. Then, multiplying this inequality from the left by $y^T$ (a valid operation since $y\geq0$) yields 
	\begin{equation*}
		y^TAx\leq 0,
	\end{equation*}
which concludes the thesis by noticing that $c^T=y^TA$. 
\par The reverse direction is not so obvious. Suppose that the implication $M$ is satisfied, and
let us show that system $N$ is feasible. Suppose in contradiction that system $N$ is infeasible,
and consider the following set
\begin{equation*}
	S=\{x\in\Rn: x= A^Ty \; y\in \Rn_+\},
\end{equation*}
which is closed and convex thanks to Lemma \ref{lemma:cone}. The infeasibility of B means that $c\not \in S$. By Theorem \ref{thm:separation}, it follows that there exists a vector $p\in \Rn \setminus \{0\}$ and $\alpha\in \R$ such that $p^Tc>\alpha$ and 
\begin{equation}\label{eq:farkas_final}
	p^Tx\leq \alpha \; \forall x\in S
\end{equation}
Since $0\in S$, from \eqref{eq:farkas_final} we have that $\alpha\geq 0$ and hence also $p^Tc>0$. In addition, \eqref{eq:farkas_final} is equivalent to 
\begin{equation*}
	p^TA^Ty\leq \alpha \;\; \forall y\geq 0 
\end{equation*}
or to 
\begin{equation*}
	(Ap)^Ty\leq \alpha \;\; \forall y\geq 0.
\end{equation*}
Let us now prove that $Ap \leq 0$ (notice that this means component-wise). By contradiction, if there was an index $i\in \{1,2, \dots,m\}$ such that $(Ap)_i>0$, then for $y=\beta e_i$ we would have that $(Ap)^Ty = \beta (Ap)_i$ which is an expression that goes to $\infty$ as $\beta\to \infty$, and, thus, cannot be bounded by a constant $\alpha$. At this point we have found a system for which $Ap\leq 0$ and $p^Tc=c^Tp>0$, which contradicts the implication $M$.
\end{proof}
\noindent In order to prove Gordon's alternative theorem, we are going to use Farkas' lemma in the following formulation
\begin{lemma}[Farkas' lemma, first formulation]\label{lemma:Farkas}
	Let $c\in \Rn$ and $A\in \Rmn.$ Then exactly one of the following system has a solution:
	\begin{itemize}
		\item[I.] $Ax\leq 0, c^Tx> 0$. 
		\item[II.] $A^Ty=c, y\geq 0$.
	\end{itemize}
\end{lemma}
\noindent To show that the two formulations are equivalent, let us notice that $II$ is equivalent to $N$ and let us write down the truth table of $M$ and $I$. In particular, let us call $M_1$ the statement $Ax\leq 0$ and $M_2$ the statement $c^Tx\leq0$ and notice that $I=M_1 \wedge \bar{M}_2$.\\

\begin{tabular}{|c|c|c|c|}
	\hline
	M$_1$& M$_2$ & M= M$_1\Rightarrow$M$_2$& $I=M_1 \wedge \bar{M}_2$ \\
	\hline
	F& F & T & F \\
	\hline
	F& T & T & F \\
	\hline
	T& F & F & T \\
	\hline
	T& T & T & F \\
	\hline
\end{tabular}\\[1\baselineskip]
In particular, this means that the two formulations are equivalent as the first formulation (Lemma \ref{lemma:Farkas}) states that exactly one between $I$ and $II$ has solutions while the second formulation (Lemma \ref{lemma:farkas}) states that $M$ and $N$ are equivalent.

\bibliographystyle{plain}
\bibliography{../biblio}
\end{document}