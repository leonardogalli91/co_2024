\documentclass[10pt,a4paper]{article}
\include{../packages}
\include{../macros}

\title{Optimization Methods}
\author{Chapter 3: Second-Order Methods for Unconstrained Optimization}
\date{}
\begin{document}
	\maketitle
	\section{Pure Newton Method}
	In the previous chapter, we have studied optimization problems like $\min_{x\in\Rn} f(x)$ with $f\in\C(\Rn)$, in particular, we only used first order information to build our methods. In this chapter we assume that $f\in\Cii(\Rn)$, and we will present second-order methods, that is, in addition to the information on function values and gradients, we will employ evaluations of the Hessian matrices. We will start from the most famous second-order method, namely Newton's method, whose main idea is the following. Given an iterate $x_k$, the next iterate $x_{k+1}$ is chosen to minimize the quadratic approximation of the function around $x_k$:
	\begin{equation*}
		x_{k+1} = \argmin_{x \in \mathbb{R}^n} q_k(x):= f(x_k) + \grad(x_k)^T (x - x_k) + \frac{1}{2}(x - x_k)^T \hess(x_k)(x - x_k).
	\end{equation*}
	The above update formula is not well-defined unless we further assume that $\hess(x_k)$ is positive definite. In that case, the unique minimizer of the minimization problem above is the unique stationary point:
	\begin{equation*}
		\grad(x_k) + \hess(x_k)(x_{k+1} - x_k) = 0,
	\end{equation*}
	which is the same as
	\begin{equation}\label{eq:newton}
		x_{k+1} = x_k - \hess(x_k)^{-1} \grad(x_k).
	\end{equation}
	The vector $-(\hess(x_k))^{-1} \grad(x_k)$ is called the Newton direction, and the algorithm induced by the update formula \eqref{eq:newton} is called the pure Newton's method. 
	
	\begin{algorithm}[H]\label{alg:newton}
		\caption{Pure Newton}
		
		\KwIn{Pick $x_0\in \Rn$ arbitrarly, $\epsilon>0$.}
		
		$k = 0$
		
		\While{$||\grad(x_k)||>\epsilon$}{
			
			Compute a direction $d_k$ as a solution to the linear system $\hess(x_k) d_k = -\grad(x_k)$
			
			$x_{k+1} = x_k +d_k$
			
			$k = k+1$
		}
	\end{algorithm}
\noindent Note that when $\hess(x_k)$ is positive definite for any $k$, Newton's directions are descent directions.
\begin{lemma}
	Let $f\in \Cii(\Rn)$ and let $\hess(x)\succ0,$ then $d_k = - \hess(x_k)^{-1} \grad(x_k)$ is a descent direction.
\end{lemma}
\begin{proof}
	That follows directly from the definition of $d_k$ and the fact that the inverse of a positive definite matrix is also positive definite, thus
	$\grad(x_k)^Td_k = - \grad(x_k)^T \hess(x_k)^{-1} \grad(x_k)<0.$ 
\end{proof}
\noindent By assuming that $\hess(x)$ is positive definite for every $x \in \mathbb{R}^n$ we also have that there exists a unique optimal solution $x^*$. However, this is not enough to guarantee convergence, as the following example illustrates.

\begin{example}
	Consider the function $f(x) = \sqrt{1 + x^2}$ defined over the real line. The minimizer of $f$ over $\mathbb{R}$ is of course $x = 0$. The first and second derivatives of $f$ are
	\begin{equation*}
		f'(x) = \frac{x}{\sqrt{1 + x^2}}, \quad f''(x) = \frac{1}{(1 + x^2)^{3/2}}.
	\end{equation*}
	Therefore, (pure) Newton's method has the form
	\begin{equation*}
		x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)} = x_k - x_k(1 + x_k^2) = -x_k^3.
	\end{equation*}
	We therefore see that for $|x_0| \geq 1$ the method diverges and that for $|x_0| < 1$ the method converges very rapidly to the correct solution $x^* = 0$.
\end{example}
\noindent Even when it converges, its worst case rate is not better than that of GD, i.e., $\BigO(\epsilon^{-2})$.

\begin{definition}
	We say that $f\in \LCii(\Rn)$ if the Hessian is Lipschitz continuous, i.e., $ \exists\, L > 0$ for which $\|\hess(x) - \hess(y)\|_2 \leq L\|x - y\|$ for any $x, y \in \mathbb{R}^n$
\end{definition}
\begin{theorem}\label{thm:slow_newton}
	Algorithm \ref{alg:newton} applied to minimize a function $f\in \LC(\Rn)$ and $f\in \LCii(\Rn)$ bounded from below may require as many as $\epsilon^{-2}$ evaluations of $f$ and $\grad$ to produce an iterate $x\in \Rn$ such that $||\grad (x)\|\leq \epsilon.$
\end{theorem}
\begin{proof}
	Our aim is to build a function $f : \mathbb{R}^2 \to \mathbb{R}$ such that, for any $\epsilon \in (0,1)$, Newton's method takes at least $\epsilon^{-2}$ iterations to find an $\epsilon$-approximate first-order minimizer $x_\epsilon$ for which
	\begin{equation*}
		\|\grad(x_\epsilon)\| \leq \epsilon,
	\end{equation*}
	when started from the origin.\\
	Consider a hypothetical sequence of iterates $\{x_k\}_{k=0}^\infty$ such that $f(x_k) = f_k$, $\grad(x_k) = g_k$, and $\hess(x_k) = H_k$ are defined, for $k \geq 0$, by the relations
	\begin{equation}\label{eq:def_fk}
		f_k = 2 - k\epsilon^2, \quad g_k = \begin{pmatrix} -\epsilon^2 f_k \\ -\epsilon f_k \end{pmatrix}, \quad \text{and } H_k = \begin{pmatrix} \epsilon^2 f_k^2 & 0 \\ 0 & f_k^2 \end{pmatrix}.
	\end{equation}
	If this sequence of function, gradient, and Hessian values could be generated by Newton's method starting from the origin and applied to a function which is bounded from below, such that $f\in \LC(\Rn)$ and $f\in \LCii(\Rn)$, then for $k=0,\dots k_\epsilon$ with
	\begin{equation*}
		k_\epsilon := \left\lceil \epsilon^{-2} \right\rceil,
	\end{equation*}
	it is easy to check that
	\begin{equation*}
		f_k \in [1,2], \quad \|g_k\| = \epsilon f_k \sqrt{1 + \epsilon^2} > \epsilon, \quad \text{and } d_k = \frac{1}{f_k} \begin{pmatrix} 1 \\ \epsilon \end{pmatrix},
	\end{equation*}
	with the last equality resulting from \eqref{eq:newton}. As a consequence, this Newton iteration would require at least $k_\epsilon$ iterations (and evaluations of $f_k$, $g_k$, and $H_k$) before terminating. In addition,
	\begin{equation*}
		x_0 = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \quad \text{and } x_k = \sum_{j=0}^{k-1} d_j. 
	\end{equation*}
	The next step in our construction is to build a smooth function with bounded second and third derivatives (which implies that its gradient and Hessian are Lipschitz continuous) interpolating \eqref{eq:def_fk} at $x_k$ as defined above. The idea is that $f(x)$ should behave exactly like its quadratic approximation $q_k$ around $x_k$. In particular, given 
	\begin{equation*}
		q_k(x) := f_k + g_k^T (x - x_k) + \frac{1}{2} (x - x_k)^T H_k (x - x_k),
	\end{equation*}
	we have $q_k(x_k) = f_k$, $\nabla q_k(x_k) = g_k$, and $\nabla^2 q_k(x_k) = H_k$. That also mean, that outside of this ball, the effects of all the other $q_k$ should be $0$. More precisely, for $k \geq 1$, let
	\begin{equation*}
		\sigma(\alpha) := \begin{cases}
			1 & \text{if } 0 \leq \alpha \leq \frac{1}{6}, \\
			1 + 27\left(\alpha - \frac{1}{6}\right)^3 \left[-10 + 45\left(\alpha - \frac{1}{6}\right) - 54\left(\alpha - \frac{1}{6}\right)^2\right] & \text{if } \frac{1}{6} \leq \alpha \leq \frac{1}{2}, \\
			0 & \text{if } \alpha \geq \frac{1}{2}.
		\end{cases}
	\end{equation*}
	This piecewise polynomial has the property that it is identically equal to 1 near the origin and to decrease smoothly to zero (with bounded first, second, and third derivatives) between $\frac{1}{6}$ and $\frac{1}{2}$ (Exercise: derive the formula for $\sigma$ with $\frac{1}{6}\leq \alpha\leq \frac{1}{2}$.). Using this function, we may then define, for each $k \geq 0$, a local support function $\sigma(\|x - x_k\|)$ that is identically equal to 1 in a (circular) neighbourhood of $x_k$ of radius $\frac{1}{6}$ and decreases smoothly (with bounded first, second, and third derivatives) to 0 for all points whose distance to $x_k$ exceeds $\frac{1}{2}$.
	We are now in a position to define the objective function as
	\begin{equation*}\label{eq:fSN}
		f_{SN}(x) = \sum_{k=0}^{k_\epsilon} \sigma(\|x - x_k\|) q_k(x)
	\end{equation*}
	for all $x$ in $\mathbb{R}^2$. Note that the sum in \eqref{eq:fSN} involves at most two nonzero terms for each $x$, because
	\begin{equation*}
		[d_k]_1 = 1/f_k > \frac{1}{2},
	\end{equation*}
	where we used the definition of $d_k$, and thus the distance between the first components of successive iterates (and thus also between the iterates themselves) exceeds $\frac{1}{2}$. This function, with its first two derivatives, obviously interpolates \eqref{eq:def_fk}, is bounded below, and has bounded first, second, and third derivatives since the large values of $q_k(x)$ that occur in their expressions always occur far from $x_k$ and are thus annihilated by the support function (verify everything as an Exercise). In particular, $f\in\LC(\Rn)$ and $f\in \LCii(\Rn)$.
\end{proof}

\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{slow_newton_contour}
	\includegraphics[width=0.5\linewidth]{slow_newton_3d}
	\caption{Contour lines of $f_{SN}(x)$ and the path of iterates for $k = 0, . . . , 10$. Below a 3D view of the function.}
\end{figure}
\noindent Despite all the negative results listed here, Newton's method does have one very attractive feature: one can prove local quadratic rate of convergence, which means that near the optimal solution the errors $e_k = \|x_k - x^*\|$ (where $x^*$ is the unique optimal solution) satisfy the inequality $e_{k+1} \leq M e_k^2$ for some positive $M > 0$. This property essentially means that the number of accuracy digits is doubled at each iteration.

\begin{theorem}[Quadratic local convergence of Newton's method]\label{thm:newton}
	Let $f\in \Cii(\Rn)$ with $\hess$ Lipschitz continuous. Moreover, let $\hess(x)$ be positive definite for any $x \in \mathbb{R}^n$. Let $\{x_k\}_{k \geq 0}$ be the sequence generated by Algorithm \ref{alg:newton}, and let $x^*$ be the unique minimizer of $f$ over $\mathbb{R}^n$. Then, we have 
	\begin{equation}\label{eq:newton_contraction}
		\|x_{k+1} - x^*\| \leq \frac{L}{2m}\|x_k - x^*\|^2 \quad \forall k = 0, 1, \ldots
	\end{equation}
	holds. In addition, if $\|x_0 - x^*\| \leq \frac{m}{L}$, then
	\begin{equation}\label{eq:local_newton}
		\|x_k - x^*\| \leq \frac{2m}{L} \left(\frac{1}{2}\right)^{2^k}.
	\end{equation}
\end{theorem}
\begin{proof}
	Let $k$ be a nonnegative integer. Then, from  $\grad(x^*)=0$ and the fundamental Theorem of Calculus, we have
	\begin{align*}
		x_{k+1} - x^* &= x_k - (\hess(x_k))^{-1} \grad(x_k) - x^* \nonumber \\
		&= x_k - x^* + (\hess(x_k))^{-1}(\grad(x^*) - \grad(x_k)) \nonumber \\
		&= x_k - x^* + (\hess(x_k))^{-1} \int_0^1 [\hess(x_k + t(x^* - x_k))](x^* - x_k) dt \nonumber \\
		&= (\hess(x_k))^{-1} \int_0^1 [\hess(x_k + t(x^* - x_k)) - \hess(x_k)](x^* - x_k) dt. \nonumber
	\end{align*}
	Combining the latter equality with the fact that $\hess(x)\succ 0 \forall x,$ i.e., $\exists m>0: \hess(x_k) \succeq mI$, implies that $\|(\hess(x_k))^{-1}\| \leq \frac{1}{m}$. Hence,
	\begin{align*}
		\|x_{k+1} - x^*\| &\leq \|(\hess(x_k))^{-1}\| \left\| \int_0^1 [\hess(x_k + t(x^* - x_k)) - \hess(x_k)](x^* - x_k) dt \right\| \nonumber \\
		&\leq \|(\hess(x_k))^{-1}\| \int_0^1 \left\| [\hess(x_k + t(x^* - x_k)) - \hess(x_k)](x^* - x_k) \right\| dt \nonumber \\
		&\leq \|(\hess(x_k))^{-1}\| \int_0^1 \|\hess(x_k + t(x^* - x_k)) - \hess(x_k)\| \cdot \|x^* - x_k\| dt \nonumber \\
		&\leq \frac{L}{m} \int_0^1 t \|x_k - x^*\|^2 dt = \frac{L}{2m} \|x_k - x^*\|^2, \nonumber
	\end{align*}
where the last inequality follows from the Lipschitz continuity of the Hessian.	Now, we will prove inequality \eqref{eq:local_newton} by induction on $k$. Note that for $k = 0$, we assumed that
	\begin{equation*}
		\|x_0 - x^*\| \leq \frac{m}{L},
	\end{equation*}	
	so in particular
	\begin{equation*}
		\|x_0 - x^*\| \leq \frac{2m}{L} \left(\frac{1}{2}\right)^{2^0},
	\end{equation*}
	establishing the basis of the induction. Assuming that \eqref{eq:local_newton} holds for an integer $k$, that is, $\|x_k - x^*\| \leq \frac{2m}{L} \left(\frac{1}{2}\right)^{2^k}$, we will show it holds for $k + 1$. Indeed, by \eqref{eq:newton_contraction} we have
	\begin{equation*}
		\|x_{k+1} - x^*\| \leq \frac{L}{2m} \|x_k - x^*\|^2 \leq \frac{L}{2m} \left(\frac{2m}{L} \left(\frac{1}{2}\right)^{2^k}\right)^2 = \frac{2m}{L} \left(\frac{1}{2}\right)^{2^{k+1}},
	\end{equation*}
	proving the desired result.
\end{proof}
\begin{remark}
	To compare Theorems \ref{thm:slow_newton} and \ref{thm:newton} it is important to notice that \eqref{eq:def_fk} is singular for $\epsilon\to 0$ otherwise, because of Theorem \ref{thm:newton}, the algorithm would achieve quadratic local convergence.
\end{remark}
\noindent Now, in order to ensure \textbf{global convergence} and maintain the local property of Theorem \ref{thm:newton}, we can employ one of the following globalization techniques:
\begin{itemize}
	\item \textbf{Line search} techniques applied to a descent direction constructed from Newton's direction;
	\item \textbf{Trust region} methods, where $s_k$ is chosen as the optimal solution (with a certain degree of approximation) of
	\begin{equation*}
		\min_{\|s_k\| \leq \Delta_k} q_k(s_k), \quad \with s_k:=x-x_k
	\end{equation*}
	appropriately updating the trust region radius $\Delta_k$ along the iterations (approximations of $\hess(x_k)$ can be used);
	\item \textbf{Regularized} methods, where $s_k$ is chosen as the optimal solution (with a certain degree of approximation) of
	\begin{equation*}
		\min_{s_k \in \mathbb{R}^n} q_k(s_k) + \sigma_k \|s_k\|^3
	\end{equation*}
	appropriately updating $\sigma_k$ along the iterations (approximations of $\hess(x_k)$ can be used).
\end{itemize}



\section{Line Search-based Newton method}
The Theorem \ref{thm:newton} above can be relaxed as follows. First of all, we don't need the Hessian to be positive definite in each $x\in \Rn$, but we just need to be able to invert it in $x^*$ and (by continuity) in a ball around it. Moreover, instead of solving the Newton equation \eqref{eq:newton} exactly, we can do it inexactly. Without writing down the whole algorithm, Inexact Newton is defined by the iteration $x_{k+1} = x_k + d_k$, where $d_k$ satisfies the condition
\begin{equation}\label{eq:truncated_Newton}
	\| \nabla q_k(d_k)\|=\|\hess(x_k) d_k + \grad(x_k)\| \leq \eta_k \|\grad(x_k)\|,
\end{equation}
\begin{proposition}[Local Convergence of Inexact Newton]\label{inexact_newton}
	Let $f\in \Cii(\mathcal{D})$ where $\mathcal{D} \subseteq \mathbb{R}^n$ is an open set. Suppose that the following conditions hold:
	\begin{itemize}
		\item[(i)] there exists a point $x^* \in \mathcal{D}$ such that $\grad(x^*) = 0$;
		\item[(ii)] the Hessian matrix $\hess(x^*)$ is non singular.
	\end{itemize}
	Then, there exist an open ball $\mathcal{B}(x^*; r) \subset \mathcal{D}$, and a value $\bar{\eta}>0$ such that, if $x_0 \in \mathcal{B}(x^*; r)$ and $\eta_k \in [0, \bar{\eta}]$ for all $k$, then the sequence $\{x_k\}$ generated by Inexact Newton (see \eqref{eq:truncated_Newton})	converges to $x^*$ with a linear convergence rate. Moreover 
	\begin{itemize}
		\item[(a)] if $\eta_k \to 0$ then $\{x_k\}$ converges with superlinear convergence rate, i.e., 
		$$ \lim_{k\to \infty} \frac{||x_{k+1}-x^*||}{||x_k-x^*||}=0;$$
		\item[(b)] if $f\in \LCii(\mathcal{D})$, and there exists a constant $C > 0$ such that $\eta_k \leq C \|\grad(x_k)\|$ for all $k$, then $\{x_k\}$ converges with quadratic convergence rate.
	\end{itemize}
\end{proposition}

\noindent In case the solution of the Newton equation is constructed via an iterative procedure which is  interrupted when is \eqref{eq:truncated_Newton} is satisfied, the resulting method is called Truncated Newton (TN).

At this point, we have studied an iterative method which solves linear system of equation, i.e., Conjugate Gradient. Thus, we can apply it to find $d_k$, however, as in the general case the Hessian matrix $\hess(x_k)$ may be not positive definite, we need to suitably adapt CG to determine a descent direction.

In the following algorithm we omit the iteration counter $k$ inside the CG procedure (all the variables there should actually have both the $k$ and $i$ subscripts).\\
\begin{algorithm}[H]\label{alg:tncg}
\caption{Truncated Newton with Conjugate Gradient (TNCG)}

\KwIn{Pick $x_0\in \Rn$ arbitrarly, $\eta > 0$, $\epsilon > 0$, $\epsilon_2 > 0$}

$k=0$

\While{$||\grad(x_k)|| > \epsilon$}{
	
	$i = 0$, $d_0 = 0$, $s_0 = -\nabla q_0 = -\grad(x_k)$
	
	\While{True}{
		
		\If{$s_i^T \hess(x_k) s_i \leq \epsilon_2 \|s_i\|^2$}{		
		
			$d_k = \begin{cases}
				-\grad(x_k), & \text{if } i = 0, \\
				d_i, & \text{if } i > 0
			\end{cases}$
	
			\textbf{break}
		}
	
		$\alpha_i = -\frac{\nabla q_i^T s_i}{s_i^T \hess(x_k) s_i}$
		
		$d_{i+1} = d_i + \alpha_i s_i$
		
		$\nabla q_{i+1} = \nabla q_i + \alpha_i \hess(x_k) s_i.$
		
		\If{$\|\nabla q_i\| \leq \eta \|\grad(x_k)\|$}{
			
			$d_k = d_i$
			
			\textbf{break}
		}
	
		$\beta_{i+1} = \frac{\nabla q_{i+1}^T \hess(x_k) s_i}{s_i^T \hess(x_k) s_i}$
		
		$s_{i+1} = -\nabla q_{i+1} + \beta_{i+1} s_i,$
		
		$i= i+1$
	}
	
	$t_k \leftarrow$ Line Search along $d_k$ with 1 as initial step.
	
	$x_{k+1} = x_k +t_k d_k$
	
	$k = k+1$
}
\end{algorithm}
\noindent Regarding Step 10 above, from (39) of Chapter 2 we would get 
$\grad(x_{k+1}) = \grad(x_k) +t_k Q d_k$ which when applied to $q_{k,i}(s_i)$ as an internal procedure becomes exactly $\nabla q_{i+1} = \nabla q_i + \alpha_i \hess(x_k) s_i.$
\par Given that $d_{k,0}=-\grad(x_k)$, CG is iteratively refining a gradient direction into a Newton direction (see Proposition \ref{inexact_newton}).

\begin{proposition}\label{prop:truncated_newton}
	Let $f \in \LC(\Rn)$ and $f\in \Cii(\Rn)$. Let $\{x_k\}$ and $\{d_k\}$ be the sequences generated by Algorithm \ref{alg:tncg}. Then $\exists\; c_1, c_2>0$ such that for all $k$ we have
	\begin{equation}\label{eq:gradient_related}
		\begin{split}
			\grad(x_k)^T d_k &\leq -c_1 \|\grad(x_k)\|^2 \\
			\|d_k\| &\leq c_2 \|\grad(x_k)\|. 
		\end{split}
\end{equation}
\end{proposition}
\begin{proof}
	
%Notice that \eqref{eq:newton} can be transformed into a quadratic optimization problem, thus all the relationship derived for that method still hold here.
From Step 10, we have
\begin{equation*}
	\nabla q_i = \nabla q_0 + \sum_{j=0}^{i-1} \alpha_j \hess(x_k) s_j.
\end{equation*}
and consequently
\begin{equation*}
	\alpha_i = -\frac{\nabla q_i^T s_i}{s_i^T \hess(x_k) s_i} = - \frac{\nabla q_0^T s_i}{s_i^T \hess(x_k) s_i}.
\end{equation*}
because all the vectors $s_j$ are mutually $\hess(x_k)$-conjugate. Now let $d_k$ be the direction computed by Algorithm \ref{alg:tncg}. By construction, either $d_k = -\grad(x_k)$ or $d_k = d_i$. In the second case, we have that 
\begin{equation}\label{eq:positive_definite}
	s_j^T \hess(x_k) s_j>0 \quad \forall j < i,
\end{equation}
and we can write
\begin{equation*}
d_k = d_i = \sum_{j=0}^{i-1} \alpha_j s_j = -\sum_{j=0}^{i-1} \frac{\nabla q_0^T s_j}{s_j^T \hess(x_k) s_j} s_j,
\end{equation*}
from which, recalling that in the algorithm we have
\begin{equation*}
\nabla q_0 = \grad(x_k), \quad s_0 = -\grad(x_k),
\end{equation*}
and, from \eqref{eq:positive_definite}, also
\begin{equation*}
\grad(x_k)^T d_k = -\sum_{j=0}^{i-1} \frac{(\grad(x_k)^T s_j)^2}{s_j^T \hess(x_k) s_j} \leq -\frac{(\grad(x_k)^T \grad(x_k))^2}{\grad(x_k)^T \hess(x_k) \grad(x_k)}.
\end{equation*}
It follows that $\grad(x_k)^T d_k < 0$, furthermore we can write
\begin{equation*}
|\grad(x_k)^T d_k| \geq \frac{\|\grad(x_k)\|^4}{\|\grad(x_k)\|^2 \|\hess(x_k)\|} \geq \frac{1}{L} \|\grad(x_k)\|^2,
\end{equation*}
as $f\in \LC(\Rn)$ is equivalent to $\|\hess(x)\|\leq L \; \forall x\in \Rn$ (Theorem 2.1 from Chapter 2). Taking into account that we can have $d_k = -\grad(x_k)$, we can conclude that the first inequality in \eqref{eq:gradient_related} is satisfied with $c_1 \leq \min\{1, 1/L\}$. Concerning the second inequality in \eqref{eq:gradient_related}, if $d_k = d_i$ then we have
\begin{equation*}
\|d_k\| = \|d_i\| \leq \sum_{j=0}^{i-1} \left|\frac{||s_j||^2}{s_j^T \hess(x_k) s_j}\right| \|\grad(x_k)\|,
\end{equation*}
and hence, since we must have
\begin{equation*}
s_j^T \hess(x_k) s_j > \epsilon_2 \|s_j\|^2,
\end{equation*}
we obtain 
$$\|d_i\|\leq \frac{i}{\epsilon}||\grad(x_k)||\leq\frac{n}{\epsilon} \|\grad(x_k)\|,$$
where the last inequality comes from the fact that either CG has only encountered $n$ vectors $s_j$ for which $s_j^T \hess(x_k) s_j >0$, and consequently it terminates because of Proposition 9.2 from Chapter 2 within $n$ iterations, or it encountered a vector for which $s_j^T \hess(x_k) s_j<0$ before. This brings to the second inequality in \eqref{eq:gradient_related} with $c_2:=\max\{1,\frac{n}{\epsilon}\}$.
\end{proof}

\noindent Directions $d_k$ that satisfy \eqref{eq:gradient_related} are called \textbf{gradient-related} and they satisfy the \textbf{angle condition} with $c=\frac{c_1}{c_2}$, i.e., the cosine between the gradient and the direction is obtuse, or, in other words, the angle with the anti-gradient is acute, that is
$$ 	\frac{\grad(x_k)^T d_k}{\|d_k\|\|\grad(x_k)\|} \leq -c.$$
Once we have Proposition \ref{prop:truncated_newton}, we can achieve global convergence if we notice the following. 
\begin{lemma} Let $f\in \LC(\Rn)$. Assume that $d_k$ satisfy \eqref{eq:gradient_related} with certain $c_1, c_2>0$, then a Line Search method on $x_k$ along $d_k$ with initial step size 1 terminates in a finite amount iteration with $t_k\geq \min \{1, \frac{2c_1(1-\alpha)\beta}{Lc_2^2}\}.$
\end{lemma}
\begin{proof}
	Exercise, it follows from \eqref{eq:gradient_related} and from the Descent Lemma (part 1).
\end{proof}
\begin{corollary}
	The iteration complexity of Algorithm \ref{alg:tncg} is $\BigO(\epsilon^{-2})$.
\end{corollary}
\begin{proof}
	Exercise, it follows from the line search condition.
\end{proof}
\begin{remark}
	The relaxation proposed in Proposition \ref{prop:truncated_newton}, obviously does not solve the slow convergence proved in Theorem \ref{thm:slow_newton} for the pure Newton method. In fact, when the directions have a negative curvature, the solution proposed by Algorithm \ref{alg:tncg} is to fall back on a gradient-like direction, which also have a $\BigO(\epsilon^{-2})$ iteration complexity. 
\end{remark}
\noindent Notice that in Proposition \ref{prop:truncated_newton}, we assume that $x_{k+1} = x_k + d_k$, while in Algorithm \ref{alg:tncg} we need to employ a line search technique to ensure a sufficient decrease. This may cut the Newton step $d_k$ possibly slowing down the local convergence rate. In order to ensure that this is not happening, we need the following Proposition. 
\begin{proposition}
	Let $f\in\Cii(\Rn)$ and let $\{x_k\}$ be generated by the Algorithm 2. Suppose that the following conditions hold:
	\begin{itemize}
		\item[(i)] $\{x_k\}$ converges to $x^*$, where $\grad(x^*)=0$ and $\hess(x^*)$ is positive definite.
		\item[(ii)] There exists an index $\hat{k}$ such that for all $k \geq \hat{k}$, the search direction $d_k$ is Newton's direction, that is,
		\begin{align*}
			d_k = -(\hess(x_k))^{-1} \grad(x_k).
		\end{align*}
	\end{itemize}
	Then, if $\alpha \in \left(0,\frac{1}{2}\right)$, there exists an index $k' \geq \hat{k}$ such that for all $k\geq k'$, it holds that
	\begin{align*}
		f(x_k+d_k) \leq f(x_k)+ \alpha \grad(x_k)^T d_k.
	\end{align*}
\end{proposition}
\begin{proof}
	Exercise.
\end{proof}
\noindent The very first nonmonotone line search was proposed to accept as often as possible the unitary step along a Newton direction \cite{grippo86a}.

\begin{example}
	Algorithm \ref{alg:tncg} (TNCG) is the current state-of-the-art method to train linear support vector machine and logistic regression models. In particular, given a sample $S=((x_i, y_i))_{i=1}^M$ of $M$ examples, with $x_i\in \Rn$ and $y_i\in\{-1,1\}$ we define the training problem as follows
	$$ \min_{w\in\Rn} f(w) = \frac{1}{2}||w||^2 + \lambda \sum_{i=1}^{M} \ell(y_i w^Tx_i),$$
	with  $\lambda>0$ the regularization parameter and the loss $\ell$ that is either
	\begin{itemize}
		\item the logistic loss $\ell(y w^Tx):= \log(1+\exp(-yw^Tx))$ or
		\item the squared hinge loss $\ell(y w^Tx):= (\max\{0, -yw^Tx\})^2$
	\end{itemize}
In the first case $f\in \Cii(\Rn)$ and in the second case $f\in \LC(\Rn)$, but TNCG can be still applied by generalizing the concept of Hessian with the use of Clarke's subdifferentials (left and right sub-differentials). 
Let us compute the gradient and the Hessian of $f$:
\begin{equation*}
	\begin{split}
		\grad (w) &= w+ \lambda\sum_{i=1}^{M} \ell'(y_i w^Tx_i)y_ix_i\\
		\hess(w) &= \Id + \lambda X^T D(w) X ,
	\end{split}
\end{equation*}
with $X= (x_1, \dots, x_M)^T$ and $D(w)$ a diagonal matrix such that $D(w)_{ii}= \ell''(y_iw^Tx_i)$, where $\ell''$ is the (generalized) second derivative of $\ell$. 
Thanks to the $\ell_2$ regularization, the Hessian is always positive definite (Exercise) and consequently $f$ is a strongly convex function. In particular, we can apply TNCG without worrying for negative curvature directions (basically removing the control in Step 5).
The Newton inequality is solved with a truncation, because the amount of features $n$ is usually very large (e.g., text data with millions of tokens) making it not viable load the Hessian in a RAM. In fact, let's say $n=50 \cdot 10^6$, then the matrix would need $n^2=2.5\cdot 10^{15}$ float which are each 32 bits = 4 bytes, for a total of $10 \cdot 10^{16}$ bytes= 10000 Terabytes (divided by 2 because the Hessian is symmetric).
On the other hand, the hessian can be computed by loading a single example (or a mini-batch of them) at the time, i.e., 
$$ \hess(w) = \Id + \lambda\sum_{i=1}^{M} D_{ii}(w) x_ix_i^T.$$
Moreover, CG only requires Hessian-vector products meaning that the whole Hessian is never explicitly formed (nor stored), only the $n$-dimensional vector $\hess(w)s$ (and their partial computations) are, i.e., 
$$ \hess(w)s = s + \lambda\sum_{i=1}^{M} D_{ii}(w) x_i(x_i^Ts).$$
\end{example}

\section{Trust-Region Methods}
This method is introduced to overcome the requirement of the pure Newton method of dealing with Hessian that are (at least in the neighborhood of $x^*$) positive definite. In fact,
\begin{equation*}
	\min_{x \in \mathbb{R}^n} q_k(x):= f(x_k) + \grad(x_k)^T (x - x_k) + \frac{1}{2}(x - x_k)^T \hess(x_k)(x - x_k).
\end{equation*}
may not admit a minimum otherwise. In order to take into account this issue, a suitable strategy could be that of performing the minimization of $q_k(s_k)$ on a neighborhood of $x_k$, i.e., 
\begin{equation}\label{eq:trust-region}
	\min_{\|s_k\| \leq \Delta_k} q_k(s_k), \quad \with s_k:=x-x_k.
\end{equation}
Notice the notation switch: in this and in the following section we will use $s_k$ instead of $d_k$, as these vectors are steps (they include the step size) and not directions (the step size is excluded). The radius $\Delta_k$ defining the spherical region around $x_k$ is usually determined in such a way that $f(x_{k+1})<f(x_k)$ and that the reduction of $f$ is close to that of the
quadratic model $q_k(s_k)$. In this way, the radius defines the region where the model can be considered reliable, i.e., the so-called trust region. 

\begin{algorithm}[H]\label{alg:trust-region}
	\caption{Trust-Region}
	
	\KwIn{Pick $x_0\in \Rn$ arbitrarly, $\epsilon>0, 0<\eta_1\leq \eta_2<1$ and $ 0 < \gamma_1< 1 < \gamma_2$.}
	
	$k = 0$
	
	\While{$||\grad(x_k)||>\epsilon$}{
		
		Compute a step $s_k$ as a solution to the problem \eqref{eq:trust-region}
		
		Compute $\rho_k:= \frac{f(x_k)-f(x_k+s_k)}{q_k(0)-q_k(s_k)}$
		
		\If{$\rho_k\geq \eta_1$}{$x_{k+1} = x_k +s_k$}
		
		\Else{$x_{k+1} = x_k$}
		
		$\Delta_{k+1} = \begin{cases}
			\gamma_2 \Delta_k \quad &\text{if } \rho_k\geq \eta_2\\
			\Delta_k \quad &\text{if } \rho_k\in [\eta_1, \eta_2)\\
			\gamma_1 \Delta_k \quad &\text{if } \rho_k<\eta_1\\
		\end{cases}$
		
		$k = k+1$
	}
\end{algorithm}
\noindent In order to study the convergence of Algorithm \ref{alg:trust-region}, we will simplify it. First of all we will allow $\hess(w_k)$ to be replaced by $B_k$, an approximation of the Hessian. We call $m_k$ the corresponding model. 
\begin{equation}\label{eq:first-order_tr}
	\min_{\|s_k\| \leq \Delta_k} m_k(s_k):= f(x_k) + \grad(x_k)^Ts_k + \frac{1}{2}s_k^T B_ks_k.
\end{equation}
Second, instead of  solving \eqref{eq:first-order_tr} exactly, we will solve it by finding a step $s_k$ such that 
\begin{equation}\label{eq:cauchy_point}
	m_k(s_k) \leq m_k(s_k^C), \quad \with s_k^C:= -t_k^C\grad(x_k), \quad t_k^C \in \argmin_{0\leq t\leq \frac{\Delta_k}{\|\grad(x_k)\|}} m_k(-t\grad(x_k)),
\end{equation}
where $s_k^C$ is called Cauchy step, achieving the best decrease possible along the anti-gradient, and $x_k^C:=x_k + s_k^C$ is called Cauchy point. The corresponding algorithm is reported here for completeness.

\begin{algorithm}[H]\label{alg:trust-region_C}
	\caption{Trust-Region with Cauchy point}
	
	\KwIn{Pick $x_0\in \Rn$ arbitrarly, $\epsilon>0, 0<\eta_1\leq \eta_2<1$ and $ 0 < \gamma_1< 1 < \gamma_2$.}
	
	$k = 0$
	
	\While{$||\grad(x_k)||>\epsilon$}{
		
		Compute a step $s_k$ that satisfies \eqref{eq:cauchy_point}. 
		
		Compute $\rho_k:= \frac{f(x_k)-f(x_k+s_k)}{m_k(0)-m_k(s_k)}$
		
		\If{$\rho_k\geq \eta_1$}{$x_{k+1} = x_k +s_k$}
		
		\Else{$x_{k+1} = x_k$}
		
		$\Delta_{k+1} = \begin{cases}
			\gamma_2 \Delta_k \quad &\text{if } \rho_k\geq \eta_2\\
			\Delta_k \quad &\text{if } \rho_k\in [\eta_1, \eta_2)\\
			\gamma_1 \Delta_k \quad &\text{if } \rho_k<\eta_1\\
		\end{cases}$
		
		$k = k+1$
	}
\end{algorithm}
\noindent Classical values for the above constants are $\eta_1=0.01, \eta_2=0.9, \gamma_1=0.5, \gamma_1=2$. We will thus study the convergence of Algorithm \ref{alg:trust-region_C} fist. Let us start by defining 
\begin{equation*}
	S := \{k \in \mathbb{N} \mid \rho_k \geq \eta_1\} \quad \text{and} \quad \mathcal{U}:= \mathbb{N} \setminus S,
\end{equation*}
the sets of \textbf{successful} and \textbf{unsuccessful} iterations, respectively, and
\begin{equation*}
	S_k := \{j \in [k] \mid \rho_j \geq \eta_1\} \quad \text{and} \quad \mathcal{U}_k := [k] \setminus S_k,
\end{equation*}
the corresponding sets up to iteration $k$. Notice that $x_{k+1} = x_k + s_k$ for $k \in S$, while $x_{k+1} = x_k$ for $k \in \mathcal{U}$.

We now state a property of Algorithm \ref{alg:trust-region_C} that solely depends on the mechanism to update the trust-region radius (Step 9).

\begin{lemma}[Successful and unsuccessful trust-region iterations]
	Let $\{x_k\}$ and $\{\Delta_k\}$ be the sequences generated by Algorithm \ref{alg:trust-region_C}. Assume that $\exists \; \Delta_{\min} > 0: \Delta_k \geq \Delta_{\min}$. Then
	\begin{equation*}
		k \leq |S_k| \left( 1 + \frac{\log \gamma_2}{|\log \gamma_1|} \right) + \frac{1}{|\log \gamma_1|} \left| \log \left( \frac{\Delta_{\min}}{\Delta_0} \right) \right|.
	\end{equation*}
\end{lemma}

\begin{proof}
	Exercise, it can be derived by noticing that $\Delta_{\min} \leq \Delta_k \leq \Delta_0 \gamma_2^{|S_k|} \gamma_1^{|\mathcal{U}_k|}$ and that $k= |S_k| + |\mathcal{U}_k|$.
\end{proof}
\noindent The first crucial result is well known in the trust-region literature as the Cauchy
decrease property, and assesses the magnitude of the function decrease at each iteration. From now on we will assume that $\grad(x_k)\neq 0$, otherwise the algorithm would have terminated.  
\begin{lemma}[Model decrease at the Cauchy point]\label{lemma:model_decrease}
	Given $s_k^C$ a Cauchy point as defined in \eqref{eq:cauchy_point}, we have
	\begin{equation}\label{eq:model_decrease}
		m_k(0)-m_k(s_k^C) \geq \frac{1}{2}\|\grad(x_k)\|\cdot \min\{\Delta_k, \frac{||\grad(x_k)||}{\|B_k\|} \}
	\end{equation}
\end{lemma}
\begin{proof}
	Let us rewrite $m_k$ as a function of one variable, $t_k$, i.e., 
	$$m_k(-t \grad(x_k))=f(x_k) -t_k \|\grad(x_k)\|^2 + \frac{1}{2}t_k^2\grad(x_k)^T B_k\grad(x_k),$$
	and let us notice that this is a parabola convex or concave depending on the sign of $\grad(x_k)^T B_k\grad(x_k)$. 
	Case I: $\grad(x_k)^T B_k \grad(x_k) \leq 0$. The parabola is concave, meaning that it is monotonically decreasing (along the gradient) for increasing values of $t > 0$, so that, the minimizer is the upper bound of the interval defining the feasible set, and hence we have
	\begin{equation*}
		t^*_k = \frac{\Delta_k}{\|\grad(x_k)\|}.
	\end{equation*}
Thus, 
\begin{align*}
	m_k(0) - m_k(s_k^C) & = \Delta_k \|\grad(x_k)\| - \frac{1}{2}{\left(t_k^C\right)}^2\grad(x_k)^T B_k\grad(x_k) \geq \Delta_k \|\grad(x_k)\|
\end{align*}
Case II: $\grad(x_k)^T B_k \grad(x_k) > 0$ (convex parabola) with the vertex that lies inside the trust-region, i.e., 
$$t^*_k = \frac{\|\grad(x_k)\|^2}{\grad(x_k)^T B_k \grad(x_k)}.$$
Thus, 
\begin{align*}
	m_k(0) - m_k(s_k^C) &=  t_k \left(\|\grad(x_k)\|^2 - \frac{1}{2} \|\grad(x_k)\|^2 \right)\\
	& = \frac{1}{2} t_k \|\grad(x_k)\|^2\\
	&\geq \frac{1}{2} \|\grad(x_k)\|^2 \cdot \frac{1}{||B_k||} %\geq \frac{1}{2} \|\grad(x_k)\|^2 \cdot \frac{1}{||B_k||}
\end{align*}
	Case III: $\grad(x_k)^T B_k \grad(x_k) > 0$ (convex parabola) with the vertex that lies outside the trust-region, i.e., 
	\begin{equation*}
		t^*_k = \frac{\Delta_k}{\|\grad(x_k)\|} \leq \frac{\|\grad(x_k)\|^2}{\grad(x_k)^T B_k \grad(x_k)},
	\end{equation*}
In particular, we can use the above inequality to get
\begin{align*}
m_k(s_k^C) -m_k(0) & =  -t_k^* \|\grad(x_k)\|^2 + \frac{1}{2} {t_k^*}^2 \grad(x_k)^T B_k \grad(x_k) \\
&\leq -t_k^* \|\grad(x_k)\|^2 + \frac{1}{2} t_k^* \|\grad(x_k)\|^2\\
& =-\frac{1}{2} t_k^* \|\grad(x_k)\|^2 = -\frac{1}{2} \Delta_k \|\grad(x_k)\|
\end{align*}
and consequently $m_k(0)- m_k(s_k^C) \geq \frac{1}{2} \Delta_k \|\grad(x_k)\|.$\\
Putting the three cases together we get \eqref{eq:model_decrease}.
\end{proof}

\begin{lemma}[Condition for a successful iteration]\label{lemma:successful_condition}
	Let $\{x_k\}$ be a sequence generated by Algorithm \ref{alg:trust-region_C}. Suppose that $f\in \LC$, that $\exists \, L_B>0: \|B_k\| \leq L_B$ and that
	\begin{equation}\label{eq:successful_condition}
		\Delta_k < \delta\|\grad(x_k)\|, \quad \with \delta:=\frac{(1 - \eta_2)}{2(L + L_B)}
	\end{equation}
	Then $\rho_k \geq \eta_2$, iteration $k$ is successful, and $\Delta_{k+1} \geq \Delta_k$.
\end{lemma}

\begin{proof}
	We obtain from \eqref{eq:successful_condition} and $0 < \eta_2 < 1$ that
	\begin{equation*}
		\Delta_k < \frac{\|\grad(x_k)\|}{2(L + L_B)} \leq \frac{\|\grad(x_k)\|}{L_B}.
	\end{equation*}
	As a consequence, together with Lemma \ref{lemma:model_decrease} and \eqref{eq:cauchy_point}, we get
	\begin{align*}
		m_k(0) - m_k(s_k) &\geq m_k(0) - m_k(s_k^c) \\
		&\geq \frac{1}{2} \|\grad(x_k)\| \min \left[ \frac{\|\grad(x_k)\|}{L_B}, \Delta_k \right] \\
		&= \frac{1}{2} \|\grad(x_k)\| \Delta_k.
	\end{align*}
	Now, from the Mean Value Theorem, the Cauchy--Schwarz inequality, $f\in \LC(\Rn)$ and $\|B_k\| \leq L_B$ we get
	\begin{equation*}
		|f(x_k + s_k) - m_k(s_k)| = |\left(\grad(x_k+\xi_k s_k) - \grad(x_k) \right)^Ts_k  - \frac{1}{2}  s_k^TB_k s_k | \leq (L + L_B) \|s_k\|^2
	\end{equation*}
	for some $\xi_k \in [0, 1]$. Combining this relation with the inequality above and using the bound $\|s_k\| \leq \Delta_k$, we then obtain that
	\begin{equation*}
		|\rho_k - 1| = \left| \frac{f(x_k + s_k) - m_k(s_k)}{m_k(0) - m_k(s_k)} \right| \leq \frac{(L + L_B) \Delta_k^2}{\frac{1}{2} \|\grad(x_k)\| \Delta_k} \leq 1 - \eta_2
	\end{equation*}
	because of \eqref{eq:successful_condition}. Hence $\rho_k \geq \eta_2 \geq \eta_1$, $k \in S_k$, and the remaining conclusion follows from Step 9 of Algorithm \ref{alg:trust-region_C}. 
\end{proof} 

\begin{lemma}[Lower bound for the trust-region radius]\label{lemma:deltamin}
	Let $\{x_k\}$ be a sequence generated by Algorithm \ref{alg:trust-region_C}. Suppose that $f\in \LC$, that $\exists\, L_B>0: \|B_k\| \leq L_B$ and that $\Delta_0\geq \gamma_1 \delta \|\grad(x_0)\|$. Then we have that
	\begin{equation}\label{eq:deltamin}
		\Delta_k \geq \gamma_1 \delta \min_{i \in [0:k]} \|\grad(x_i)\| \quad \forall k\geq 0,
	\end{equation}
	where $\delta$ is defined in Lemma \ref{lemma:successful_condition}.
\end{lemma}
\begin{proof}
By assumption, \eqref{eq:deltamin} holds for $k = 0$. By contradiction, assume that iteration $k \geq 1$ is the first such that \eqref{eq:deltamin} fails. Since Step 9 of Algorithm \ref{alg:trust-region_C} ensures that $\gamma_1 \Delta_{k-1} \leq \Delta_k$, we deduce that
\begin{equation*}
	\Delta_{k-1} \leq \frac{1}{\gamma_1}\Delta_k < \delta \min_{i \in [0:k]} \|\grad(x_i)\|,
\end{equation*}
and consequently also
\begin{equation*}
	\frac{\Delta_{k-1}}{\|\grad(x_{k-1})\|} \leq \frac{\Delta_{k-1}}{\min_{i \in [0:k-1]} \|\grad(x_i)\|} \leq \frac{\Delta_{k-1}}{\min_{i \in [0:k]} \|\grad(x_i)\|} < \delta.
\end{equation*}
Hence Lemma \ref{lemma:successful_condition} guarantees that $\Delta_{k-1} \leq \Delta_k$, and therefore, from the fact that \eqref{eq:deltamin} is violated at iteration $k$, we get
\begin{equation*}
	\frac{\Delta_{k-1}}{\min_{i \in [0:k-1]} \|\grad(x_i)\|} \leq \frac{\Delta_k}{\min_{i \in [0:k]} \|\grad(x_i)\|} < \gamma_1 \delta.
\end{equation*}
As a consequence, \eqref{eq:deltamin} is also violated at iteration $k - 1$. But this contradicts the assumption that iteration $k$ is the first such that this inequality fails. 
\end{proof}
\noindent Notice that the assumption $\Delta_0\geq \gamma_1 \delta \|\grad(x_0)\|$ can be removed, but the corresponding analysis only gets heavier, but does not add insights.

\begin{lemma}[Model decrease]\label{lemma:model_decrease2}
	Let $\{x_k\}$ be a sequence generated by Algorithm \ref{alg:trust-region_C}. Suppose that $f\in \LC$, that $\exists\, L_B>0: \|B_k\| \leq L_B$ and that $\Delta_0\geq \gamma_1 \delta \|\grad(x_0)\|$. Then we have that, for all $k \geq 0$,
	\begin{equation*}
		m_k(0) - m_k(s_k) \geq c_1 \|\grad(x_k)\| \min_{i \in [0:k]} \|\grad(x_i)\|,
	\end{equation*}
	where
	\begin{equation*}
		c_1 := \frac{1}{2} \min \left[ \frac{1}{L_B}, \gamma_1\delta \right] \quad \with \delta \text{ defined in Lemma \ref{lemma:successful_condition}}.
	\end{equation*}
\end{lemma}

\begin{proof}
	Exercise, it follows from \eqref{eq:cauchy_point}, Lemma \ref{lemma:model_decrease} and Lemma \ref{lemma:deltamin}.
\end{proof}

\noindent We are now in a position to bound the total number of successful iterations necessary to find an $\epsilon$-approximate first-order minimizer using Algorithm \ref{alg:trust-region_C}.

\begin{lemma}[Bound on the number of successful iterations of Algorithm \ref{alg:trust-region_C}] \label{lemma:tr_succ}
	Assume that $f\in \LC$, that $\exists\, L_B>0: \|B_k\| \leq L_B$ and that $\Delta_0\geq \gamma_1 \delta \|\grad(x_0)\|$. Additionally, let $f$ be lower bounded by $f^*$. Then Algorithm \ref{alg:trust-region_C} requires at most $\frac{f(x_0) - f^*}{\eta_1c_1\epsilon^2}$ successful iterations before an iterate $x_{k_\epsilon+1}$ is computed for which $\|\grad(x_{k_\epsilon+1})\| \leq \epsilon$.
\end{lemma}
\begin{proof}
First recall that if $j\in \mathcal{U}$, we have $f(x_j) = f(x_{j+1})$ and $\grad(x_j)=\grad(x_{j+1})$. As $k_\epsilon+1$ is the first iteration for which $\|\grad(x_{k_\epsilon+1})\| \leq \epsilon$, we have $\|\grad(x_k)\|>\epsilon \; \forall \, k <k_\epsilon+1$. This, together with the fact that $f(x)\geq f^*$ and Lemma \ref{lemma:model_decrease2}, by telescoping sum brings to 
\begin{align*}
	f(x_0)-f^* &\geq f(x_0) - f(x_{k_\epsilon+1})\\
	& =\sum_{j\in S_{k_\epsilon}} f(x_j) - f(x_{j+1})\\
	& \geq \eta_1 \sum_{j\in S_{k_\epsilon}} m_j(0)- m_j(s_j)\\
	& \geq \eta_1 c_1 |S_{k_\epsilon}| \epsilon^2,
\end{align*}
where the second inequality follows from Step 5 of Algorithm \ref{alg:trust-region_C}. Thus, $|S_k|\leq  \frac{f(x_0) - f^*}{\eta_1c_1 \epsilon^2}.$
\end{proof}

\begin{theorem}
	Assume that $f\in \LC$, $f$ be lower bounded by $f^*$, that $\exists\, L_B>0: \|B_k\| \leq L_B$ and that $\Delta_0\geq \gamma_1 \delta \|\grad(x_0)\|$. Algorithm \ref{alg:trust-region_C} has a iteration complexity of $\BigO(\epsilon^{-2})$.
\end{theorem}
\begin{proof}
	Exercise, by computing the precise constants.
\end{proof}
Now that we have the convergence rate of Algorithm \ref{alg:trust-region_C}, let us move back to the convergence analysis of Algorithm \ref{alg:trust-region} and see whether employing directly the Hessian, instead of $B_k$ will allow us to achieve a better rate. Unfortunately, the Trust-Region algorithm (Algorithm \ref{alg:trust-region}) inherits the same slow convergence of Newton. In fact, we have the following lower bound.

\begin{theorem}\label{thm:slow_trust_region}
	Algorithm \ref{alg:trust-region} applied to minimize a function $f\in \LC(\Rn)$ bounded from below may require as many as $\epsilon^{-2}$ iterations to produce an iterate $x\in \Rn$ such that $||\grad (x)\|\leq \epsilon.$
\end{theorem}
\begin{proof}
	Exercise, one needs to prove that the sequence of iterates in Theorem \ref{thm:slow_newton} can be generated also by Algorithm \ref{alg:trust-region}. In particular, the function defined in Theorem \ref{thm:slow_newton} behaves quadratically around the iterates, yielding decreases in $f$ that fully align ($\rho_k=1$) with those of the quadratic model employed by the trust-region method.
\end{proof}
Given this result, there is no need to solve $q_k(s)$ to higher precision than that achieved by the Cauchy step (at least when concerned with the global speed of convergence). In particular, we can accept a step $s_k$ such that 
\begin{equation*}
	q_k(s_k) \leq q_k(s_k^C), \quad \with s_k^C:= -t_k^C\grad(x_k), \quad t_k^C \in \argmin_{0\leq t\leq \frac{\Delta_k}{\|\grad(x_k)\|}} m_k(-t\grad(x_k)),
\end{equation*}
where the only difference with \eqref{eq:cauchy_point} is that we use $\hess(x_k)$ instead of $B_k$. For this algorithm, and consequently also for Algorithm \ref{alg:trust-region} the convergence speed can be inherited from that of Algorithm \ref{alg:trust-region_C} when we notice that $\hess(x_k)$ is a special case of $B_k$. For this result, the assumption that $\|B_k\|\leq L_B$ is not needed, as the Hessian is upper bounded if and only if $f\in \LC(\Rn)$, which was already a requirement of the results above. 

Concerning the local convergence, if $\{x_k\}$ converges towards $x^*$ and $\hess(x^*)\succ0$, Algorithm \ref{alg:trust-region} will achieve local quadratic convergence, as the resulting steps around $x^*$ will be the same of pure Newton. In fact, similarly to the line search-based globalization of Newton, the quadratic model will be accurate and the trust-region radius will be always doubled (multiplied by $\gamma_2$).

\section{Cubic Regularized Methods}
The adaptive-regularization algorithm may be motivated in two different ways. The first
is to consider the method as a variant of the second-order trust-region scheme, in which
the steplength is controlled not explicitly (by requiring that $\|s_k\|\leq \Delta_k$), but implicitly
instead. This is achieved by adding a term to the quadratic model $q_k(s)$ used in \eqref{eq:trust-region} to penalize long steps. Thus, rather than updating $\Delta_k$ from iteration to iteration, it is the strength of the penalization that is updated. However, this says little about which form the penalization should take.
\par Instead, let us motivate the method with the following proposition.

\begin{proposition}\label{prop:arc}
	Let $f\in \LCii$. Then we have
	\begin{itemize}
		\item[(i)] $\|\nabla f(x + s) - \nabla f(x) - \hess(x)s\| \leq \frac{L}{2}\|s\|^2$ $\forall s \in \mathbb{R}^n$,
		\item[(ii)] $|f(x + s) - f(x) - \nabla f(x)^T s - \frac{1}{2}s^T \hess(x)s| \leq \frac{L}{6}\|s\|^3$ $\forall s \in \mathbb{R}^n$.
	\end{itemize}
\end{proposition}
\begin{proof}
	We choose $x, s \in \mathbb{R}^n$. By the mean value theorem we can write
	\begin{equation*}
		\nabla f(x + s) - \nabla f(x) = \int_0^1 \hess(x + ts) sdt.
	\end{equation*}
	Subtracting $\hess(x)s$ from both sides and applying the norm, we obtain
	\begin{align*}
		\|\nabla f(x + s) - \nabla f(x) - \hess(x)s\| &\leq \int_0^1 \|(\hess(x + ts) - \hess(x))s\| dt \\
		&\leq \|s\| \int_0^1 \|\hess(x + ts) - \hess(x)\| dt \\
		&\leq L\|s\|^2 \int_0^1 t dt = \frac{L}{2}\|s\|^2,
	\end{align*}
where the last inequality follows from the fact that $\hess$ is Lipschitz continuous. Hence point (i) is verified.
\par Using the mean value theorem again, we can write
	\begin{equation*}
		f(x + s) - f(x) = \int_0^1 \nabla f(x + ts)^T s dt.
	\end{equation*}
	Subtracting $\nabla f(x)^T s + \frac{1}{2}s^T \hess(x)s$ from both sides and applying the absolute value, we obtain
	\begin{align*}
		\left|f(x + s) - f(x) - \nabla f(x)^T s - \frac{1}{2}s^T \hess(x)s\right| &= \left|\int_0^1 (\nabla f(x + ts) - \nabla f(x) - t\hess(x)s)^T s dt\right| \\
		&\leq \int_0^1 |(\nabla f(x + ts) - \nabla f(x) - t\hess(x)s)^T s| dt \\
		&\leq \|s\| \int_0^1 \|\nabla f(x + ts) - \nabla f(x) - t\hess(x)s\| dt \\
		&\leq \frac{L}{2}\|s\|^3 \int_0^1 t^2 dt = \frac{L}{6}\|s\|^3,
	\end{align*}
	where the second inequality follows from the Cauchy-Schwarz inequality, while the last inequality follows from the first point of the proposition. Hence point (ii) is also verified.
\end{proof}
Thus, at each iteration $k$, we can consider a cubic model $m_k(s)$ that will give a (local) upper bound for $f$, i.e., 
$$ f(x_k+s) \leq m_k(s):= f(x_k) +\nabla f(x_k)^T s + \frac{1}{2}s^T \hess(x_k)s + \frac{L}{6}\|s\|^3.$$
The term $\frac{L}{6}\|s\|^3$ is called cubic regularization term, and it is summed to the quadratic model $q_k(s)$, (reminder $q_k(s) := f(x_k)+ \grad(x_k)^Ts + \frac{1}{2}s^T\hess(x_k) s).$
\begin{remark}
	Notice that $m_k(s)$ always admits minimum as it is a coercive function. This solves the problem of the negative curvature directions, in fact, even if $s^T \hess(x)s<0$, meaning that the second-order term can be as negative as $-||s||^2 ||\hess(x_k)||$, the cubic term will be able to make up for it with $||s||$ large enough, i.e., along that direction we can find a step $s$ such that $\frac{L}{6} ||s||^3 -||s||^2 ||\hess(x_k)||>0$.
\end{remark}
Now, by minimizing the model $m_k(s)$ at each iteration $k$, we will ensure a decrease in the function value, however, there are two practical difficulties:
\begin{itemize}
	\item we don't know $L$;
	\item $m_k(s)$ may be difficult to solve exactly.
\end{itemize}
To fix the first problem, instead of $L$ we use an approximation $\sigma_k$, i.e., 
\begin{equation}\label{eq:cubic_reg_model}
	m_k(s) := f(x_k)+ \grad(x_k)^Ts + \frac{1}{2}s^T\hess(x_k)s + \frac{\sigma_k}{6} ||s||^3,
\end{equation}
which will be adapted in a similar way as the trust-region radius. 
\par To fix the second problem, we will not solve $m_k(s)$ exactly, but it will be sufficient to achieve a decrease in the model, i.e., $m_k(s)\leq m_k(0)$ and that the gradient of the model behaves quadratic in $s$, \\i.e., $\|\nabla m_k(s)\|\leq \frac{1}{2}\theta ||s||^2$ (as the model itself behave like a cubic in $\|s\|$).


\begin{algorithm}[H]\label{alg:arc}
	\caption{Adaptive Regularization with Cubic (ARC)}
	
	\KwIn{Pick $x_0\in \Rn$ arbitrarly, $\epsilon>0, 0<\eta_1\leq \eta_2<1$ and $ 0 < \gamma_1< 1 < \gamma_2$, $\sigma_{min}<\sigma_0$, $\theta>0$.}
	
	$k = 0$
	
	\While{$||\grad(x_k)||>\epsilon$}{
		
		Compute a step $s_k$ such that $m_k(s_k)\leq m_k(0)$ and $\|\nabla m_k(s_k)\|\leq \frac{1}{2}\theta ||s_k||^2$
		
		Compute $\rho_k:= \frac{f(x_k)-f(x_k+s_k)}{q_k(0)-q_k(s_k)}$
		
		\If{$\rho_k\geq \eta_1$}{$x_{k+1} = x_k +s_k$}
		
		\Else{$x_{k+1} = x_k$}
		
		$\sigma_{k+1} = \begin{cases}
			\max\{\gamma_1 \sigma_k, \sigma_{min} \} \quad &\text{if } \rho_k\geq \eta_2\\
			\sigma_k \quad &\text{if } \rho_k\in [\eta_1, \eta_2)\\
			\gamma_2 \sigma_k \quad &\text{if } \rho_k<\eta_1\\
		\end{cases}$
		
		$k = k+1$
	}
\end{algorithm}
\begin{remark}
	$ $
	\begin{itemize}
		\item Differently from the trust-region method, where the model used was the same in Step 3 and to compute $\rho_k$, here $s_k$ is computed minimizing $m_k(s)$, while $\rho_k$ is computed evaluating the accordance between $f$ and the quadratic model (without regularization). 
		\item $\sigma_k$ plays a role which is reciprocal to that of the trust-region radius, in fact, when the model do not agree with the function, the next step needs to be smaller, otherwise, when they strongly agree, we can reduce $\sigma_k$ to take larger steps. 
	\end{itemize}
\end{remark}

Let us now prove convergence for the method and let us notice that the proof structure is quite similar to that of the Trust Region method. 

\begin{lemma}[Successful and unsuccessful adaptive-regularization iterations]\label{lemma:arc_success}
Let $\{x_k\}$ and $\{\sigma_k\}$ be the sequences generated by Algorithm \ref{alg:arc}. Assume that $\exists \; \sigma_{\max} > 0: \sigma_k\leq \sigma_{\max}$. Then
\begin{equation*}
	k \leq |S_k| \left(1 + \frac{|\log \gamma_1|}{\log \gamma_2}\right) + \frac{1}{\log \gamma_2} \log \left(\frac{\sigma_{\max}}{\sigma_0}\right).
\end{equation*}
\end{lemma}
\begin{proof}
	Exercise, follows from $\sigma_0 \gamma_1^{|S_k|} \gamma_2^{|U_k|} \leq \sigma_k\leq \sigma_{\max}$ and $k = |S_k| + |\mathcal{U}_k|$.
\end{proof}

\begin{lemma}[Decrease in the quadratic model]\label{lemma:arc_decrease}
	Let $s_k$ be computed as in Step 3 of Algorithm \ref{alg:arc}, then
	\begin{equation*}
		q_k(0)- q_k(s_k) > \frac{\sigma_k}{6} \|s_k\|^3.
	\end{equation*}
\end{lemma}
\begin{proof}
	Exercise.
\end{proof}

\noindent We now show that the regularization parameter $\sigma_k$ remains upper bounded. Note in case of the trust-region radius, we needed it to be lower bounded.

\begin{lemma}[Upper bound on the regularization parameter]\label{lemma:arc_ub}
	Let $f\in \LCii(\Rn)$. Let $\{x_k\}$ be generated by Algorithm \ref{alg:arc}. Then
	\begin{equation*}
		\sigma_k \leq \sigma_{\max} :=\max \left[ \sigma_0, \frac{\gamma_2 L}{1 - \eta_2} \right].
	\end{equation*}
\end{lemma}
\begin{proof}
	From the definition of $\rho_k$, Proposition \ref{prop:arc} and Lemma \ref{lemma:arc_decrease}, we obtain that
	\begin{align*}
		|\rho_k - 1| &= \frac{|f(x_k) - f(x_k + s_k) - q_k(0) + q_k(s_k)|}{|q_k(0) - q_k(s_k)|} \\
		&= \frac{|f(x_k + s_k) - q_k(s_k)|}{|q_k(0) - q_k(s_k)|} \\
		&\leq \frac{L}{\sigma_k}.
	\end{align*}
	Thus, if $\sigma_k \geq L/(1 - \eta_2)$, then $\rho_k \geq \eta_2\geq\eta_1$, meaning that iteration $k$ is successful and Step 9 of Algorithm \ref{alg:arc} implies that $\sigma_{k+1} \leq \sigma_k$. In other words, as soon as $\sigma_k$ crosses the threshold $L/(1 - \eta_2)$, the corresponding iteration will be successful, shrinking $\sigma_k$. At the same time, if $\sigma_{k-1}$ is be just below $L/(1 - \eta_2)$, and $k-1$ is a successful iteration, $\sigma_k$ will be increased by $\gamma_2$, bringing to the possibility that $\sigma_k$ reaches $\gamma_2L/(1 - \eta_2)$. Moreover, if $\sigma_0\geq \gamma_2L/(1 - \eta_2)$ already from the start of Algorithm \ref{alg:arc}, then $\sigma_1$ will be reduced and all the following $\sigma_k$ will stay below $\frac{L}{(1 - \eta_2)}<\frac{\gamma_2L}{(1 - \eta_2)}\leq \sigma_0$. 	 
\end{proof}

\noindent Next, we establish a crucial lower bound on the step length.

\begin{lemma}[Lower bound on the length of the step]\label{lemma:arc_lb}
	Let $f\in \LCii(\Rn)$. Let $\{x_k\}$ be generated by Algorithm \ref{alg:arc}. Then, if iteration $k$ is successful, we have
	\begin{equation*}
		\|s_k\|^2 \geq \frac{2}{L + \theta + \sigma_{\max}} \|\grad(x_{k+1})\|,
	\end{equation*}
where $\sigma_{max}$ is defined in Lemma \ref{lemma:arc_ub}.
\end{lemma}

\begin{proof}
Since iteration $k$ is successful, we have $\grad(x_{k+1}) = \grad(x_k + s_k)$. From $f\in \LCii(\Rn)$ and Proposition \ref{prop:arc} (i) to deduce that
\begin{equation*}
	\|\grad(x_{k+1}) - \nabla q_k(s_k)\| = \|\grad(x_k + s_k) - \nabla q_k(s_k)\| \leq \frac{1}{2}L\|s_k\|^2.
\end{equation*}	
We also deduce by computing $\nabla m_k(s_k)$ and Step 3 of Algorithm \ref{alg:arc} that
\begin{equation*}
	\left \|\nabla q_k(s_k) + \frac{1}{2}\sigma_k \|s_k\| s_k \right\|=\|\nabla m_k(s_k)\| \leq \frac{1}{2}\theta \|s_k\|^2.
\end{equation*}
Now, from the last two inequalities, and adding and subtracting $\nabla q_k(s_k)$ and $\frac{1}{2}\sigma_k \|s_k\| s_k$, we have 
\begin{align*}
	\|\grad(x_{k+1})\| &= \left\|\grad(x_{k+1}) - \nabla q_k(s_k) + \nabla q_k(s_k) + \frac{1}{2}\sigma_k \|s_k\| s_k - \frac{1}{2}\sigma_k \|s_k\| s_k \right\|\\
	&\leq \left \|\grad(x_{k+1}) - \nabla q_k(s_k) \right \|  + \left\|\nabla q_k(s_k) + \frac{1}{2}\sigma_k \|s_k\| s_k \right \| + \frac{1}{2}\sigma_k \|s_k\|^2\\
	&\leq \frac{1}{2}L\|s_k\|^2 + \frac{1}{2}\theta \|s_k\|^2+ \frac{1}{2}\sigma_k \|s_k\|^2,
\end{align*} 
which concludes the proof.
\end{proof}
\pagebreak 
\noindent We are now ready to derive an upper bound on the number of successful iterations of Algorithm \ref{alg:arc}.

\begin{lemma}[Bound on the number of successful iterations]
	Let $f\in \LCii(\Rn)$ and let $f$ be lower bounded by $f^*$. Then Algorithm \ref{alg:arc} requires at most $K$ successful iterations before an iterate $x_{k_\epsilon+1}$ is computed for which $\|\grad(x_{k_\epsilon+1})\| \leq \epsilon$, where $K:= \frac{6(L + \theta + \sigma_{\max})^{3/2}}{2^{3/2} \eta_1 \sigma_{\min}} \frac{f(x_0) - f^*}{\epsilon^{3/2}}$ and $\sigma_{\max}$ is defined as in Lemma \ref{lemma:arc_ub}.
\end{lemma}

\begin{proof}
	Similar to the proof of Lemma \ref{lemma:tr_succ} if $j\in \mathcal{U}$, we have $f(x_j) = f(x_{j+1})$ and $\grad(x_j)=\grad(x_{j+1})$. Thus, by telescopic sum, we have 
\begin{align*}
	f(x_0) - f^* &\geq f(x_0) - f(x_{k_\epsilon+1}) \\
	&= \sum_{j \in S_{k_\epsilon}} [f(x_j) - f(x_{j+1})] \\
	&\geq \eta_1 \sum_{j \in S_{k_\epsilon}} [q_j(0) - q_j(s_j)],
\end{align*}
where the last inequality follows from Step 5 of Algorithm \ref{alg:arc}. We now use Lemma \ref{lemma:arc_decrease} into the above and the fact that $\sigma_k \geq \sigma_{\min}$ for all $k$ (because of Step 9 of Algorithm \ref{alg:arc}), and obtain that
\begin{equation*}
	f(x_0) - f^* \geq \frac{1}{6}\eta_1 \sigma_{\min} \sum_{j \in S_{k_\epsilon}} \|s_j\|^3 \geq \frac{2^{3/2} \eta_1 \sigma_{\min}}{6(L + \theta + \sigma_{\max})^{3/2}} \sum_{j \in S_{k_\epsilon}} \|\grad(x_{j+1})\|^{3/2}> \frac{2^{3/2} \eta_1 \sigma_{\min}}{6(L + \theta + \sigma_{\max})^{3/2}} |S_{k_\epsilon}| \epsilon^{3/2}
\end{equation*}
where we used Lemma \ref{lemma:arc_lb} to deduce the second inequality and $\|\grad(x_k)\|>\epsilon \; \forall k<k_\epsilon+1$ to achieve the third one.
\end{proof}

\noindent We may now conclude our analysis calling upon Lemma \ref{lemma:arc_success}.
\begin{theorem}
	Assume that $f\in \LCii$, $f$ be lower bounded by $f^*$, that $\exists\, L_B>0: \|B_k\| \leq L_B$ and that $\Delta_0\geq \gamma_1 \delta \|\grad(x_0)\|$. Algorithm \ref{alg:arc} has a iteration complexity of $\BigO(\epsilon^{-3/2})$.
\end{theorem}
\begin{proof}
	Exercise, by computing the precise constants.
\end{proof}

\begin{theorem}
	Algorithm \ref{alg:arc} applied to minimize a function $f\in \LCii(\Rn)$ bounded from below may require as many as $\epsilon^{-3/2}$ iterations to produce an iterate $x\in \Rn$ such that $||\grad (x)\|\leq \epsilon.$
\end{theorem}
\noindent The proof of this theorem is out of the scope of this course, see the proof of Theorem 12.2.15 from \cite{cartis22a}.


\section{Quasi-Newton Methods}
While (pure) Newton method is described by the iteration
\begin{equation*}
	x_{k+1} = x_k - \hess(x_k)^{-1} \grad(x_k),
\end{equation*}
Quasi-Newton iteration generalizes this approach to use another matrix $B_k$, which may be for instance be an approximation of the Hessian
\begin{equation*}
	x_{k+1} = x_k - t_kB_k^{-1} \grad(x_k),
\end{equation*}
and $t_k$ chosen with a line search procedure. In order to choose $B_k$, one can notice that when $f$ is quadratic, i.e., $f(x):= \frac{1}{2}x^TQx -c^Tx$, with $Q$ a symmetric matrix, we have that $\nabla f(x) = Qx - c$ and hence, given two points $x$ and $y$ in $\Rn$, we can write
\begin{equation*}
	\nabla f(y) - \nabla f(x) = Q(y - x).
\end{equation*}
Thus, in the general case, we can think of determining the matrix $B_{k+1}$ in a way that the following condition (known as \textit{Quasi-Newton equation}) is satisfied:
\begin{equation}\label{eq:quasi-newton}
\nabla f(x_{k+1}) - \nabla f(x_k) = B_{k+1}(x_{k+1} - x_k).
\end{equation}
This implies that we impose a condition that the Hessian matrix actually satisfies when $f$ is quadratic. With
\begin{align*}
 	s_k &= x_{k+1} - x_k \\
 	y_k &= \nabla f(x_{k+1}) - \nabla f(x_k),
\end{align*}
we can determine $B_{k+1}$ by updating $B_k$ in the following way
\begin{equation*}
	B_{k+1} = B_k + \Delta B_k, \quad \text{with } y_k = (B_k + \Delta B_k) s_k.
\end{equation*}
Following a similar reasoning we can refer to the Quasi-Newton equation written in the form
\begin{equation*}
	H_{k+1}(\nabla f(x_{k+1}) - \nabla f(x_k)) = x_{k+1} - x_k,
\end{equation*}
where now $H_{k+1}$ is intended as an approximation to the inverse of the Hessian. In this case, a Quasi-Newton iteration is of the form
\begin{equation*}
	x_{k+1} = x_k - t_k H_k \nabla f(x_k),
\end{equation*}
and the updating rule becomes:
\begin{equation*}
	H_{k+1} = H_k + \Delta H_k, \quad \text{with } (H_k + \Delta H_k) y_k = s_k.
\end{equation*}
We call \textbf{direct formulae} those working on $B_k$ and \textbf{inverse formulae} those updating instead $H_k$.

When solving system of nonlinear equations $F(x) = 0$, where $F : \Rn \to \Rn$, the Quasi-Newton methods can be viewed as $n$-dimensional extensions of the secant method and can be defined through the direct formula
\begin{equation*}
	x_{k+1} = x_k - t_k B_k^{-1} F(x_k),
\end{equation*}
or the inverse formula
\begin{equation*}
	x_{k+1} = x_k - t_k H_k F(x_k),
\end{equation*}
where $B_k$ and $H_k$ can be viewed as approximations, respectively, of $J(x_k)$ or $J(x_k)^{-1}$, and $J(x)$ is the Jacobian matrix of $F$, assumed to be non singular.\\
The requirement imposed on $B_{k+1}$ in \eqref{eq:quasi-newton} is not yet an algorithm, as it does not uniquely characterize the matrix $B_{k+1}$. However, if we limit ourselves to updates $\Delta_k B_k$ having rank 1 or 2, i.e., 
$$\Delta_k B_k = t_k v_k v_k^T \quad (rank 1)$$
$$\Delta_k B_k = t_k u_k u_k^T + \beta_k v_k v_k^T \quad (rank 2)$$
and by adding further conditions on the vectors $u_k$ and $v_k$, we can obtain the following update formulas
\begin{align*}
	 \Delta B_k&:=\frac{(y_k - B_k s_k) s_k^T }{s_k^T s_k}\quad &(Broyden)\\
	 \Delta B_k&:=\frac{y_k y_k^T}{y_k^T s_k} - \frac{B_k s_k s_k^TB_k}{s_k^T B_k s_k}  \quad &(Broyden-Fletcher-Goldfarb-Shanno\;(BFGS))
\end{align*}

Let us now focus on the BFGS method, which has been shown to achieve remarkable results in practice for optimizing general functions. In order to prove convergence for this method, we fist need to introduce the following Wolfe line search, 
\begin{equation}\label{eq:wolfe}
	\begin{split}
		f(x_{k+1}) &\leq f(x_k) + \alpha \grad(x_k)^Td_k\\
		\grad(x_{k+1})^T d_k&\geq \sigma \grad(x_k)^Td_k, \quad \with \sigma\in(0,1)
	\end{split}
\end{equation}
where the first inequality is the Armijo condition. The second condition implies that the directional derivative in the new point $x_{k+1}$ is at least $\sigma$-times than that in $x_k$. More precisely this imposes that the slope of $\phi$ at $t_k$ either is non negative (see Figure \ref{fig:wolfe}), or, when negative, has a modulus not grater than $|\sigma \grad(x_k)^Td_k|$. It can be shown that if we have $0<\alpha<\sigma<1$ then there exists an interval of acceptable $t$-values for the Wolfe condition (Exercise), ensuring also that $t$ is not too small. 

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{weak_wolfe}
	\caption{Wolfe line search condition.}
	\label{fig:wolfe}
\end{figure}

\begin{proposition}[Global Convergence in the Convex Case] Let $f \in \Cii(\mathcal{D})$, with $\mathcal{D}$ open and containing the level set $\mathcal{L}_0$ and suppose that $\mathcal{L}_0$ is compact. Let $\{x_k\}$ be an infinite sequence generated by the BFGS method with Wolfe line search where $\sigma \in (0, 1)$, $0 < c < \min[\sigma, 1/2]$. Then,
	$$ \lim_{k\to \infty} ||\grad(x_k)\| = 0.$$
\end{proposition}
\noindent In particular, the above proposition follows from the fact that if $B_k\succ 0$, thanks to the Wolfe condition we have
$$ \grad(x_{k+1})^Td_k \geq \sigma \grad(x_k)^Td_k > \grad(x_k)^Td_k,$$
which also means that $y_k^Ts_k>0$. In particular, together with 
\begin{lemma}
	For the BFGS update, if $B_k \succ 0$ and $y_k^T s_k > 0$, then $B_{k+1} \succ 0$.
\end{lemma}
\begin{proof}
	Exercise, use and prove that 
	\begin{equation*}
		B_{k+1} = (I - \rho_k y_k s_k^T) B_k (I - \rho_k s_k y_k^T) + \rho_k y_k y_k^T
	\end{equation*}
	where $\rho_k = \frac{1}{y_k^T s_k} > 0$.
%	For any $z \neq 0$, we have:
%	\begin{equation*}
%		z^T B_{k+1} z = \tilde{z}^T B_k \tilde{z} + \rho_k (z^T y_k)^2
%	\end{equation*}
%	where $\tilde{z} = (I - \rho_k s_k y_k^T) z$.
%	We consider two cases:
%	\textbf{Case 1:} If $\tilde{z} \neq 0$, then $z^T B_{k+1} z > 0$ since $B_k \succ 0$ and $\rho_k (z^T y_k)^2 \geq 0$.
%	\textbf{Case 2:} If $\tilde{z} = 0$, then $(I - \rho_k s_k y_k^T) z = 0$, which implies:
%	\begin{equation*}
%		z = \rho_k (z^T y_k) s_k
%	\end{equation*}
%	In this case:
%	\begin{equation*}
%		z^T B_{k+1} z = \rho_k (z^T y_k)^2 > 0
%	\end{equation*}
%	unless $z^T y_k = 0$, which would imply $z = 0$ from $z = \rho_k (z^T y_k) s_k$.
%	Therefore, for any $z \neq 0$, we have $z^T B_{k+1} z > 0$, which means $B_{k+1} \succ 0$.
\end{proof}
\noindent Convexity, instead is used to ensure that the matrix $B_k$ remains bounded, i.e., $||B_k||\leq M$. Whether BFGS can converge beyond convex functions is still an open question. 
%
%In particular, it can be observed that the convergence result given in Proposition 15.3 depends essentially on the fact that the convexity assumption implies the validity of the following inequality for all $k$ and for some $M > 0$
%
%\begin{equation*}
%	\frac{\|y_k\|^2}{s_k^T y_k} \leq M.
%\end{equation*}
%
%Thus, the assertion of Proposition 15.3 holds also for non convex functions, provided that (15.28) is satisfied. In the general, non convex case, if validity of (15.28) cannot be established, the global convergence of the (unmodified) BFGS method has not been demonstrated. We note that the model algorithm given in the preceding section

%Another interpretation of many Quasi-Newton methods can be that of viewing the search direction employed in the method as the steepest descent direction with respect to the non-Euclidean metric derived from the norm
%\begin{equation*}
%	\|x\|_{B_k} = \left[x^T B_k x\right]^{1/2},
%\end{equation*}
%where $B_k$ is assumed to be a positive definite symmetric matrix. In fact the direction that minimizes the directional derivative among those with $\|d\|_{B_k} = 1$ is given by
%\begin{equation*}
%	d_k = -\frac{B_k^{-1} \nabla f(x_k)}{\left[\nabla f(x_k)^T B_k^{-1} \nabla f(x_k)\right]^{1/2}}.
%\end{equation*}
%As $B_k$ varies with $k$, Quasi-Newton methods have been often defined as \textbf{variable metric methods}.

%TODO Concerning the local convergence, the results are similar to those of the Newton and Trust-Region method. In particular, if $\{x_k\}$ converges towards $x^*$ and $\hess(x^*)\succ0$, Algorithm \ref{alg:arc} will achieve local quadratic convergence, as the resulting steps around $x^*$ will be the same of pure Newton. In fact, similarly to the line search-based globalization of Newton, the quadratic model will be accurate and the trust-region radius will be always doubled (multiplied by $\gamma_2$).

\bibliographystyle{plain}
\bibliography{../biblio}
\end{document}