\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage[left=2.50cm, right=2.50cm, top=2.0cm, bottom=2.0cm]{geometry}
\include{../macros}
\title{Continuous Optimization}
\author{Chapter 3: Constrained Optimization}
\date{}
\begin{document}
	\maketitle
\section{Definitions}
	In this chapter we will consider constrained optimization problems with the following shape
	\begin{equation}\label{eq:problem}
		\begin{split}
			\min \;\; &f(x)\\
			\st& x \in C
		\end{split}
	\end{equation}
\begin{definition}[Convex Set]
	A set $C$ is said to be convex if given $x_1,x_2\in C$ and $\lambda\in [0,1]$, then $\lambda x_1 +(1-\lambda) x_2 \in C.$
\end{definition}
\begin{definition}[Convex Function]
A function $f:C\to \R$ defined on a convex set $C$ is said to be convex if given $x_1,x_2\in C$ and $\lambda\in [0,1]$, then 
\begin{equation*}
	f(\lambda x_1 +(1-\lambda) x_2) \leq \lambda f(x_1) +(1-\lambda) f(x_2).
\end{equation*}
\end{definition}
\begin{definition}[Strictly Convex Function]
A function $f:C\to \R$ defined on a convex set $C$ is said to be strictly convex if given $x_1,x_2\in C$ and $\lambda\in [0,1]$, then 
\begin{equation*}
	f(\lambda x_1 +(1-\lambda) x_2) < \lambda f(x_1) +(1-\lambda) f(x_2).
\end{equation*}
\end{definition}
\noindent A function is called concave if $-f$ is convex and strictly concave if $-f$ is strictly convex.\\
Now, given $\Delta_k$ the unit-simplex, that is the
subset of $\R^k$ comprising all nonnegative vectors whose sum is 1, i.e., 
\begin{equation*}
	\{\lambda \in \R^k: \lambda\geq 0, e^t\lambda=1\},
\end{equation*}
we can provide the following very useful result by Jensen's.
\begin{theorem}[Jensen's Inequality]
	Let $f:C\to \R$ be a convex function over a convex set $C$. Then for any $x_1, x_2, \dots, x_k \in C$ and $\lambda\in \Delta_k$ we have
	\begin{equation}\label{eq:jensen}
		f\left(\sum_{i=1}^{k}\lambda_ix_i\right) \leq \sum_{i=1}^{k} \lambda_if(x_i).
	\end{equation}
\end{theorem}
\begin{proof}
	We will prove \eqref{eq:jensen} by induction on $k$. For $k=1$ the result is obvious ($f(x_1)\leq f(x_1)\;\;\forall x_1 \in C$). We now assume that \eqref{eq:jensen} holds for $k$ and we will prove that it also holds for $k+1$. Suppose we have $x_1, x_2, \dots, x_{k+1} \in C$ and $\lambda\in \Delta_{k+1}$, we will show that $f(z)\leq \sum_{i=1}^{k+1} \lambda_i f(x_i)$ with $z=\sum_{i=1}^{k+1} \lambda_i x_i$. If $\lambda_{k+1} =1$, then $z=x_{k+1}$ and \eqref{eq:jensen} is obvious. If $\lambda_{k+1}<1$, then
	\begin{equation*}
		\begin{split}
			f(z)& = f\left(\sum_{i=1}^k \lambda_i x_i + \lambda_{k+1}x_{k+1} \right)\\
			&= f\left((1-\lambda_{k+1})\sum_{i=1}^k \frac{\lambda_i}{1-\lambda_{k+1}} x_i + \lambda_{k+1}x_{k+1} \right)\\
			&\leq (1-\lambda_{k+1})f(v) + \lambda_{k+1} f(x_{k+1}),
		\end{split}
	\end{equation*}
with $v= \sum_{i=1}^k\frac{\lambda_i}{1-\lambda_{k+1}} x_i$. Since $\sum_{i=1}^k\frac{\lambda_i}{1-\lambda_{k+1}} = \frac{1-\lambda_{k+1}}{1-\lambda_{k+1}} = 1,$ it follows that $v$ is a convex combination of $k$ points from $C$, hence by the induction hypotesis we have that $f(v)\leq \sum_{i=1}^k\frac{\lambda_i}{1-\lambda_{k+1}} f(x_i)$, which combined with the equality above yields
\begin{equation*}
	f(z) \leq \sum_{i=1}^{k+1} \lambda_i f(x_i).
\end{equation*}
\end{proof}
\section{Characterizations of Convex Functions}
\begin{theorem}[Gradient characterization of convex functions]\label{thm:gradient_ineq}
	Let $f\in \C(C)$, where $C$ is convex. Then f is convex over $C$ if and only if
	\begin{equation}\label{eq:grad_ineq}
		f(x) +\grad(x)^T(y-x)\leq f(y) \quad \forall x, y\in C.
	\end{equation}
\end{theorem}
\begin{proof}
	Exercise.
\end{proof}
\begin{proposition}[Sufficiency of stationarity under convexity]\label{prop:stationarity}
	Let $f\in \C(C)$, where $C\subseteq\Rn$ is convex. Suppose that $\nabla f(x^*)=0$ for some $x^*\in C$. Then $x^*$ is a global minimizer of $f$ over $C$.
\end{proposition}
\begin{proof}
	Let $z\in C$. Plugging $x=x^*$ and $y=z$ in Theorem \ref{thm:gradient_ineq} we obtain that 
	\begin{equation*}
		f(z)\geq f(x^*) +\grad(x^*)^T(z-x^*),
	\end{equation*}
which implies that $f(z)\geq f(x^*) $ because $\grad(x^*)=0$.
\end{proof}
We note that Proposition \ref{prop:stationarity} establishes only the sufficiency of the stationarity condition $\grad(x^*) = 0$ for guaranteeing that $x^*$ is a global optimal solution. When $C$ is not the
entire space, this condition is not necessary, in fact it might be that the points for which $\grad(\cdot)=0$ are not in $C$. On the other hand, when $C=\Rn$ and $f$ is convex, $\grad(x^*) = 0$ is both sufficient and necessary condition for $x^*$ to be a global minimum.
We can now establish the conditions under which a twice continuously differentiable function $f$ is convex.
\begin{theorem}[Second order characterization of convexity]
	Let $f\in \Cii(C)$, where $C\subseteq\Rn$ is convex and open. Thus, we have that $f$ is convex iff $\hess(x)\succcurlyeq0\quad \forall x\in C.$
\end{theorem}
\begin{proof}
	Suppose that $\hess(x) \succcurlyeq 0$ for all $x \in C$. We will prove \eqref{eq:grad_ineq} which is enough to establish convexity. Let $x,y\in C$, then by the Mean Value Theorem$^2$ (Theorem 2.6 from Chapter 1) we get that there exists $z\in[x,y]$ (and hence $z\in C$) for which 
	\begin{equation}\label{eq:mvt2_app}
		f(y) = f(x)+ \grad (x)^T(y-x) +\frac{1}{2}(y-x)^T\hess(z)(y-x).
	\end{equation}
Since $\hess(z) \succcurlyeq 0$, it follows that $(y-x)^T\hess(z)(y-x)\geq0$, which implies \eqref{eq:grad_ineq}.
To prove the opposite direction, assume that $f$ is convex over $C$. Let $x\in C$ and $y\in \Rn$. Since $C$ is open, it follows that $x+\lambda y \in C$, for $0<\lambda<\epsilon$, where $\epsilon$ is a small enough positive constant. Using now the gradient characterization of convex functions \eqref{eq:grad_ineq} we get 
\begin{equation*}
	f(x+\lambda y) \geq f(x) + \lambda\grad(x)^Ty.
\end{equation*}
In addition, by the quadratic approximation theorem (Theorem 2.4 from Chapter 1), we have that 
\begin{equation*}
	f(x+\lambda y) = f(x) + \lambda \grad(x)^Ty + \frac{\lambda^2}{2}y^T\hess(x)y + o(\lambda^2||y||^2),
\end{equation*}
which combined with the above inequality gives 
\begin{equation*}
	\frac{\lambda^2}{2}y^T\hess(x)y + o(\lambda^2||y||^2) \geq 0 \quad \forall \lambda\in (0,\epsilon).
\end{equation*}
Dividing the latter inequality by $\lambda^2$ and taking the limit for $\lambda\to 0^+$, we have 
\begin{equation*}
	y^T\hess(x)y \geq 0 \quad \forall y \in \Rn,
\end{equation*}
which concludes the proof.
\end{proof}
The same theorem works with positive definiteness and strict convexity, meaning also that the minimum in this case is unique.

\section{Optimization over convex problems}
From now on, we consider \eqref{eq:problem} where $f$ and $C$ are convex. As a direct consequence of the convexity of $f$ we have the following two theorems. Notice that the following result is not a direct consequence of Proposition \ref{prop:stationarity} as the local (and global) minimum, might be on the boundary of the set and not be stationary (in the sense of unconstrained optimization).
\begin{theorem}[global=local in convex optimization] Let $f:C\to\R$ be a convex function over a convex set $C\subseteq \Rn$. Let $x^*\in C$ be a local minimum of $f$ over $C$. Then $x^*$ is a global minimum of $f$ over $C$.	
\end{theorem}
\begin{proof}
	Since $x^*$ is a local minimum of $f$ over $C$ there exists $r$ such that $f(x)\geq f(x^*)$ for any $x\in C \cap B[x^*,r]$. Now let $y\in C$ with $y\neq x^*$. We want to show that $f(y) \geq f(x^*)$. Let $\lambda\in(0,1]$ be such that $x^*+\lambda(y-x^*)\in B[x^*,r]$, for instance $\lambda=\frac{r}{||y-x^*||}$. Now, since $x^*+\lambda (y-x^*) \in B[x^*,r]\cap C$, it follows that $f(x^*)\leq f(x^*+\lambda (y-x^*))$, and hence, by convexity of $f$, also 
	\begin{equation*}
		f(x^*)\leq f(x^*+\lambda (y-x^*)) \leq (1-\lambda)f(x^*) +\lambda f(y)
	\end{equation*}
Thus, $\lambda f(x^*) \leq \lambda f(y)$, which concludes the proof.
\end{proof}
\begin{theorem}[Convexity of the optimal set in convex optimization]\label{thm:unique}
	Let $f:C\to \R$ be a convex function with $C\subseteq \Rn$ convex. Then, the set of optimal solutions of the problem \eqref{eq:problem}, which we denote by $X^*$ is convex. Moreover, if $f$ is strictly convex over $C$, then there exists at most one optimal solution.
\end{theorem}
\begin{proof}
	If $X^*=\emptyset$, the result follows trivially. Suppose that $X\neq\emptyset$ and denote the optimal value of $f$ by $f^*$. Let $x,y\in C$ with $\lambda\in[0,1]$. Then, by convexity $f(\lambda x+(1-\lambda)y)\leq \lambda f^* +(1-\lambda)f^*= f^*$, hence $\lambda x +(1-\lambda)y$ is also optimal, i.e., it belongs to $X^*$, establishing the convexity of $X^*$. Suppose now that $f$ is strictly convex and $X^*$ is nonempty, and suppose by contradiction that there are 2 points $x,y$ in $X^*$. Then $\lambda x +(1-\lambda)y \in C$, and by the strict convexity of $f$ we have 
	\begin{equation*}
		f(\lambda x +(1-\lambda) y) < \lambda f(x) + (1-\lambda) f(y) = f^*,
	\end{equation*}
which is a contradiction to the fact that $f^*$ is the optimal value.
\end{proof}
\subsection{Stationarity}
Note that the following definition and the following theorem are given also for the more general case in which $f$ is not convex.
\begin{definition}[Stationary points of convex constrained problems]
	Let $f\in \C(C)$, where $C$ is closed and convex. Then $x^*$ is a stationary point of \eqref{eq:problem} if
	\begin{equation}\label{eq:stationarity}
	 \grad(x^*)(x-x^*)\geq 0 \; \forall x\in C.
\end{equation}
\end{definition}
\noindent In words, this means that there are no feasible descent directions of $f$ at $x^*$. This suggests that stationarity is in fact a necessary condition for a local minimum of \eqref{eq:problem}.
\begin{theorem}[Stationarity as necessary optimality condition of a convex constrained problem]\label{thm:stationarity}
	Let $f\in \C(C)$, where $C$ is closed and convex and let $x^*$ be a local minimum of \eqref{eq:problem}. Then $x^*$ is a stationary point of \eqref{eq:problem}.
\end{theorem}
\begin{proof}
	Let $x^*$ be a local minimum of $f$ and assume by contradiction that is not a stationary point of \eqref{eq:problem}. Then there exists $x\in C$ such that $\grad(x^*)(x-x^*)< 0$. Therefore, $f'(x,d)<0$, where $d=x-x^*$. Hence, by Lemma 1.1 of Chapter 2, there exists $\epsilon\in(0,1)$ such that $f(x^*+td)<f(x^*)\;\forall t\in(0,\epsilon).$ Since $C$ is convex, we have that $x+td = (1-t)x^*+tx\in C$, leading to the conclusion that $x^*$ is not a local optimum of \eqref{eq:problem}, which is a contradiction.
\end{proof}

\begin{theorem}[Stationarity as necessary and sufficient optimality condition for a convex problem]\label{thm:convex_stationarity}
	Let $f\in \C(C)$, where $C$ is closed and convex and $f$ is also convex. Let $x^*$ be a local minimum of \eqref{eq:problem}. Then $x^*$ is a stationary point of \eqref{eq:problem} iff $x^*$ is a optimal solution of \eqref{eq:problem}.
\end{theorem}
\begin{proof}
	The necessity of the stationarity condition follows from Theorem \ref{thm:stationarity}. To prove the sufficiency, assume that $x^*$ is a stationary point of \eqref{eq:problem} and let $x\in C$. Then, the gradient characterization of convex functions \eqref{eq:grad_ineq} and stationarity of $x^*$, we get
	\begin{equation*}
		f(x) \geq f(x^*) +\grad(x^*)^T(x-x^*) \geq f(x^*),
	\end{equation*}
which concludes the proof.
\end{proof}
\noindent Unfortunately, \eqref{eq:stationarity} is not an easy condition to check, we need something else.
\subsection{Orthogonal Projection}
We can instead characterize stationary points by using the projection operator.
Given a nonempty closed convex set $C$, the orthogonal projection operator $P_C:\Rn \to C$ is defined by 
\begin{equation}\label{eq:projection}
	P_C(x) = \argmin\left\{||x-y||^2: y\in C\right\}
\end{equation}
The orthogonal projection operator with input $x$ returns the vector in $C$ that is closest
to $x$. Note that the orthogonal projection operator is defined as a solution of a convex
optimization problem, specifically, a minimization of a convex quadratic function subject to a convex feasibility set. The first orthogonal projection theorem states that the orthogonal projection operator is in fact well-defined, meaning that the optimization problem in \eqref{eq:projection} has a unique optimal solution.
\begin{theorem}[First Projection Theorem]
	Let $C$ be a nonempty closed convex set. Then problem \eqref{eq:projection} has a unique optimal solution.
\end{theorem}
\begin{proof}
	As $C$ is closed and $||x-y||^2$ is coercive, we have that the problem admits at least one solution. Moreover, $||x-y||^2$ is strictly convex as the objective function is quadratic with
	positive definite matrix. %TODO
	Thus, from Theorem \ref{thm:unique} we get that \eqref{eq:projection} has a unique solution.
\end{proof}
The second projection theorem, provides an useful characterization of the projection operator. Geometrically it states that for a given closed and convex set $C$, $x \in \Rn$, and for any $y\in C$, the angle between $x-P_C (x)$ and $y-P_C (x)$ is obtuse.
This phenomenon is illustrated in Figure \ref{fig:second_projection}.
\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{second_projection}.
	\caption{The orthogonal projection operator.} \label{fig:second_projection}
\end{figure}
\begin{theorem}[Second Projection Theorem]\label{thm:second_projection}
	Let $C$ be a nonempty closed convex set. Then $z=P_C(x)$ iff 
	\begin{equation}\label{eq:second_projection}
		(x-z)^T(y-z) \leq 0\quad \forall \,y\in C.
	\end{equation}
\end{theorem}
\begin{proof}
	$z=P_C(x)$ iff it is the optimal solution of \eqref{eq:projection} iff (by Theorem \ref{thm:convex_stationarity})
	\begin{equation*}
		\grad(z)^T(y-z)\geq 0 \quad \forall\,y\in C,
	\end{equation*}
which concludes the proof as $\grad(z) = x-z.$
\end{proof}
Another important property of the orthogonal projection operator is given in the
following theorem, which also establishes the so-called nonexpansiveness property of $P_C$.
\begin{theorem}[Nonexpansiveness of the projection operator]
	Let $C$ be a closed and convex set. Then, for any $v,w\in \Rn$
	\begin{itemize}
		\item[a)] 
		\begin{equation}\label{eq:nonexpansiveness_1}
			(P_C(v)-P_C(w))^T(v-w)\geq ||P_C(v)-P_C(w)||^2
		\end{equation}
		\item[b)]
		\begin{equation}\label{eq:nonexpansiveness_2}
			 ||P_C(v)-P_C(w)||\leq ||v-w||.
		\end{equation}
	\end{itemize}
\end{theorem}
\begin{proof}
	From Theorem \ref{thm:second_projection} we have that for any $x \in \Rn$ and $y\in C$
	\begin{equation*}
		(x-P_C(x))^T(y-P_C(x)) \leq 0.
	\end{equation*}
Replacing $x=v$ and $y=P_C(w)$ we have
\begin{equation*}
	(v-P_C(v))^T(P_C(w)-P_C(v)) \leq 0.
\end{equation*}
Replacing, instead, $x=w$ and $y=P_C(v)$
\begin{equation*}
	(w-P_C(w))^T(P_C(v)-P_C(w)) \leq 0.
\end{equation*}
Now, summing the two inequalities we get
\begin{equation*}
	(P_C(w)-P_C(v))^T(v-w+P_C(w)-P_C(v)) \leq 0,
\end{equation*}
and hence, 
\begin{equation*}
	(P_C(v)-P_C(w))^T(v-w) \geq ||P_C(w)-P_C(v)||^2.
\end{equation*}
To prove \eqref{eq:nonexpansiveness_2}, we note that if $P_C(v) = P_C(w)$ inequality is trivial. Thus, we assume $P_C(v) \neq P_C(w)$. Then by Cauchy-Schwartz, we have 
\begin{equation*}
	(P_C(v)-P_C(w))^T(v-w) \leq ||P_C(v)-P_C(w)|| \cdot||v-w||,
\end{equation*}
which combined with \eqref{eq:nonexpansiveness_1} gives 
\begin{equation*}
	||P_C(v)-P_C(w)||^2 \leq ||P_C(v)-P_C(w)|| \cdot||v-w||,
\end{equation*}
which concludes the proof as $P_C(v) \neq P_C(w)$.
\end{proof}
Coming back to stationarity, let us provide the alternative characterization of a stationary point through the projection operator. 
\begin{theorem}
	Let $f\in C^1(C)$ with $C$ closed and convex and let $s>0$. $x^*$ is a stationary point of the problem \eqref{eq:problem} iff 
	\begin{equation}\label{eq:stationary_projection}
		x^*=P_C(x^*-s\grad(x^*)).
	\end{equation}
\end{theorem}
\begin{proof}
	By the second projection theorem (Theorem \ref{thm:second_projection}), we get that $x^*=P_C(x^*-s\grad(x^*))$ iff
	\begin{equation*}
		(x^*-s\grad(x^*) -x^*)^T(x-x^*)\leq 0, 
	\end{equation*}
which concludes the proof, as $x^*$ is a stationary point when $\grad(x^*)^T(x-x^*)\geq0$
\end{proof}
\subsection{Projected Gradient Method}
The characterization of stationary points through equation \eqref{eq:stationary_projection} directly suggest a new algorithm for solving convex constrained optimization methods.\\
\begin{algorithm}[H]\label{alg}
	\caption{Projected Gradient Method}
	
	\KwIn{$x_0\in \Rn$, $\epsilon>0$}
	
	$k = 0$
	
	\While{$||x_{k-1}-x_k||> \epsilon$}{
		
		Pick a step size $t_k$ with a line search procedure
		
		$x_{k+1} = P_C(x_k-t_k\grad(x_k)$
		
		$k = k+1$
	}
\end{algorithm}

Sufficient Descent lemma with the proof as exercise\\

The monotonicity of the gradient map with the proof\\

The convergence theorem for PGM with the proof as exercise
\bibliographystyle{plain}
\bibliography{../biblio}
\end{document}