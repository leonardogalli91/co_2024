\documentclass{ExerciseSheet}

%Set Number of the Exercise sheet and the submission deadline.
\setExerciseSheetNumber{7}
\setSubmissionDate{xx.xx.2024}



%boolean variable to determine whether the solutions should be included
\newif\ifsolutions
\solutionstrue
%\solutionsfalse

%We have a figure in this sheet
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}

\begin{document}


%Start with exercises
%-----------------------------------------------------------------------%


%\subsection*{Notation}
%\vskip 0.5cm 
%-----------------------------------------------------------------------%
\begin{problem}

Let $A$ be an $m \times n$ matrix and let $b \in \mathbb{R}^m$. The problem
\[
\min_{x \in \mathbb{R}^n} f(x) \equiv \|Ax - b\|_a^q
\]
where $\|\cdot\|_a$ is any norm on $\mathbb{R}^m$ and $q > 0$, admits a global optimal solution in $\mathbb{R}^n$.
\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
We suppose that $\mathrm{rank}(A) < n$, for, otherwise, the result follows from the preceding examples (when $f$ is continuous and a coercive function on $\mathbb{R}^n$. Then there exists a global minimum
point of f on $\mathbb{R}^n$).

Then, after a reordering, if needed, we can partition the matrix in the form
\[
A = (B \; C)
\]
where $B$ is a sub-matrix $m \times n_1$ of rank $n_1 < n$, and the columns of $C$ of dimension $m \times n_2$ can be expressed as linear combinations of the columns of $B$. If $C_i$ is the $i$-th column of $C$, we can write
\[
C_i = B \Gamma_i,
\]
where $\Gamma_i$ is a vector in $\mathbb{R}^{n_1}$. Thus, there exists a matrix $\Gamma \in \mathbb{R}^{n_1 \times n_2}$ such that
\[
C = B \Gamma.
\]

Given a vector $x \in \mathbb{R}^n$, we can partition it into the components $x(1) \in \mathbb{R}^{n_1}$, $x(2) \in \mathbb{R}^{n_2}$ and hence we can write:
\[
\|Ax - b\|_a^q = \|B x(1) + C x(2) - b\|_a^q = \|B (x(1) + \Gamma x(2)) - b\|_a^q.
\]

Letting
\[
y = x(1) + \Gamma x(2),
\]
we have
\[
\|Ax - b\|_a^q = \|By - b\|_a^q.
\]

Consider now the problem:
\begin{equation}
    \min_{y \in \mathbb{R}^{n_1}} \|By - b\|_a^q.
\end{equation}
As B has rank $n_1$, the function $\|By - b\|_a^q$ is coercive on $\mathbb{R}^n$ and hence it admits a global minimum $y^* \in \mathbb{R}^{n_1}$ such that 
\begin{equation} \label{eqn 2}
    \|By^* - b\|_a^q \leq \|By - b\|_a^q \quad \forall \  y \ \in \ \mathbb{R}^{n_1}
\end{equation}
We can define the point $x^* \in \mathbb{R}^n$ such that $x^*(1) = y^*, \ x^*(2) = 0 $ so we obtain
\begin{equation}
    \|Ax^* - b\|_a^q = \|By^* - b\|_a^q
\end{equation}
we claim that $x^*$ is a global minimizer of $\|Ax - b\|_a^q$\\

Reasoning by contradiction, suppose there exists $\hat{x}\in \mathbb{R}^n$ with vector corresponding $\hat{x}(1), \hat{x}(2)$ such that 
\begin{equation}
    \|A\hat{x} - b\|_a^q < \|Ax^* - b\|_a^q 
\end{equation}
Then we can consider the point 
$$\hat{y} = \hat{x}(1) + \Gamma \hat{x}(2)$$
so that we can write 
$$ \|A\hat{x} - b\|_a^q = \|B\hat{y} - b\|_a^q $$
It follows that we must have 
$$\|B\hat{y} - b\|_a^q  < \|Ax^* - b\|_a^q = \|By^* - b\|_a^q$$
which contradicts (\ref{eqn 2})
\end{solution}

\fi

%-----------------------------------------------------------------------%
\vskip 0.5cm
\begin{problem}
Consider the problem
\begin{equation} \label{problem}
    \min_{x \in \mathbb{R}^n} f(x) = \frac{1}{2} x^T Q x - c^T x, 
\end{equation}

Let $Q$ be a symmetric positive semidefinite matrix and assume that problem (\ref{problem}) admits a solution. Then the conjugate gradient method converges in at most $n$ iterations to a solution of (\ref{problem}).
\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
In order to prove this, it is sufficient to show that \begin{equation}\label{task}
    d^T_kQd_{k} \implies g_k = 0
\end{equation}

Let's define the following:
\begin{itemize}
    \item \underline{Range Space:} The range space of a matrix $Q$ is the linear subspace 
    $$\mathcal{R}(Q) = \{x\in \mathbb{R}^n : x = Qy, \ y\in \mathbb{R}^n\}$$
    \item \underline{Null Space:} The nullspace is the linear subspace
    $$\mathcal{N}(Q) = \{x\in \mathbb{R}^n: Qx = 0\}$$
    From known linear algebra results, we have
    \begin{equation} \label{algebra}
        \mathcal{R}(Q) \cap \mathcal{N}(Q) = 0
    \end{equation}
\end{itemize}

Indeed, if $d_k^T Q d_k > 0$ for every $k \geq 0$, then we can repeat the reasonings used in the proof of Proposition $9.3$ in the lecture note by obtaining the same conclusions. \\

First, by induction, we show that for every $k \geq 0$ we have
\begin{equation} \label{condition}
g_k, d_k \in \mathcal{R}(Q), 
\end{equation}
where $\mathcal{R}(Q)$ is the range space of $Q$. \\
Condition (\ref{condition}) is true for $k = 0$. Indeed, problem (\ref{problem}) admits solution, therefore there exists a point $x^\star$ such that
\[
Q x^\star = c.
\]
Then $c \in \mathcal{R}(Q)$ and hence it follows that $g_0 = Q x_0 - c \in \mathcal{R}(Q)$. Moreover, as $d_0 = -g_0$, we have $d_0 \in \mathcal{R}(Q)$. \\

Assume that (\ref{condition}) holds for $k - 1$ (with $k \geq 1$): we prove by induction that it holds for $k$. We have
\[
g_k = g_{k-1} + \alpha_{k-1} Q d_{k-1},
\]
from which it follows that $g_k \in \mathcal{R}(Q)$. Then, as
\[
d_k = -g_k + \beta_k d_{k-1},
\]
we have that $d_k \in \mathcal{R}(Q)$.

To prove (\ref{task}) suppose $d_k^T Q d_k = 0$. We can prove that $d_k$ belongs to the null space of $Q$, i.e., $d_k \in \mathcal{N}(Q)$. Indeed, by using the spectral decomposition of $Q$, we can write
\[
d_k^T Q d_k = \sum_{i=1}^n \lambda_i d_k^T u_i u_i^T d_k = \sum_{i=1}^n \lambda_i (u_i^T d_k)^2 = 0,
\]
being $\lambda_i, u_i$, for $i = 1, \ldots, n$, the eigenvalues and the eigenvectors of $Q$, respectively. Since $\lambda_i \geq 0$, we must have $\lambda_i (u_i^T d_k) = 0$ for $i = 1, \ldots, n$. Then we have
\[
Q d_k = \sum_{i=1}^n u_i \lambda_i (u_i^T d_k) = 0,
\]
i.e., $d_k \in \mathcal{N}(Q)$. \\
Using (\ref{condition}) it follows that
\[
d_k \in \mathcal{R}(Q) \cap \mathcal{N}(Q),
\]
and hence, from (\ref{algebra}), we obtain $d_k = 0$. \\
Recalling that $g_k^T d_k = -\|g_k\|^2$ (remember that $g^T_kd_k = -g^T_kg_k$) we have that $g_k = 0$ and hence we can conclude that $x_k$ is a solution of (\ref{problem}). \qed


\end{solution}

\fi

%-----------------------------------------------------------------------%
\vskip 0.5cm

\begin{problem} 
We observe that the expression of $\beta_{k+1}$ in $ \beta_{k+1}  = \dfrac{g^T_{k+1}Qd_k}{d^T_kQd_k}$ can be simplified in a way that the Hessian matrix does not appear explicitly and this will be useful in the extension of the CGM to non-quadratic problems.
Use
\begin{equation}
    Q d_k = \frac{g_{k+1} - g_k}{\alpha_k}.
\end{equation} and the fact that $g^T_{k+1}d_k = 0, \ g^T_k d_k = -g^T_kg_k$ to get \begin{equation}
    \beta_{k+1} = \dfrac{\|g_{k+1}\|^2}{\|g_k\|^2}
\end{equation}
\end{problem}

\ifsolutions
\vskip 0.3cm
\begin{solution}

 We can rewrite $ \beta_{k+1}  = \dfrac{g^T_{k+1}Qd_k}{d^T_kQd_k}$ as
\begin{equation} \label{beta-k}
    \beta_{k+1} = \frac{g_{k+1}^T (g_{k+1} - g_k)/\alpha_k}{d_k^T (g_{k+1} - g_k)/\alpha_k} 
    = \frac{g_{k+1}^T (g_{k+1} - g_k)}{d_k^T (g_{k+1} - g_k)}.
\end{equation}

From (\ref{beta-k}), taking into account ($g^T_{k+1}d_k = 0$), we have:
\begin{equation} \label{newbeta}
    \beta_{k+1} = \frac{g_{k+1}^T (g_{k+1} - g_k)}{d_k^T g_k}.
\end{equation}

Using ($g^T_kd_k = -g^T_kg_k$), from ($\ref{newbeta}$) we get
\begin{equation}\label{beta_next}
    \beta_{k+1} = \frac{g_{k+1}^T (g_{k+1} - g_k)}{\|g_k\|^2}.
\end{equation}

From (\ref{beta_next}), as $ g_{k+1}^T g_k = 0 $, by ($g^T_ig_j = 0$), we have also:
\begin{equation}
    \beta_{k+1} = \frac{\|g_{k+1}\|^2}{\|g_k\|^2}.
\end{equation}

\end{solution}

\fi

%-----------------------------------------------------------------------%
\vskip 0.3cm


\begin{algorithm}[H]
\caption{Preconditioned Conjugate Gradient Method (PCGM)}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Starting point $x_0 \in \mathbb{R}^n$, symmetric positive definite matrix $M$
\STATE Compute $g_0 = Qx_0 - c$, $d_0 = -Mg_0$, set $k = 0$
\WHILE{ $g_k \neq 0$ }
    \STATE $z_k = M g_k$, \quad $\tau_k = z_k^T g_k$ \hfill
    \STATE $\alpha_k = \frac{\tau_k}{d_k^T Q d_k}$ \hfill 
    \STATE $x_{k+1} = x_k + \alpha_k d_k$ \hfill 
    \STATE $g_{k+1} = g_k + \alpha_k Q d_k$ \hfill 
    \STATE $z_{k+1} = M g_{k+1}$, \quad $\tau_{k+1} = z_{k+1}^T g_{k+1}$ \hfill 
    \STATE $\beta_k = \frac{\tau_{k+1}}{\tau_k}$ \hfill
    \STATE $d_{k+1} = -g_{k+1} + \beta_k d_k$ \hfill 
    \STATE $k = k + 1$
\ENDWHILE
\end{algorithmic}
\end{algorithm}
\begin{problem}
Consider the linear system
\[
A^TAx = A^Tb,
\]
and its equivalent form
\[
MQx = Mc,
\]
where $Q = A^TA$, $c = A^Tb$, and $M$ is a symmetric positive definite matrix that approximates $Q^{-1}$, but the product $MQ$ is not necessarily symmetric. To restore symmetry and improve conditioning, suppose we define a preconditioner $S \approx B^{-1}$, where $Q = B^2$, and perform a change of variables $y = S^{-1}x$. This yields the transformed symmetric system:
\begin{equation} \label{symmetrics}
SQSy = Sc 
\end{equation}
where $SQS$ is symmetric positive definite and approximates the identity.
Write out the explicit iterative formulae for computing $d_{k+1}, \alpha_k, \beta_{k+1}$ in algorithm (1) above.

Hint: From equation (\ref{symmetrics}), we can write
\[
y_{k+1} = y_k + \alpha_k \tilde{d}_k,
\]
where
\[
\alpha_k = \frac{\tilde{g}_k^T \tilde{g}_k}{\tilde{d}_k^T SQS \tilde{d}_k},
\quad \text{and} \quad
\tilde{g}_k = SQS y_k - Sc = S(QSy_k - c)
\] where $\tilde{d}_k $ and $\tilde{g}_k$ are the search direction and the residual in the CGM for system (\ref{symmetrics})
%\begin{itemize}
 %   \item[(a)] Derive the Conjugate Gradient Method (CGM) for solving the transformed system (3), and write out the explicit iterative formulae for computing $y_k$, $r_k$, and $p_k$.
  %  \item[(b)] Express how to recover the original variable $x$ from the solution $y$.
%\end{itemize}
\end{problem}
\ifsolutions
\vskip 0.3cm
\begin{solution}
    Let us write the formulae of the CGM method applied to (\ref{symmetrics}). For $ k \geq 0 $, denoting by $\tilde{d}_k $ and $ \tilde{g}_k $ the search direction and the residual in the CGM for system (\ref{symmetrics}), we can write

\begin{equation} \label{update}
    y_{k+1} = y_k + \alpha_k \tilde{d}_k,
\end{equation}

where
\begin{equation}\label{stepsize}
    \alpha_k = \frac{\tilde{g}_k^T \tilde{g}_k}{\tilde{d}_k^T S Q S \tilde{d}_k},
\end{equation}

\begin{equation} \label{gradient}
    \tilde{g}_k = S Q S y_k - S c = S(Q S y_k - c) 
\end{equation}

Pre-multiplying both members of (\ref{update}) by $S$ and using the transformation $ x = S y $, we have
\begin{equation}
    x_{k+1} = x_k + \alpha_k S \tilde{d}_k,
\end{equation}

whence, letting
\begin{equation} \label{direction}
    d_k = S \tilde{d}_k, 
\end{equation}

we obtain
\begin{equation}
    x_{k+1} = x_k + \alpha_k d_k.
\end{equation}

Equation (\ref{gradient}) can be rewritten as
\begin{equation}
    \tilde{g}_k = S(Q x_k - c) = S g_k. 
\end{equation}
and then, from (\ref{stepsize}), using (\ref{direction}) and letting $ M = S^2 $, we can write:

\begin{equation} \label{newstepsize}
    \alpha_k = \frac{g_k^T SS g_k}{d_k^T Q d_k} = \frac{g_k^T M g_k}{d_k^T Q d_k}. 
\end{equation}

We have also
\begin{equation} \label{d_0}
    \tilde{d}_0 = -\tilde{g}_0, 
\end{equation}
\begin{equation} \label{tilde_d_k}
    \tilde{d}_k = -\tilde{g}_k + \beta_k \tilde{d}_{k-1} \quad \text{for } k \geq 1,
\end{equation}

where
\begin{equation} \label{direction1}
    \beta_k = \frac{\|\tilde{g}_k\|^2}{\|\tilde{g}_{k-1}\|^2} = \frac{g_k^T SS g_k}{g_{k-1}^T SS g_{k-1}} = \frac{g_k^T M g_k}{g_{k-1}^T M g_{k-1}}.
\end{equation}

From (\ref{d_0}) and (\ref{tilde_d_k}), multiplying by $ S$, we get
\begin{equation}
    d_0 = -S \tilde{g}_0 = -S S g_0 = -M g_0 
\end{equation}
\begin{equation}
    d_k = -SSg_k + \beta_k d_{k-1} \quad \text{for } k \geq 1. 
\end{equation}

Now, by defining the vector $ z_k = M g_k $ and the scalar $ \tau_k = z_k^T g_k $, from (\ref{stepsize}) and (\ref{direction1}) we obtain the expressions

\begin{equation}
    \alpha_k = \frac{\tau_k}{d_k^T Q d_k}, \quad 
    \beta_{k+1} = \frac{\tau_{k+1}}{\tau_k}.
\end{equation}
\end{solution}
\end{document}