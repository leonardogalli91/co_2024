\documentclass{ExerciseSheet}

%Set Number of the Exercise sheet and the submission deadline.
\setExerciseSheetNumber{5}
\setSubmissionDate{xx.xx.2024}

%boolean variable to determine whether the solutions should be included
\newif\ifsolutions
\solutionstrue
%\solutionsfalse

%We have a figure in this sheet
\usepackage{graphicx}

\begin{document}


%Start with exercises
%-----------------------------------------------------------------------%


%\subsection*{Notation}
%\vskip 0.5cm 
%-----------------------------------------------------------------------%

\begin{problem}
Consider the Problem (Differentiable Function: We want to minimize a differentiable function $f: \mathbb{R}^d \rightarrow \mathbb{R}$. We require that the problem is well-posed, in the sense that argmin $f \neq \emptyset $)and assume that $f $ is convex and $L $-smooth, for some $ L > 0 $. Let $ (x^t)_{t \in \mathbb{N}} $ be the sequence of iterates generated by the GD algorithm, with a stepsize satisfying $ 0 < \gamma \leq \frac{1}{L} $. Then, for all $x^* \in \arg\min f $, for all $ t \in \mathbb{N} $, we have that
\begin{equation*}
    f(x^t) - \inf f \leq \frac{\|x^0 - x^*\|^2}{2 \gamma t}.
\end{equation*}
Hint: Prove with Lyapunov arguments.
\end{problem}

\ifsolutions
\vskip 0.3cm
\begin{solution}
    \textbf{Proof  with Lyapunov arguments.}
Let $x^* \in \arg\min f$ be any minimizer of $f$.
First, we will show that $f(x^t)$ is decreasing.
Indeed we know from the fact that if $f$ is $L$-smooth and $\lambda > 0$ then $f(x-\lambda \nabla f(x)) - f(x) \leq -\lambda\bigg(1-\dfrac{\lambda L}{2}\bigg) \|\nabla f(x)\|^2 \ \forall \ x,\ y \in \mathbb{R}^d$ (Lemma 2.28 of the handbook of convergence theorems for (Stochastic) Gradient Methods), and from our assumption $\gamma L \leq 1$, that
\begin{equation}
f(x^{t+1}) - f(x^t) \leq -\gamma \left(1 - \frac{\gamma L}{2} \right)\|\nabla f(x^t)\|^2 \leq 0.
\tag{1}
\end{equation}

Second, we will show that $\|x^t - x^*\|^2$ is also decreasing.
For this we expand the squares to write
\begin{align}
\frac{1}{2\gamma} \|x^{t+1} - x^*\|^2 - \frac{1}{2\gamma} \|x^t - x^*\|^2 
&= \frac{-1}{2\gamma} \|x^{t+1} - x^t\|^2 - \langle \nabla f(x^t), x^{t+1} - x^* \rangle \nonumber \\
&= \frac{-1}{2\gamma} \|x^{t+1} - x^t\|^2 - \langle \nabla f(x^t), x^{t+1} - x^t \rangle + \langle \nabla f(x^t), x^t - x^* \rangle.
\tag{2}
\end{align}

Now to bound the right-hand side we use the convexity of $f$ and this lemma: If $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is convex and differentiable then $$\text{fo all}\ x, y \in \mathbb{R}^d, \quad f(x) \geq f(y)+\langle \nabla f(y), x-y  \rangle$$ to write
\[
\langle \nabla f(x^t), x^t - x^* \rangle \leq f(x^*) - f(x^t) = \inf f - f(x^t).
\]

To bound the other inner product we use the smoothness of $L$ and $f(y)\leq f(x) + \langle \nabla f(x), y-x \rangle + \dfrac{L}{2}\|y-x\|^2\ \forall\ x, y \in \mathbb{R}^d, \ \text{if}\ f\ \text{is}\ L-\text{smooth}$ which gives
\[
-\langle \nabla f(x^t), x^{t+1} - x^t \rangle \leq \frac{L}{2} \|x^{t+1} - x^t\|^2 + f(x^t) - f(x^{t+1}).
\]

By using the two above inequalities in (2) we obtain
\begin{align}
\frac{1}{2\gamma} \|x^{t+1} - x^*\|^2 - \frac{1}{2\gamma} \|x^t - x^*\|^2 
&\leq -\frac{1}{2\gamma} \|x^{t+1} - x^t\|^2 - (f(x^{t+1}) - \inf f), \nonumber \\
&\leq - (f(x^{t+1}) - \inf f).
\tag{3}
\end{align}

Let us now combine the two positive decreasing quantities $f(x^t) - \inf f$ and $\frac{1}{2\gamma} \|x^t - x^*\|^2$, and introduce the following Lyapunov energy, for all $t \in \mathbb{N}$:
\[
E_t := \frac{1}{2\gamma} \|x^t - x^*\|^2 + t(f(x^t) - \inf f).
\]

We want to show that it is decreasing with time.
For this we start by writing
\begin{align}
E_{t+1} - E_t &= (t+1)(f(x^{t+1}) - f(x^*)) - t(f(x^t) - \inf f) + \frac{1}{2\gamma} \|x^{t+1} - x^*\|^2 - \frac{1}{2\gamma} \|x^t - x^*\|^2 \nonumber \\
&= f(x^{t+1}) - \inf f + t(f(x^{t+1}) - f(x^t)) + \frac{1}{2\gamma} \|x^{t+1} - x^*\|^2 - \frac{1}{2\gamma} \|x^t - x^*\|^2.
\tag{4}
\end{align}

Combining now (4), (1), and (3), we finally obtain (after cancelling terms) that
\begin{align*}
E_{t+1} - E_t &\leq f(x^{t+1}) - \inf f + \frac{1}{2\gamma} \|x^{t+1} - x^*\|^2 - \frac{1}{2\gamma} \|x^t - x^*\|^2 \\
&\leq f(x^{t+1}) - \inf f - (f(x^{t+1}) - \inf f) \\
&\leq 0.
\end{align*}
Thus $E_t$ is decreasing. Therefore we can write that
\[
t(f(x^t) - \inf f) \leq E_t \leq E_0 = \frac{1}{2\gamma} \|x^0 - x^*\|^2,
\]
and the conclusion follows after dividing by $t$. \qed
\end{solution}
\fi

%-----------------------------------------------------------------------%
\vskip 0.5cm

\begin{problem}

For a set $K\subset \R^n$ and $p\in \N$, $x_i \in K$, $1\leq i\leq p$, and scalars $\lambda_i \geq 0$, $1\leq i\leq p$, satisfying $ \displaystyle\sum_{i=1}^{p}{\lambda_i}=1$ one denotes by $ \displaystyle\sum_{i=1}^{p}{\lambda_i x_i}$ the \emph{convex combinations of elements from $K$}. 

\begin{enumerate}
 \item Prove that a set $K \subset \R^n$ is convex if and only if every convex combination of elements from $K$ is contained in $K$.

 \item Let $f$ be convex and $D = \left\{\displaystyle\sum_{i=0}^{n}\lambda_ix_i ~|~ \lambda_i\geq0, \sum_{i=0}^{n}\lambda_i=1\right\}$
 % \begin{enumerate}
 
       Prove that 
         \begin{align*}
             \forall x\in D ~~ f(x) \leq \displaystyle\max_{i\in\{0,\dots,n\}}f(x_i)
         \end{align*}
         %\item [(ii)] Set $\displaystyle\max_{i\in\{0,\dots,n\}}f(x_i):= C$ and show that $f(x)\le C$ for all $x\in\Delta$ 
  %\end{enumerate}
 \end{enumerate}

\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
\begin{enumerate}
    \item ($\Leftarrow$) Let assume every convex combination of elements from $K$ is contained in $K$ and prove $K$ is convex.\\
    Let $x,y\in K$ and $\lambda\in [0, 1],$ given that every convex combination of element of $K$ belong to $K$ then particularly $(1-\lambda)x+\lambda y\in K.$\\
    ($\Rightarrow$) Now let us assume $K\subset \R^n$ is convex and show that every convex combination of elements from $K$ is contained in $K$ i.e. $\displaystyle \sum_{i=1}^{p}\lambda_ix_i\in K.$ We will show this by induction.\\
    Induction start:  $p=1,$ is trivial and for $p=2,$~ $\lambda_1x_1+\lambda_2x_2\in K$ by definition as $K$ is convex.\\
    Let assume $\displaystyle \sum_{i=1}^{p}\lambda_ix_i\in K$ with $x_i\in K$ and $\displaystyle \sum_{i=1}^{p}\lambda_i=1$ then prove $\displaystyle \sum_{i=1}^{p+1}\lambda_ix_i\in K$ with $x_i\in K$ and $\displaystyle \sum_{i=1}^{p+1}\lambda_i=1.$
    We have
      \begin{align}
          \sum_{i=1}^{p+1}\lambda_ix_i&= \sum_{i=1}^{p}\lambda_ix_i+\lambda_{p+1}x_{p+1}\nonumber\\
                           &=(1-\lambda_{p+1})\sum_{i=1}^{p}\frac{\lambda_i}{1-\lambda_{p+1}}x_i+\lambda_{p+1}x_{p+1} \label{ induction step}
      \end{align}
    On the other hand, $0\leq \lambda_{p+1}\leq 1,~\frac{\lambda_i}{1-\lambda_{p+1}}\geq0$ and in addition, $\displaystyle \sum_{i=1}^{p+1}\lambda_i=1$ that is $\displaystyle \sum_{i=1}^{p}\lambda_i+ \lambda_{p+1}=1$ we have
    \begin{align*}
        \sum_{i=1}^{p}\lambda_i=1-\lambda_{p+1}
    \end{align*}
   therefore,
   \begin{align*}
        \sum_{i=1}^{p}\frac{\lambda_i}{1-\lambda_{p+1}}=\frac{1-\lambda_{p+1}}{1-\lambda_{p+1}}=1
    \end{align*}
 which means $\displaystyle \sum_{i=1}^{p}\frac{\lambda_i}{1-\lambda_{p+1}}x_i\in K$ by induction hypothesis.Therefore, by using the convexity of $K$ we get 
  \begin{align*}
      (1-\lambda_{p+1})\sum_{i=1}^{p}\frac{\lambda_i}{1-\lambda_{p+1}}x_i+\lambda_{p+1}x_{p+1}\in K.
  \end{align*}
 It follows from $\eqref{ induction step}$ that $\displaystyle \sum_{i=1}^{p+1}\lambda_ix_i\in K$
 




    \item Let $x\in D$ and prove 
       \begin{align*}
           f(x)\leq \max_{i\in \{0,\dots,n\}}f(x_i)
       \end{align*}
 $x\in D,$ means $\displaystyle x=\sum_{i=0}^{n}\lambda_ix_i$ with $\displaystyle \lambda_i\geq 0,~\sum_{i=0}^{n}\lambda_i=1.$ Therefore,
   \begin{align*}
       f(x)&=f\left(\sum_{i=0}^{n}\lambda_ix_i\right)  \\
           &\leq \sum_{i=0}^{n}\lambda_if(x_i)  \text{ since $f$ in convex}\\
           &\leq  \left(\max_{i\in \{0,\dots,n\}}f(x_i)\right)\sum_{i=0}^{n}\lambda_i \\
           &=\max_{i\in \{0,\dots,n\}}f(x_i)  \text{ since $\sum_{i=0}^{n}\lambda_i=1$}
   \end{align*}





       
\end{enumerate}
\end{solution}

\fi

%-----------------------------------------------------------------------%
\vskip 0.5cm

\begin{problem}[Gradient characterization of convex functions]\label{thm:gradient_ineq}
	Let $f\in \C(C)$, where $C$ is convex. 
 
 Prove that $f$ is convex over $C$ if and only if
	\begin{equation}\label{eq:grad_ineq}
		f(x) +\nabla f(x)^T(y-x)\leq f(y) \quad \forall x, y\in C.
	\end{equation}
\end{problem}

\ifsolutions
\vskip 0.3cm
\begin{solution}

\begin{itemize}
    \item [($\Rightarrow$)] Let us assume $f$ is convex and show that
    \begin{equation*}%\label{eq:grad_ineq}
		f(x) +\nabla f(x)^T(y-x)\leq f(y) \quad \forall x, y\in C.
	\end{equation*}
 Let $x, y\in C$ and $\lambda \in (0, 1].$ We will focus on the case $x\neq y$ as $\eqref{eq:grad_ineq}$ is trivial for $x=y.$
 The convexity of $f$ gives
 \begin{align*}
     f\left(\lambda y + (1-\lambda)x\right) \leq \lambda f(y) + (1-\lambda)f(x)
 \end{align*}
that is 
  \begin{align*}
     f\left(x+\lambda (y-x)\right) \leq \lambda f(y) + f(x)-\lambda f(x)
 \end{align*}
sending $f(x)$ on the left side and diving by $\lambda$ yields
   \begin{align*}
     \frac{f\left(x+\lambda (y-x) \right) - f(x)}{\lambda}\leq f(y)- f(x)
 \end{align*}
letting $\lambda$ tend to $0^+$ yields
        \begin{align*}
     f'(x;y-x)\leq f(y)- f(x) \text{ where $f'(x;y-x)$ is the directional }
 \end{align*}
 Given that, $f$ is continuously differentiable we have $f'(x;y-x)=\nabla f(x)^T(y-x)$ and substituting this into the above inequality gives  
    \begin{align*}
     \nabla f(x)^T(y-x)\leq f(y)- f(x) 
 \end{align*}
which prove $\ref{eq:grad_ineq}.$
\item [($\Leftarrow$)] Now let us assume $\eqref{eq:grad_ineq}$ holds and prove that $f$ is convex over $C.$\\
Let $x,~y\in C,$ and $\lambda\in (0,~1).$ Set $x'=(1-\lambda)x+\lambda y$ then $x'\in C$ since $C$ is convex, and $y=\frac{x'-(1-\lambda)x}{\lambda}$ it follows
   \begin{align}
       y-x' &= \frac{x'-(1-\lambda)x-\lambda x'}{\lambda}\nonumber\\
            &=-\frac{(1-\lambda)}{\lambda}(x-x')   \label{y-x'}
   \end{align}
 Applying $\eqref{eq:grad_ineq}$ on the pairs $x',~y$ and $x',~x$ gives           
    \begin{align}
        f(x') +\nabla f(x')^T(y-x')\leq f(y) \label{intermediate step 1}\\
        f(x') +\nabla f(x')^T(x-x')\leq f(x)  \label{intermediate step 2}
    \end{align}

    substituting $\eqref{y-x'}$ into $\eqref{intermediate step 2}$ yields
     \begin{align*}
         f(x') -\frac{\lambda}{(1-\lambda)}\nabla f(x')^T(y-x')\leq f(x)
     \end{align*}
    multiplying $\eqref{intermediate step 1}$ by $\frac{\lambda}{(1-\lambda)}$ and adding it the above gives
        \begin{align*}
           f(x')+ \frac{\lambda}{(1-\lambda)}f(x') \leq f(x) + \frac{\lambda}{(1-\lambda)}f(y)
        \end{align*}
    that is
     \begin{align*}
            \frac{1}{(1-\lambda)}f(x') \leq f(x) + \frac{\lambda}{(1-\lambda)}f(y)
        \end{align*}
    which implies
        \begin{align*}
            f(x') \leq (1-\lambda)f(x) + \lambda f(y)
        \end{align*}
    recalling $x'=(1-\lambda)x+\lambda y$ we have
     \begin{align*}
            f((1-\lambda)x+\lambda y) \leq (1-\lambda)f(x) + \lambda f(y)
        \end{align*}
\end{itemize}

\end{solution}

\fi

%-----------------------------------------------------------------------%
\vskip 0.5cm
\begin{problem}
 Let $A\subset \R^d,$ $$\text{conv}(A)=\left\{\displaystyle\sum_{i=1}^p\lambda_i x_i\,:\,\displaystyle\sum_{i=1}^p\lambda_i=1,~\lambda_i\geq 0,~ x_i \in A\text{ for all } i=1,\ldots,p\right\}$$

\item Use Jensen's inequality (Theorem 4.1 of the lecture note Chapter 2) to show the following 2 propositions
\begin{itemize}
    \item Let $f:\text{conv}(A)\rightarrow \R$ be convex. Show 
    \begin{align*}
     \sup_{x \in \text{conv}(A)} f(x)=\sup_{x\in A} f(x). 
    \end{align*}

    \item Show that for all $x\in \R^n$ such that $x_i >0$, $1\leq i\leq n$ the following inequality between the arithmetic and geometric mean holds:
	\begin{align*}
	\left( \prod_{i=1}^n x_i \right)^{\frac{1}{n}} \leq \frac{1}{n} \sum_{i=1}^n x_i.
	\end{align*}
 \end{itemize}


\end{problem}

\ifsolutions
\vskip 0.3cm
\begin{solution}
\begin{itemize}
    \item Let us prove 
            \begin{align*}
                 \sup_{x \in \text{conv}(A)} f(x)=\sup_{x\in A} f(x). 
            \end{align*}
        $\text{conv}(A)$ is the smallest convex set containing A, hence, $A\subset \text{conv}(A).$ Therefore,
            $$\sup_{x \in \text{conv}(A)} f(x)\geq\sup_{x\in A} f(x).$$
        Let us now show 
            $$\sup_{x \in \text{conv}(A)} f(x)\leq\sup_{x\in A} f(x).$$
        We have
        \begin{align*}
            \sup_{x \in \text{conv}(A)}f(x) &= \sup_{x_i \in A,~\sum_{i=1}^{p}\lambda_i=1,~0\leq \lambda_i\leq 1}f\left(\sum_{i=1}^{p}\lambda_ix_i\right) 
        \end{align*}
        Since $f$ is convex applying Jensen's inequality (Theorem 4.1 of the lecture note Chapter 2) on the right hand side gives
        \begin{align*}
            \sup_{x \in \text{conv}(A)}f(x) &\leq \sup_{x_i \in A,~\sum_{i=1}^{p}\lambda_i=1,~0\leq \lambda_i\leq 1}\left(\sum_{i=1}^{p} \lambda_if(x_i)\right) \\
                                            &\leq \sup_{x\in A}f(x)\left( \sup_{\sum_{i=1}^{p}\lambda_i=1,~0\leq \lambda_i\leq 1}\sum_{i=1}^{p} \lambda_i\right) \\
                                            &= \sup_{x\in A}f(x) \text{ since $\sum_{i=1}^{p} \lambda_i=1$}
        \end{align*}
        
    \item For $i,~1\leq i\leq n$ set $y_i=\ln{x_i}.$ Then
        \begin{align*}
            \left(\prod_{i=1}^{n}x_i\right)^{\frac{1}{n}}&= \left(\prod_{i=1}^{n}e^{y_i}\right)^{\frac{1}{n}} \\
                                                          &=  \left(e^{\sum_{i=1}^{n}y_i}\right)^{\frac{1}{n}} \\
                                                           &= e^{\frac{1}{n}\sum_{i=1}^{n}y_i}
        \end{align*}
    Given that, $e$ (exponential) is convex, applying Jensen's inequality yields
    
        \begin{align*}
            \left(\prod_{i=1}^{n}x_i\right)^{\frac{1}{n}}&\leq\dfrac{1}{n} \sum_{i=1}^{n}e^{y_i}\\
                                                         &= \dfrac{1}{n}\sum_{i=1}^{n}x_i
        \end{align*}
    

\end{itemize}     
\end{solution}
\fi

%-----------------------------------------------------------------------%
\vskip 0.5cm

\end{document}