\documentclass{ExerciseSheet}

%Set Number of the Exercise sheet and the submission deadline.
\setExerciseSheetNumber{9}
\setSubmissionDate{xx.xx.2024}

%boolean variable to determine whether the solutions should be included
\newif\ifsolutions
\solutionstrue
%\solutionsfalse

%We have a figure in this sheet
\usepackage{graphicx}
\newcommand{\LC}{\mathord{\mathrm{C^{1,1}_{L}}}}
\DeclareMathOperator*{\grad}{\mathit{\nabla \!f}}
\newcommand{\with}{\mathord{\mathrm{with }}\,}
\newcommand{\BigO}{\mathcal{O}}

%\usepackage{graphicx}
\usepackage{amsmath}

\begin{document}


%Start with exercises
%-----------------------------------------------------------------------%


%\subsection*{Notation}
%\vskip 0.5cm 
%-----------------------------------------------------------------------%
\begin{problem}(Successful and unsuccessful trust-region iterations). Let $\{x_k\}$ and $\{\Delta_k\}$ be the sequences generated by Algorithm 4 (Lecture note, Trust-Region with Cauchy point). Assume that $\exists \, \Delta_{\min} > 0 : \Delta_k \geq \Delta_{\min}$. Then
\[
k \leq |S_k| \left(1 + \left|\frac{\log \gamma_2}{\log \gamma_1} \right| \right) + \frac{1}{|\log \gamma_1|} \left| \log \left( \frac{\Delta_{\min}}{\Delta_0} \right) \right|.
\]
Hint: It can be derived by noticing that 
\[
\Delta_{\min} \leq \Delta_k \leq \Delta_0 \gamma_2^{|S_k|} \gamma_1^{|U_k|},
\]
and that 
\[
k = |S_k| + |U_k|.
\]
\end{problem}
\ifsolutions
\vskip 0.3cm

\begin{solution}
    We are given that the trust-region algorithm (TR1) ensures the trust-region radius satisfies:
\[
\Delta_k \geq \Delta_{\min} > 0.
\]

Also, for each iteration $i$:

\begin{itemize}
    \item If $i \in S_k$ (successful step), then $\Delta_{i+1} \leq \gamma_2 \Delta_i$
    \item If $i \in \mathcal{U}_k$ (unsuccessful step), then $\Delta_{i+1} \leq \gamma_1 \Delta_i$
\end{itemize}

We want to prove the bound:
\[
k \leq |S_k| \left(1 + \frac{\log \gamma_2}{|\log \gamma_1|} \right) + \frac{1}{|\log \gamma_1|} \left| \log \left( \frac{\Delta_{\min}}{\Delta_0} \right) \right|. 
\]



Starting from the initial radius $\Delta_0$, we apply the update rules over $k$ steps.

\[
\Delta_k \leq \Delta_0 \cdot \gamma_2^{|S_k|} \cdot \gamma_1^{|\mathcal{U}_k|}.
\]
This follows by applying the update rule for each iteration:
\begin{itemize}
    \item Every time there's a successful step, you multiply by at most $\gamma_2$
\item Every time there's a unsuccessful step, you multiply by at most $\gamma_1$

\end{itemize}

The assumption given is 

\[
\Delta_k \geq \Delta_{\min},
\]

and we have that
\[
\Delta_{\min} \leq \Delta_0 \cdot \gamma_2^{|S_k|} \cdot \gamma_1^{|\mathcal{U}_k|}.
\]
\[
\frac{\Delta_{\min}}{\Delta_0} \leq \gamma_2^{|S_k|} \cdot \gamma_1^{|\mathcal{U}_k|}.
\]

Take logarithms of both sides to yield
\[
\log \left( \frac{\Delta_{\min}}{\Delta_0} \right) \leq |S_k| \log \gamma_2 + |\mathcal{U}_k| \log \gamma_1.
\]

Since the total iterations is the sum of successful and unsuccessful ones:
\[
k = |S_k| + |\mathcal{U}_k| \quad \Rightarrow \quad |\mathcal{U}_k| = k - |S_k|.
\]

Substitute for $|U_k|$, to yield
\[
\log \left( \frac{\Delta_{\min}}{\Delta_0} \right) \leq |S_k| \log \gamma_2 + (k - |S_k|) \log \gamma_1.
\]

\[
\log \left( \frac{\Delta_{\min}}{\Delta_0} \right) \leq k \log \gamma_2 + |S_k| (\log \gamma_2 - \log \gamma_1).
\]

To Solve for $k$, we rearrange 
\[
k \log \gamma_1 \geq \log \left( \frac{\Delta_{\min}}{\Delta_0} \right) - |S_k| (\log \gamma_2 - \log \gamma_1).
\]

Now divide by $\log \gamma_2 < 0$, flipping the inequality:

\[
k \leq |S_k| \left(1 + \frac{\log \gamma_2}{|\log \gamma_1|} \right) + \frac{1}{|\log \gamma_1|} \left| \log \left( \frac{\Delta_{\min}}{\Delta_0} \right) \right|.
\]
\end{solution}
\begin{problem}
    Assume that $f\in \LC$, $f$ be lower bounded by $f^*$, that $\exists\, L_B>0: \|B_k\| \leq L_B$ and that $\Delta_0\geq \gamma_1 \delta \|\grad(x_0)\|$. The Trust-Region with Cauchy point Algorithm (Algorithm 4, lecture note) has a iteration complexity of $\BigO(\epsilon^{-2})$.
\end{problem}
\ifsolutions
\vskip 0.3cm
\begin{solution}
    The number of successful iterations needed to find an $ \epsilon $ approximate first-order minimizer is bounded above by Lemma 3.6 of the lecture note given as \begin{equation}
        |S_k| \leq \dfrac{f(x_0) - f^*}{\eta_1 c_2 \epsilon^2}.
    \end{equation} 
    Since for all iterations before termination $\| \nabla_x f(x_k) \| > \epsilon $, $\Delta_k \geq \gamma_1 \delta \min_{i \in [0:k]}\|\grad f(x_i)\| , \ \delta:=\frac{1-\eta_2}{2(L+L_B)}$ (lemma 3.4, lecture note) implies that $\Delta_k \geq \Delta_{\min} $, where
\begin{equation}
    \Delta_{\min} = \frac{\gamma_1 (1 - \eta_2)}{2 (L + L_B)}\epsilon.
\end{equation}
Thus, the total number of iterations (including unsuccessful ones) can be bounded above using (Lemma 3.1, lecture note)
\[
k \leq |S_k| \left(1 + \frac{\log \gamma_2}{|\log \gamma_1|} \right) + \frac{1}{|\log \gamma_1|} \left| \log \left( \frac{\Delta_{\min}}{\Delta_0} \right) \right|,
\] yielding that at most
\begin{equation}
\kappa^S_{\text{TR1}}  \frac{(f(x_0) - f^*)}{\epsilon^2} \left(1 + \frac{\log \gamma_2}{|\log \gamma_1|}\right) + \frac{1}{|\log \gamma_1|} \left| \log \left(\frac{\Delta_{\min}}{\Delta_0} \right) \right|, \   \kappa^S_{\text{TR1}} = \dfrac{1}{\eta_1 c_2}
\end{equation}
iterations are required.

Since algorithm 4 uses at most one evaluation of $ f $ per iteration and at most one evaluation of its gradient per successful iteration (plus one evaluation of $f $ and $g $ at the final iteration), we deduce that the desired conclusion holds with
\begin{align}
    \kappa^A_{\text{TR1}} &\triangleq \kappa^S_{\text{TR1}} \left(1 + \frac{\log \gamma_2}{|\log \gamma_1|} \right), \\
    \kappa^B_{\text{TR1}} &\triangleq \frac{1}{|\log \gamma_1|}, \\
    \kappa^C_{\text{TR1}} &\triangleq \frac{1}{|\log \gamma_1|} \left| \log \left( \frac{\gamma_1(1 - \eta_2)}{2 \Delta_0 (L + L_B)}  \epsilon \right) \right| + 1.
\end{align}
Algorithm 4 requires at most
\begin{equation}
    \#^f_{\text{TR1}} \triangleq \frac{\kappa^A_{\text{TR1}} (f(x_0) - f^{*})}{\epsilon^2} + \kappa^B_{\text{TR1}} |\log \epsilon| + \kappa^C_{\text{TR1}} = \mathcal{O}(\epsilon^{-2})
\end{equation}
function evaluations and at most
\begin{equation}
    \#^p_{\text{TR1}} \triangleq \frac{\kappa^S_{\text{TR1}} (f(x_0) - f^{*})}{\epsilon^2} + 1 = \mathcal{O}(\epsilon^{-2})
\end{equation}
gradient evaluations to produce an iterate $ x_\epsilon $ such that $ \| \nabla_x f(x_\epsilon) \| \leq \epsilon $.
\end{solution}
\begin{problem}[Model decrease]\label{lemma:model_decrease2}
	Let $\{x_k\}$ and $\{\Delta_k\}$ be the sequences generated by  Algorithm 4 (Lecture note, Trust-Region with Cauchy point). Suppose that $f\in \LC$, that $\exists\, L_B>0: \|B_k\| \leq L_B$ and that $\Delta_0\geq \gamma_1 \delta \|\grad(x_0)\|$. Then we have that, for all $k \geq 0$,
	\begin{equation*}
		m_k(0) - m_k(s_k) \geq c_1 \|\grad(x_k)\| \min_{i \in [0:k]} \|\grad(x_i)\|,
	\end{equation*}
	where
	\begin{equation*}
		c_1 := \frac{1}{2} \min \left[ \frac{1}{L_B}, \gamma_1\delta \right] \quad \with \delta \text{ defined in} 
	\end{equation*}
    \begin{equation}\label{eq:successful_condition}
		\Delta_k < \delta\|\grad(x_k)\|, \quad \with \delta:=\frac{(1 - \eta_2)}{2(L + L_B)}
	\end{equation}
Hint: It follows from \begin{align}
		m_k(0) - m_k(s_k) &\geq m_k(0) - m_k(s_k^c) \\
		&\geq \frac{1}{2} \|\nabla_x f(x_k)\| \min \left[ \frac{\|\grad(x_k)\|}{L_B}, \Delta_k \right] \\
		&= \frac{1}{2} \|\grad(x_k)\| \Delta_k.
	\end{align}
    
    \begin{equation}\label{eq:model_decrease}
		m_k(0)-m_k(s_k^C) \geq \frac{1}{2}\|\grad(x_k)\|\cdot \min\{\Delta_k, \frac{||\grad(x_k)||}{\|B_k\|} \}
	\end{equation} and \begin{equation}\label{eq:deltamin}
		\Delta_k \geq \gamma_1 \delta \min_{i \in [0:k]} \|\grad(x_i)\| \quad \forall k\geq 0,
	\end{equation}

\end{problem}
\ifsolutions
\vskip 0.3cm


\begin{solution}
    \begin{itemize}
    \item $ f \in \mathcal{C}^{1,1} $ i.e., $ \nabla f $ is Lipschitz continuous.
    \item $ \|B_k\| \leq L_B $
    \item $ \Delta_0 \geq \gamma_1 \delta \|\nabla f(x_0)\| $
    \item $ \Delta_k < \delta \|\nabla f(x_k)\| $, where $ \delta := \frac{1 - \eta_2}{2(L + L_B)} $
\end{itemize}

\vskip 0.3cm
From the model decrease at the Cauchy point
\[
m_k(0) - m_k(s_k) \geq m_k(0) - m_k(s_k^C) \geq \frac{1}{2} \|\nabla f(x_k)\| \cdot \min\left\{ \Delta_k, \frac{\|\nabla f(x_k)\|}{\|B_k\|} \right\}
\]

and using the bound on $\|B_k\|$,
Since $ \|B_k\| \leq L_B $, it follows that
\[ \frac{\|\nabla f(x_k)\|}{\|B_k\|} \geq \frac{\|\nabla f(x_k)\|}{L_B} \]
So the decrease becomes
\[m_k(0) - m_k(s_k) \geq \frac{1}{2} \|\nabla f(x_k)\| \cdot \min\left\{ \Delta_k, \frac{\|\nabla f(x_k)\|}{L_B} \right\}\]

%\textbf{Step 3: Apply Assumption on $ \Delta_k $}
%Given $ \Delta_k < \delta \|\nabla f(x_k)\| $, we know:
%\[\min\left\{ \Delta_k, \frac{\|\nabla f(x_k)\|}{L_B} \right\} = \Delta_k\]
%Therefore,
%\[
%m_k(0) - m_k(s_k) \geq \frac{1}{2} \|\nabla f(x_k)\| \cdot \Delta_k
%\]

By assumption, we have that
\[
\Delta_k \geq \gamma_1 \delta \cdot \min_{i \in [0:k]} \|\nabla f(x_i)\|
\]
Substitute into the previous inequality
\begin{align*}
    m_k(0) - m_k(s_k) \geq & \frac{1}{2} \|\nabla f(x_k)\| \cdot \min\left\{ \Delta_k, \frac{\|\nabla f(x_k)\|}{L_B} \right\} \\
    &\geq \frac{1}{2} \|\nabla f(x_k)\| \cdot \min\left\{\gamma_1 \delta \cdot \min_{i \in [0:k]} \|\nabla f(x_i)\| , \frac{\|\nabla f(x_k)\|}{L_B} \right\} \\
    &\geq \frac{1}{2} \|\nabla f(x_k)\| \cdot \min\left\{\gamma_1 \delta \cdot \min_{i \in [0:k]} \|\nabla f(x_i)\| , \frac{\min_{i \in [0:k]}\|\nabla f(x_i)\|}{L_B} \right\}\\
    &\geq \frac{1}{2} \|\nabla f(x_k)\| \cdot \min\left\{\gamma_1 \delta \cdot , \frac{1}{L_B} \right\} \min_{i \in [0:k]} \|\nabla f(x_i)\|
\end{align*}

Let
\[
c_1 := \frac{1}{2} \min\left\{ \frac{1}{L_B}, \gamma_1 \delta \right\}
\]
%In this case, since we used the $ \gamma_1 \delta $ branch, we conclude
Then, we have the bound
\[ m_k(0) - m_k(s_k) \geq c_1 \|\nabla f(x_k)\| \cdot \min_{i \in [0:k]} \|\nabla f(x_i)\|\quad \blacksquare\]
\end{solution}

\begin{problem} Algorithm 3 (lecture note, The Trust region) applied to minimize a function $f\in \LC(\Rn)$ bounded from below may require as many as $\epsilon^{-2}$ evaluations of $f$ and $\grad$ to produce an iterate $x\in \Rn$ such that $||\grad (x)\|\leq \epsilon.$

Hint: one needs to prove that the sequence of iterates in Theorem 1.1 (Lecture note) can be generated also
by Algorithm 3. 
\end{problem}

\ifsolutions
\vskip 0.3cm
\begin{solution}

The trust region algorithm applied to a function $ f \in C^1 $ satisfying the assumptions above, may require up to $\mathcal{O}(\epsilon^{-2}) $ iterations to generate a point $ x_\epsilon $ such that $ \|\nabla f(x_\epsilon)\| \leq \epsilon$.\\
\vskip 0.2cm

Let us construct the sequence. Let $ \epsilon \in (0, 1] $ be given and fix a step size $ t_{\text{ini}} > 0 $ and define the gradient norms as
\[
\|g_k\| = \begin{cases}
2\epsilon & \text{for } k < k_\epsilon \\
\epsilon & \text{for } k = k_\epsilon
\end{cases}
\]
We define the iterates
\[ x_{k+1} = x_k - t_{\text{ini}} g_k\]
Initialize the function value as $f_0 = 4t_{\text{ini}} $, and define the sequence
\[
f_{k+1} = f_k - t_{\text{ini}} \|g_k\|^2
\Rightarrow f_k = f_0 - 4k t_{\text{ini}} \epsilon^2 \quad \text{for } k < k_\epsilon
\]
for $f_k \geq 0$, we choose
\[
k_\epsilon = \left\lfloor \frac{f_0}{4 t_{\text{ini}} \epsilon^2} \right\rfloor = \mathcal{O}(\epsilon^{-2})
\]

By Whitneyâ€™s extension theorem, there exists a function $ f \in C^1(\mathbb{R}) $ such that:
\[f(x_k) = f_k, \quad f'(x_k) = g_k\]
Furthermore, $ f $ satisfies
\begin{itemize}
    \item $f $ is bounded below: $ f(x) \geq 0 $
    \item $ f $ has Lipschitz continuous gradient near all $ x_k $
\end{itemize}
So $ f $ satisfies the assumptions: $f$ is Bounded below and $\grad $ is Lipschitz cont. \\
\vskip 0.2cm
We construct the model as
\[
m_k(s) = f_k + g_k s + \frac{1}{2} B_k s^2
\]
To ensure that the step is feasible, we choose the following
\[
B_k = \frac{1}{t_{\text{ini}}}, \quad s_k = -t_{\text{ini}} g_k, \quad \|s_k\| = t_{ini}\|g_k\| \leq \Delta_k = 1 \implies 2t_{ini}\epsilon \leq 1
\]  \\

We will then compute the model decrease
\[
m_k(0) - m_k(s_k) = t_{\text{ini}} \|g_k\|^2 - \frac{1}{2} t_{\text{ini}} \|g_k\|^2 = \frac{1}{2} t_{\text{ini}} \|g_k\|^2.
\]
And compute the actual  decrease as
\[
f_k - f_{k+1} = t_{\text{ini}} \|g_k\|^2
\]
Then calculate the acceptance of the trial point
\[
\rho_k = \frac{f_k - f_{k+1}}{m_k(0) - m_k(s_k)} = 2 \geq \eta
\]
So all steps are accepted by the algorithm.\\
\vskip 0.2cm

Since $ \|\nabla f(x_k)\| = 2\epsilon > \epsilon $ for $ k < k_\epsilon $, and equals $ \epsilon $ at $ k = k_\epsilon $, we conclude that the number of iterations is
\[
 k_\epsilon = \mathcal{O}(\epsilon^{-2})
\]
We have constructed a smooth function $ f \in C^1 $ such that the trust region algorithm with fixed Hessian approximation $ B_k = 1/t_{\text{ini}} I, \text{and}\  \Delta_k = 1 $ takes $ \mathcal{O}(\epsilon^{-2}) $ iterations to reduce the gradient norm below $ \epsilon $. \qed
\end{solution}
%-----------------------------------------------------------------------%
% \vskip 0.5cm
% \begin{problem}
% \end{problem}

% \ifsolutions
% \vskip 0.3cm

% \begin{solution}
% \end{solution}
 \fi

%-----------------------------------------------------------------------%
\vskip 0.5cm

%\fi

\end{document}