\documentclass{ExerciseSheet}

%Set Number of the Exercise sheet and the submission deadline.
\setExerciseSheetNumber{11}
\setSubmissionDate{xx.xx.2024}

%boolean variable to determine whether the solutions should be included
\newif\ifsolutions
\solutionstrue
% \solutionsfalse

%We have a figure in this sheet
\usepackage{graphicx}

\begin{document}


%Start with exercises
%-----------------------------------------------------------------------%



\begin{problem}
	For the BFGS update, if $B_k \succ 0$ and $y_k^T s_k > 0$, then $B_{k+1} \succ 0$.


\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{proof}
    Since 
    \begin{equation*}
	B_{k+1} = (I - \rho_k y_k s_k^T) B_k (I - \rho_k s_k y_k^T) + \rho_k y_k y_k^T
	\end{equation*}
	where $\rho_k = \frac{1}{y_k^T s_k} > 0$ using Woodbury formula, for any $z \neq 0$, we have:
	\begin{equation*}
		z^T B_{k+1} z = \tilde{z}^T B_k \tilde{z} + \rho_k (z^T y_k)^2
	\end{equation*}
	where $\tilde{z} = (I - \rho_k s_k y_k^T) z$. We consider two cases:

    
	\noindent \textbf{Case 1:} If $\tilde{z} \neq 0$, then $z^T B_{k+1} z > 0$ since $B_k \succ 0$ and $\rho_k (z^T y_k)^2 \geq 0$.

    
	\noindent \textbf{Case 2:} If $\tilde{z} = 0$, then $(I - \rho_k s_k y_k^T) z = 0$, which implies:
	\begin{equation*}
		z = \rho_k s_k y_k^T z
	\end{equation*}
	In this case:
	\begin{equation*}
		z^T B_{k+1} z = \rho_k (z^T y_k)^2 > 0
	\end{equation*}
	unless $z^T y_k = 0$, which would imply $z = 0$ from $z = \rho_k s_k y_k^T z$.
	Therefore, for any $z \neq 0$, we have $z^T B_{k+1} z > 0$, which means $B_{k+1} \succ 0$.
\end{proof}

\fi
%-----------------------------------------------------------------------%
\vskip 0.5cm

\begin{problem}
  Let $f\in \mathcal{C}^1(C)$, where $C$ is closed convex and $f$ is convex. Let $x^*$ be a local minimum of 
  \begin{align}
      \min_{x\in C} f(x) \label{min}.
  \end{align}
  Then, $x^*$ is a stationary point of \eqref{min} if and only if $x^*$ is a global minimum of \eqref{min}.
\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
    $(\Rightarrow)$ Since $x^*$ is a stationary point of a convex constrained problem, we have
    \begin{align*}
        \nabla f(x^*)^T(x-x^*) \geq 0
    \end{align*}
    for all $x\in C$. Using the convexity of $f,$ it holds that
    \begin{align*}
        f(x)-f(x^*)\geq \nabla f(x^*)^T(x-x^*),
    \end{align*}
    and thus, $f(x)\geq f(x^*)$ for all $x \in C$.

    $(\Leftarrow)$ Since $x^*$ is a global minimum, it implies a stationary point by Theorem 1.3.
\end{solution}

\fi

%-----------------------------------------------------------------------%


\vskip 0.5cm

\begin{problem}

For a set $K\subset \R^n$ and $p\in \N$, let 
$x_i \in K$ for $1\leq i\leq p$, and let $\lambda_i \geq 0$ for $1\leq i\leq p$ with $ \displaystyle\sum_{i=1}^{p}{\lambda_i}=1$. Then, the expression $ \displaystyle\sum_{i=1}^{p}{\lambda_i x_i}$ is called a convex combination of elements from $K$. 

\begin{enumerate}
 \item Prove that a set $K \subset \R^n$ is convex if and only if every convex combination of elements from $K$ is contained in $K$.

 \item Let $f$ be convex and $D = \left\{\displaystyle\sum_{i=0}^{n}\lambda_ix_i ~|~ \lambda_i\geq0, \sum_{i=0}^{n}\lambda_i=1\right\}$. Prove that  $\forall x\in D ~~ f(x) \leq \displaystyle\max_{i\in\{0,\dots,n\}}f(x_i).$
         %\item [(ii)] Set $\displaystyle\max_{i\in\{0,\dots,n\}}f(x_i):= C$ and show that $f(x)\le C$ for all $x\in\Delta$ 
  %\end{enumerate}
 \end{enumerate}

\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
\begin{enumerate}
    \item ($\Leftarrow$) Let assume every convex combination of elements from $K$ is contained in $K$ and prove $K$ is convex.\\
    Let $x,y\in K$ and $\lambda\in [0, 1],$ given that every convex combination of element of $K$ belong to $K$ then particularly $(1-\lambda)x+\lambda y\in K.$\\
    ($\Rightarrow$) Now let us assume $K\subset \R^n$ is convex and show that every convex combination of elements from $K$ is contained in $K$ i.e. $\displaystyle \sum_{i=1}^{p}\lambda_ix_i\in K.$ We will show this by induction.\\
    Induction start:  $p=1,$ is trivial and for $p=2,$~ $\lambda_1x_1+\lambda_2x_2\in K$ by definition as $K$ is convex.\\
    Let assume $\displaystyle \sum_{i=1}^{p}\lambda_ix_i\in K$ with $x_i\in K$ and $\displaystyle \sum_{i=1}^{p}\lambda_i=1$. For $x_i \in K$ and $\displaystyle\sum_{i=1}^{p+1}\lambda_i=1$, we will prove that $ \displaystyle\sum_{i=1}^{p+1}\lambda_ix_i \in K.$
    We have
      \begin{align}
          \sum_{i=1}^{p+1}\lambda_ix_i&= \sum_{i=1}^{p}\lambda_ix_i+\lambda_{p+1}x_{p+1}\nonumber\\
                           &=(1-\lambda_{p+1})\sum_{i=1}^{p}\frac{\lambda_i}{1-\lambda_{p+1}}x_i+\lambda_{p+1}x_{p+1} \label{ induction step}
      \end{align}
    From now on, we will show that $\displaystyle(1-\lambda_{p+1})\sum_{i=1}^{p}\frac{\lambda_i}{1-\lambda_{p+1}}x_i \in K$. 

Due to $0\leq \lambda_{p+1}\leq 1$, it holds that $\frac{\lambda_i}{1-\lambda_{p+1}}\geq0$ . Since, $\displaystyle \sum_{i=1}^{p+1}\lambda_i=1$, it holds that  $\displaystyle \sum_{i=1}^{p}\lambda_i+ \lambda_{p+1}=1$ and thus, $\sum_{i=1}^{p}\lambda_i=1-\lambda_{p+1}.$
      Therefore, we obtain
   \begin{align*}
        \sum_{i=1}^{p}\frac{\lambda_i}{1-\lambda_{p+1}}=\frac{1-\lambda_{p+1}}{1-\lambda_{p+1}}=1
    \end{align*}
 which means $\displaystyle \sum_{i=1}^{p}\frac{\lambda_i}{1-\lambda_{p+1}}x_i\in K$ by induction hypothesis. Finally, by using the convexity of $K$, we get 
  \begin{align*}
      (1-\lambda_{p+1})\sum_{i=1}^{p}\frac{\lambda_i}{1-\lambda_{p+1}}x_i+\lambda_{p+1}x_{p+1}\in K.
  \end{align*}
 




    \item Let $x\in D$ and prove 
       \begin{align*}
           f(x)\leq \max_{i\in \{0,\dots,n\}}f(x_i).
       \end{align*}
 For $x\in D,$ means $\displaystyle x=\sum_{i=0}^{n}\lambda_ix_i$ with $\displaystyle \lambda_i\geq 0,~\sum_{i=0}^{n}\lambda_i=1.$ Therefore,
   \begin{align*}
       f(x)&=f\left(\sum_{i=0}^{n}\lambda_ix_i\right)  \\
           &\leq \sum_{i=0}^{n}\lambda_if(x_i)  \text{ since $f$ in convex}\\
           &\leq  \left(\max_{i\in \{0,\dots,n\}}f(x_i)\right)\sum_{i=0}^{n}\lambda_i \\
           &=\max_{i\in \{0,\dots,n\}}f(x_i).
   \end{align*}





       
\end{enumerate}
\end{solution}

\fi

%-----------------------------------------------------------------------%
\vskip 0.5cm
\begin{problem}

  
 Let $f:\R^n\rightarrow\R$ be strictly convex. Prove that
 \begin{enumerate}
  \item Let $a\in\R$. The lower level set 
  \begin{align*}
   U(f,a):=\{x \in\R^n\,\mid\,f(x)\leq a\}
  \end{align*}
is convex.
\item Let $x^*\in\R^n$ be a minimizer of $f$. Prove that $U(f,f(x^*))=\{x^*\}$. 
\item Is $f$ necessarily convex if all lower level sets of $f$ are convex? Justify your answer.
 \end{enumerate}
 


\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
\begin{enumerate}
    \item Let $x,y\in  U(f,a)$ and $\lambda\in [0,~1]$ let us show $(1-\lambda)x+\lambda y\in  U(f,a).$\\
    For $\lambda=0,1$ we have  $(1-\lambda)x+\lambda y\in  U(f,a).$ 
  Now let $\lambda\in (0,1)$ the strictly convexity of $f$ yields
  \begin{align*}
      f( (1-\lambda)x+\lambda y) &< (1-\lambda)f(x)                                 +\lambda f(y)\\
                                &\leq(1-\lambda)a+\lambda a\\
                                &=a
  \end{align*}
where we get the second inequality by using the fact that $x,~y\in U(f,a)$ means $f(x)\leq a$ and $f(y)\leq a.$ It follows from the last inequality that $(1-\lambda)x+\lambda y\in U(f,a).$
 \item Let $x^*\in\R^n$ be a minimizer of $f$ and let us prove $U\left(f,f(x^*)\right)=\{x^*\}.$\\
 Let $y\in U\left(f,f(x^*)\right)$ and assume by contradiction that $y\neq x^*.$ For $\lambda\in [0,~1]$ applying the strictly convexity of $f$ to $y$ and $x^*$ yields  
   \begin{align*}
       f( (1-\lambda)x^*+\lambda y) < (1-\lambda)f(x*) +\lambda f(y)< (1-\lambda)f(x^*) +\lambda f(x^*)=f(x^*)
   \end{align*}
   which contradicts the fact that $x^*$ is a minimizer, therefore, $y=x^*.$ 
 \item The Statement is not true in general. For instance,
 Consider \begin{align*}
         f: \R_+\to \R\\
         x \mapsto \sqrt{x}
 \end{align*}
  $f$ is concave and its lower level set $U(f,a)$ given by
    \begin{align*}
        U(f,a)=\begin{cases}
            (-\infty, a^2] &if ~a\geq 0\\
            \emptyset    &~~otherwise
        \end{cases}
    \end{align*}
    is convex.
\end{enumerate}
\end{solution}

\fi
%-----------------------------------------------------------------------%
\vskip 0.5cm
\begin{problem}Let $C\subset \R^n$ be a convex closed subset. The distance function $d_C:\R^n\rightarrow \R_{\geq 0}$ of a point to the set $C$ is given as 
\begin{equation*}
    d_C(x):=\inf_{y\in C} \|x-y\|_2.
\end{equation*}
\begin{enumerate}
    \item Show that $d_C(x)= \|x-P_C(x)\|$.
    \item Show that $d_C$ is a convex function. 
\end{enumerate}
\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
\begin{enumerate}
    \item Since $C$ is convex and $\lVert x-y\rVert^2$ is coercive, $P_C(x)$ admits at least one solution. Moreover, $\lVert x-y\rVert^2$ is strictly convex, and thus $P_C(x)$ has a unique solution. Finally, since norm and norm squared have the same minimizer, we conclude the claim.
    \item Let $x, y \in \R^n$ and $\lambda \in (0,1)$, define $v:= P_C(x)$ and $w:=P_C(y)$. Because $C$ is convex also $\lambda v + (1-\lambda )w \in C.$ 
    \begin{align*}
        d_C(\lambda x + (1-\lambda) y) &= \inf_{c \in C} \|\lambda x + (1-\lambda) y - c\|_2 \\
        & \leq \|\lambda x + (1-\lambda) y - \left( \lambda v + (1-\lambda )w\right)\|_2 \\
        & = \|\lambda (x-v) + (1-\lambda) (y-w) \|_2 \|\\
        &\leq \lambda \|x-v\|_2 + (1-\lambda) \|y-w\|_2 \\
        &= \lambda d_c(x) + (1-\lambda) d_c(y).
    \end{align*}
\end{enumerate}

\end{solution}

\fi

%-----------------------------------------------------------------------%
\end{document}