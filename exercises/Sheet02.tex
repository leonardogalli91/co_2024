\documentclass{ExerciseSheet}

%Set Number of the Exercise sheet and the submission deadline.
\setExerciseSheetNumber{2}
\setSubmissionDate{xx.xx.2024}

%boolean variable to determine whether the solutions should be included
\newif\ifsolutions
\solutionstrue
%\solutionsfalse

%We have a figure in this sheet
\usepackage{graphicx}

\begin{document}


%Start with exercises
%-----------------------------------------------------------------------%


%\subsection*{Notation}
% \vskip 0.5cm 
%-----------------------------------------------------------------------%
\begin{problem}
Recall the definition of the Rayleigh quotient for a symmetric matrix $A\in \R^{n \times n}$:
\begin{equation*}
    R_A: \R^n\backslash \{0\} \longrightarrow \R, \quad x \mapsto  \frac{x^T A x}{\|x\|^2}.
\end{equation*}
Then show that for all $ x\neq 0$:
\begin{align*}
    \lambda_n(A) \leq R_A(x) \leq \lambda_1(A)
\end{align*}
And deduce: 
\begin{align*}
    \min_{x\neq 0} R_A(x)=\lambda_n(A)\quad \text{ and }\quad \max_{x\neq 0} R_A(x)=\lambda_1(A).
\end{align*}
\end{problem}

\ifsolutions
\vskip 0.3cm

\begin{solution}
By the spectral decomposition theorem, we write $A$ as $A=UDU^T$, where $U$ is an orthogonal matrix (consisting of the eigenvectors) and $D$ is a diagonal matrix, consisting of the eigenvalues. Without loss of generality, we assume that the eigenvalues are ordered, i.e., $D=\text{diag}(\lambda_1(A), ..., \lambda_n(A))$ and $\lambda_i(A) \geq \lambda_{i+1}(A)$ for $i=1,...,n-1$. Then, we have

\begin{align}
    \frac{x^TA x}{x^Tx} \overset{A=UDU^T}{=} \frac{x^TUDU^Tx}{x^Tx}\overset{UU^T=Id}{=} \frac{x^TUDU^Tx}{x^TUU^Tx} \overset{y:=U^Tx}{=} \frac{y^TDy}{y^Ty} = \frac{\sum_{i=1}^n y_i^2 \lambda_i(A)} {\sum_{i=1}^n y_i^2}\label{eq: equivalence} .
\end{align}
Thus, it holds that
\begin{align*}
    R_A(x)=\frac{\sum_{i=1}^n y_i^2 \lambda_i(A)} {\sum_{i=1}^n y_i^2}  \leq \frac{\sum_{i=1}^n y_i^2 \lambda_1(A)}{\sum_{i=1}^n y_i^2}=\lambda_1(A)
\end{align*}
and 
\begin{align*}
    R_A(x)=\frac{\sum_{i=1}^n y_i^2 \lambda_i(A)} {\sum_{i=1}^n y_i^2}  \geq \frac{\sum_{i=1}^n y_i^2 \lambda_n(A)}{\sum_{i=1}^n y_i^2}=\lambda_n(A).
\end{align*}
From \eqref{eq: equivalence}, we have $\frac{x^TA x}{x^Tx} =\frac{y^TDy}{y^Ty}$. Taking the maximum on both sides yields
\begin{align*}
    \max_{x\not=0} \frac{x^TA x}{x^Tx} =\max_{y\not=0} \frac{y^TDy}{y^Ty}=\lambda_1(A)
\end{align*}
where the last equality holds when choosing $y=e_1$, i.e., $U^Tx=e_1$ thus $x=u_1$. (Note that $u_i^Tu_i=1$ for all $i=1,\ldots,n.)$ The argument for the minimum is identical.
\end{solution}

\fi
\vskip 0.5cm

%-----------------------------------------------------------------------%
% \vskip 0.5cm
% \begin{problem}
% Show that for all $x,y,z \in \R^n$
% \begin{equation*}
%     \|x-y\| \leq \|x-z\| + \|z-y\|.
% \end{equation*}
% \end{problem}

% \ifsolutions
% \vskip 0.3cm

% \begin{solution}
% We simply use the triangle inequality 
% \begin{equation*}
%     \|x-y\| = \|x-z+z-y\| \leq \|x-z\| + \|z-y\|.
% \end{equation*}
% \end{solution}
% \fi
%-----------------------------------------------------------------------%

\begin{problem}
Prove that for all $x\in \R^n$, one has
\begin{equation*}
    \|x\|_\infty = \lim_{p\rightarrow \infty} \|x\|_p.
\end{equation*}
\end{problem}

\ifsolutions
\vskip 0.3cm
\begin{solution}
We want to show that $\lVert x\rVert_p\rightarrow\lVert x\rVert_\infty$ as $p\rightarrow\infty$, i.e., 
\begin{align*}
    \frac{\lVert x\rVert_p}{\lVert x\rVert_\infty}\rightarrow 1 \quad \text{  as }p\rightarrow\infty.
\end{align*}
It holds that 
\begin{align*}
    \frac{\lVert x\rVert_p}{\lVert x\rVert_\infty}=\frac{\left(\sum_{i=1}^n|x_i|^p\right)^\frac{1}{p}}{\max\{|x_1|,\ldots,|x_n|\}}\leq \frac{\left(n\cdot\max\{|x_1|^p,\ldots,|x_n|^p\}\right)^\frac{1}{p}}{\max\{|x_1|,\ldots,|x_n|\}}=n^{\frac{1}{p}}\rightarrow 1
\end{align*}
as $p\rightarrow \infty$. Moreover, it holds that
\begin{align*}
    \lVert x\rVert_p = \left(\sum_{i=1}^n|x_i|^p\right)^\frac{1}{p} \geq \left(\max\{|x_1|^p,\ldots,|x_n|^p\}\right)^\frac{1}{p}=\max\{|x_1|,\ldots,|x_n|\}=\lVert x\rVert_\infty,
\end{align*}
which implies $\frac{\lVert x\rVert_p}{\lVert x\rVert_\infty}\geq 1$. Combining the upper bound and the lower bound together, we claim the proof. 
\end{solution}

\fi
%-----------------------------------------------------------------------%
\vskip 0.5cm

\begin{problem}
Prove the Cauchy-Schwarz inequality, i.e. for any $x,y \in \R^n$:
\begin{equation*}
    |\langle x, y \rangle | \leq \|x\| \|y\|.
\end{equation*}
And show that the equality holds if and only if $x$ and $y$ are linearly dependent. 
\end{problem} 

\ifsolutions
\vskip 0.3cm
\begin{solution}
If $y=0$, then the statement is clear. Hence we assume $y\not=0$. Let $z:=x - \frac{\inner{y}{x}}{\inner{y}{y}}y$. Then 
\begin{equation*}
    \inner{z}{y} = \inner{x}{y} - \frac{\inner{y}{x}}{\inner{y}{y} }\inner{y}{y} = 0,
\end{equation*}
therefore $z$ is orthogonal to $y$. Then, it holds that
\begin{align*}
    \|x\|_2^2 &= \left\|\frac{\inner{y}{x}}{\inner{y}{y} } y + z \right\|_2^2 \\
    &= \inner{\frac{\inner{y}{x}}{\inner{y}{y} } y + z }{\frac{\inner{y}{x}}{\inner{y}{y} } y + z } \\
    &= \left|\frac{\inner{y}{x}}{\inner{y}{y} }\right|^2\langle y,y\rangle + 2 \frac{\inner{y}{x}}{\inner{y}{y} }\langle y, z \rangle + \langle z, z \rangle \\
    &= \frac{\left|\inner{y}{x}\right|^2}{\|y\|_2^2 }+ \|z\|_2^2 \\
    &\geq \frac{\left|\inner{y}{x}\right|^2}{\|y\|_2^2 }.
\end{align*}
Multiplying both sides by $\|y\|_2^2$ and taking the square root yields the result. Notice that we have equality iff $z=0$, which is the case if $x,y$ are linearly dependent. 
\end{solution}

\fi
%-----------------------------------------------------------------------%
\vskip 0.5cm

\begin{problem}
Recall the following definition from the lecture. Given a matrix $A\in \R^{m\times n}$ and two norms $\| .\|_p$ and $\|.\|_q$ on $\R^n$ and $\R^m$, respectively. Then the induced matrix-norm $\|A\|_{p\rightarrow q} $ is defined as 
\begin{equation*}
    \|A\|_{p\rightarrow q} = \max_{\|x\|_p=1} \|Ax\|_q.
\end{equation*}
\begin{itemize}
 \item Show the definition is equivalent to $\|A\|_{p\rightarrow q} = \max_{x\neq 0} \frac{\|Ax\|_q}{\|x\|_p}$.
\item Show that the induced matrix norm is submultiplicative, i.e. $\|Ax\|_q \leq \|A\|_{p\rightarrow q} \|x\|_p$.
    \item Show that the maximum in the definition of the induced norm is attained, i.e., demonstrate that there exists a vector $x \in \R^n$ such that $\|A\|_{p\rightarrow q} =  \|Ax\|_q$.
   
    
\end{itemize}
\end{problem}

\ifsolutions
\vskip 0.3cm
\begin{solution}
$.$
\begin{itemize}
    \item   
    By the absolute homogeneity, i.e., for $a\in\mathbb{R}$ and $z\in\mathbb{R}^n$: $|a|\lVert z\rVert=\lVert az\rVert$, it holds that 
    \begin{equation*}
         \max_{x\not=0}\frac{\left\lVert Ax\right\rVert_q}{\lVert x\rVert_p}= \max_{x\not=0}\frac{\left\lVert A\frac{x}{\lVert x\rVert_p}\lVert x\rVert_p\right\rVert_q}{\lVert x\rVert_p}= \max_{\lVert y\rVert_p=1}\lVert Ay\rVert_q
    \end{equation*}
    where the last follows from $\frac{x}{\lVert x\rVert_p}=:y$.
    \item Trivial for $x=0$. Assume $x\neq 0$. Then, we have
    \begin{equation*}
        \|Ax\|_q = \|x\|_p \frac{\|Ax\|_q}{\|x\|_p} \leq \|x\|_p \max_{x\neq 0} \frac{\|Ax\|_q}{\|x\|_p} \leq \|x\|_p \|A\|_{p\rightarrow q}.
    \end{equation*}
    \item we need to prove that this maximum is not just a supremum, but is actually attained by some vector $x\in\mathbb{R}^n$ with $\lVert x\rVert_p=1$. 
    
    We will use compactness and continuity. We define $f:\{x\in \R^n\mid \|x\|_p=1\}\rightarrow \R$, with $ x \mapsto f(x)=\|Ax\|_q$. The map $f$ is continuous, as $x\mapsto Ax$ is linear, and the norm is continuous. And the set $\{x\in \R^n| \|x\|_p=1\}$ is a compact (bounded and closed) subset of $\R^n$. Hence, $f$ obtains its maximum in the domain. In other words, $\max \|Ax\|_q$ is attained for some $x\in\R^n$ with $\lVert x\rVert_p=1$.
\end{itemize}
\end{solution}

\fi

%-----------------------------------------------------------------------%
\vskip 0.5cm

\begin{problem}
Let $A \in \R^{m\times n}$. Show that the following holds:
\begin{align*}
    \|A\|_1=\max_{j=1,..,n} \sum_{i=1}^m|A_{i,j}|.
\end{align*}

\end{problem}

\ifsolutions
\vskip 0.3cm
\begin{solution}
    By the definition, it holds that
    \begin{align}\label{eq: RHS}
        \lVert A\rVert_1=\max_{\lVert x\rVert_1=1}\lVert Ax\rVert_1 = \max_{\lVert x\rVert_1=1} \sum_{i=1}^m\left\rvert\sum_{j=1}^n A_{ij}x_j\right\rvert \leq \max_{\lVert x\rVert_1=1} \sum_{i=1}^m\sum_{j=1}^n\left\rvert A_{ij}x_j\right\rvert= \max_{\lVert x\rVert_1=1}\sum_{j=1}^n |x_j|\sum_{i=1}^m|A_{ij}|\leq \max_{j=1,\ldots,n}\sum_{i=1}^m|A_{ij}|
    \end{align}
    where the third inequality holds due to the triangle inequality, and the last holds because the expression $\sum_{j=1}^n |x_j|\sum_{i=1}^m|A_{ij}|$ is a convex combination of the column sums $\sum_{i=1}^m|A_{ij}|$, since $\sum_{j=1}^n |x_j|=\lVert x\rVert_1=1$.
    
    Now, we consider the maximum column sum, i.e., $\max_{j=1,..,n} \sum_{i=1}^m|A_{i,j}|$. Let $k$ be an index where this maximum is attained. Define the standard basis vector $e_j\in\R^n$. Then, it holds that
    \begin{align}\label{eq: LHS}
        \max_{j=1,..,n} \sum_{i=1}^m|A_{i,j}|= \sum_{i=1}^m|A_{i,k}| = \lVert Ae_k\rVert_1\leq \lVert A\rVert_1\lVert e_k\rVert_\infty = \lVert A\rVert_1
    \end{align}
    where the third inequality holds due to HÃ¶lder's inequality, i.e., let $Q\in \R^{m\times n}, v \in \R^n$ and $p,q \in[1,\infty]$ with $\frac{1}{p}+\frac{1}{q}=1$, then we have
    \begin{align*}
        \lVert Qv\lVert_1 \leq \lVert Q\rVert_p\lVert v\rVert_q.
    \end{align*}
    Combining \eqref{eq: RHS} and \eqref{eq: LHS} completes the proof.
\end{solution}

\fi

\end{document}